{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import argparse\n",
    "import matplotlib.pyplot as plt\n",
    "import japanize_matplotlib\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import japanese_clip as ja_clip\n",
    "from transformers import MLukeTokenizer, LukeModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # コマンドライン引数の処理\n",
    "# parser = argparse.ArgumentParser(description=\"設定用プログラム\")\n",
    "\n",
    "\n",
    "# parser.add_argument(\"--num_workers\", type = int, default = 16, help = \"データローダが使用するCPUのスレッド数(GPUの総スレッド数の8割が推奨)\")\n",
    "# parser.add_argument(\"--reset_data\", action = \"store_true\", help = \"データセットを再作成するか\")\n",
    "# parser.add_argument(\"--use_unreal_image\", action = \"store_true\", help = \"現実写真以外を使用する\")\n",
    "# parser.add_argument(\"--use_word_image\", action = \"store_true\", help = \"文字を含む画像を使用する\")\n",
    "# parser.add_argument(\"--use_unique_noun_boke\", action = \"store_true\", help = \"固有名詞を含む大喜利を使用する\")\n",
    "# parser.add_argument(\"--use_caption\", action = \"store_true\", help = \"負例としてキャプションを使用する\")\n",
    "# parser.add_argument(\"--use_miss_boke\", action = \"store_true\", help = \"負例として別の画像の大喜利を使用する\")\n",
    "# parser.add_argument(\"--num_ratio_miss_boke\", type = int, default = 1, help = \"正例の何倍の別の画像の大喜利を使用するか\")\n",
    "# parser.add_argument(\"--epoch\", type = int, default = 15, help = \"学習反復回数\")\n",
    "# parser.add_argument(\"--batch_size\", type = int, default = 64, help = \"バッチサイズ\")\n",
    "# parser.add_argument(\"--learning_rate\", type = float, default = 0.001, help = \"学習率\")\n",
    "# parser.add_argument(\"--feature_dim\", type = int, default = 1024, help = \"モデルの特徴量次元数\")\n",
    "\n",
    "\n",
    "# args = parser.parse_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "result directory: ../../results/Boke_Judge/False_False_False_True_True_1_15_64_0.0001_1024/\n"
     ]
    }
   ],
   "source": [
    "NUM_WORKERS = 16 # args.num_workers\n",
    "RESET_DATA = False # args.reset_data\n",
    "\n",
    "USE_UNREAL_IMAGE = False # args.use_unreal_image\n",
    "USE_WORD_IMAGE = False # args.use_word_image\n",
    "USE_UNIQUE_NOUN_BOKE = False # args.use_unique_noun_boke\n",
    "USE_CAPTION = True # args.use_caption\n",
    "USE_MISS_BOKE = True # args.use_miss_boke\n",
    "NUM_RATIO_MISS_BOKE = 1 # args.num_ratio_miss_boke\n",
    "\n",
    "EPOCH = 15 # args.epoch\n",
    "BATCH_SIZE = 64 # args.batch_size\n",
    "LEARNING_RATE = 0.0001 # args.learning_rate\n",
    "FEATURE_DIM = 1024 # args.feature_dim\n",
    "\n",
    "RESULT_DIR = f\"../../results/Boke_Judge/{USE_UNREAL_IMAGE}_{USE_WORD_IMAGE}_{USE_UNIQUE_NOUN_BOKE}_{USE_CAPTION}_{USE_MISS_BOKE}_{NUM_RATIO_MISS_BOKE}_{EPOCH}_{BATCH_SIZE}_{LEARNING_RATE}_{FEATURE_DIM}/\"\n",
    "\n",
    "if not os.path.exists(\"../../results/Boke_Judge/\"):\n",
    "    os.mkdir(\"../../results/Boke_Judge/\")\n",
    "if not os.path.exists(RESULT_DIR):\n",
    "    os.mkdir(RESULT_DIR)\n",
    "print(f\"result directory: {RESULT_DIR}\")\n",
    "with open(f\"{RESULT_DIR}config.json\", \"w\") as f:\n",
    "    json.dump({\n",
    "        \"USE_UNREAL_IMAGE\": USE_UNREAL_IMAGE,\n",
    "        \"USE_WORD_IMAGE\": USE_WORD_IMAGE,\n",
    "        \"USE_UNIQUE_NOUN_BOKE\": USE_UNIQUE_NOUN_BOKE,\n",
    "        \"USE_CAPTION\": USE_CAPTION,\n",
    "        \"USE_MISS_BOKE\": USE_MISS_BOKE,\n",
    "        \"NUM_RATIO_MISS_BOKE\": NUM_RATIO_MISS_BOKE,\n",
    "        \"EPOCH\": EPOCH,\n",
    "        \"BATCH_SIZE\": BATCH_SIZE,\n",
    "        \"LEARNING_RATE\": LEARNING_RATE,\n",
    "        \"FEATURE_DIM\": FEATURE_DIM\n",
    "    }, f)\n",
    "\n",
    "DATA_DIR = \"../../datas/boke_data_assemble/\"\n",
    "IMAGE_DIR = \"../../datas/boke_image/\"\n",
    "CLIP_IMAGE_FEATURE_DIR = \"../../datas/encoded/clip_image_feature/\"\n",
    "CLIP_SENTENCE_FEATURE_DIR = \"../../datas/encoded/clip_sentence_feature/\"\n",
    "LUKE_SENTENCE_FEATURE_DIR = \"../../datas/encoded/luke_sentence_feature/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "学習に用いる大喜利の数: 2297194\n",
      " 学習に用いるキャプションの数: 251799\n",
      " 検証に用いる大喜利の数: 23204\n",
      " 検証に用いるキャプションの数: 2544\n"
     ]
    }
   ],
   "source": [
    "# データセットの作成\n",
    "if not os.path.exists(f\"{RESULT_DIR}test_caption_datas.json\") or RESET_DATA:\n",
    "    \n",
    "    boke_datas = list()\n",
    "    caption_datas = list()\n",
    "\n",
    "    max_num_boke = 0\n",
    "    for JP in tqdm(os.listdir(DATA_DIR)):\n",
    "        N = int(JP.split(\".\")[0])\n",
    "\n",
    "        with open(f\"{DATA_DIR}{JP}\", \"r\") as f:\n",
    "            a = json.load(f)\n",
    "\n",
    "        image_information = a[\"image_information\"]\n",
    "        is_photographic_probability = image_information[\"is_photographic_probability\"]\n",
    "        ja_caption = image_information[\"ja_caption\"]\n",
    "        ocr = image_information[\"ocr\"]\n",
    "\n",
    "        # 現実写真以外を除去\n",
    "        if not USE_UNREAL_IMAGE:\n",
    "            if is_photographic_probability < 0.8: continue\n",
    "            \n",
    "        # 文字のある画像を除去\n",
    "        if not USE_WORD_IMAGE:\n",
    "            if len(ocr) != 0: continue\n",
    "\n",
    "        bokes = a[\"bokes\"]\n",
    "\n",
    "        max_num_boke = max(max_num_boke, len(a[\"bokes\"]))\n",
    "        for i, B in enumerate(bokes):\n",
    "\n",
    "            # 固有名詞を含む大喜利を除去\n",
    "            if not USE_UNIQUE_NOUN_BOKE:\n",
    "                if len(B[\"unique_nouns\"]) != 0: continue\n",
    "\n",
    "            boke_datas.append({\n",
    "                \"boke_number\": i,\n",
    "                \"image_number\": N\n",
    "            })\n",
    "\n",
    "        caption_datas.append({\n",
    "            \"caption_number\": N,\n",
    "            \"image_number\": N\n",
    "        })\n",
    "\n",
    "    # データセットの保存\n",
    "    train_boke_datas, test_boke_datas = train_test_split(boke_datas, test_size = 0.01)\n",
    "    train_caption_datas, test_caption_datas = train_test_split(caption_datas, test_size = 0.01)\n",
    "\n",
    "    with open(f\"{RESULT_DIR}train_boke_datas.json\", \"w\") as f:\n",
    "        json.dump(train_boke_datas, f)\n",
    "    with open(f\"{RESULT_DIR}train_caption_datas.json\", \"w\") as f:\n",
    "        json.dump(train_caption_datas, f)\n",
    "\n",
    "    with open(f\"{RESULT_DIR}test_boke_datas.json\", \"w\") as f:\n",
    "        json.dump(test_boke_datas, f)\n",
    "    with open(f\"{RESULT_DIR}test_caption_datas.json\", \"w\") as f:\n",
    "        json.dump(test_caption_datas, f)\n",
    "\n",
    "# データセットの読み込み\n",
    "else:\n",
    "    with open(f\"{RESULT_DIR}train_boke_datas.json\", \"r\") as f:\n",
    "        train_boke_datas = json.load(f)\n",
    "    with open(f\"{RESULT_DIR}train_caption_datas.json\", \"r\") as f:\n",
    "        train_caption_datas = json.load(f)\n",
    "\n",
    "    with open(f\"{RESULT_DIR}test_boke_datas.json\", \"r\") as f:\n",
    "        test_boke_datas = json.load(f)\n",
    "    with open(f\"{RESULT_DIR}test_caption_datas.json\", \"r\") as f:\n",
    "        test_caption_datas = json.load(f)\n",
    "\n",
    "print(f\"学習に用いる大喜利の数: {len(train_boke_datas)}\\n\", \n",
    "      f\"学習に用いるキャプションの数: {len(train_caption_datas)}\\n\", \n",
    "      f\"検証に用いる大喜利の数: {len(test_boke_datas)}\\n\", \n",
    "      f\"検証に用いるキャプションの数: {len(test_caption_datas)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# データローダを作る関数\n",
    "def make_dataloader(boke_datas, caption_datas, \n",
    "                    use_caption = False, use_miss_boke = False, num_ratio_miss_boke = 1,\n",
    "                    num_workers = 4):\n",
    "    \"\"\"\n",
    "        boke_datas: {\"image_number\":画像のお題番号 ,\"boke_number\":image_numberの画像に投稿されたboke_number番目の大喜利}からなるリスト\n",
    "        caption_datas: {\"image_number\":画像のお題番号}からなるリスト\n",
    "        use_caption: キャプションを負例として用いるか\n",
    "        use_miss_boke: 他の画像の大喜利を負例として用いるか\n",
    "        num_ratio_miss_boke: 正例の何倍の別の画像の大喜利を使用するか\n",
    "        num_workers: データローダが使用するCPUのスレッド数\n",
    "    \"\"\"\n",
    "    class LoadNpyDataset(Dataset):\n",
    "        def __init__(self, image_file_paths, sentence_file_paths, teacher_signals):\n",
    "            \"\"\"\n",
    "                image_file_paths: 画像の特徴量のパス(ディレクトリ，.npyを含めない)からなるリスト\n",
    "                sentence_file_paths: 文章の特徴量のパス(boke/image_number/boke_number，またはcaption/image_number，.npyを含めない)からなるリスト\n",
    "                teacher_signals: 教師信号(0または1)からなるリスト\n",
    "            \"\"\"\n",
    "            if len(image_file_paths) != len(sentence_file_paths) and len(sentence_file_paths) != len(teacher_signals):\n",
    "                raise ValueError(\"データリストの長さが一致しません\")\n",
    "\n",
    "            self.image_file_paths = image_file_paths\n",
    "            self.sentence_file_paths = sentence_file_paths\n",
    "            self.teacher_signals = teacher_signals\n",
    "\n",
    "        def __len__(self):\n",
    "            return len(self.teacher_signals)\n",
    "\n",
    "        def __getitem__(self, idx):\n",
    "            clip_image_feature = np.load(f\"{CLIP_IMAGE_FEATURE_DIR}{self.image_file_paths[idx]}.npy\")\n",
    "            clip_sentence_feature = np.load(f\"{CLIP_SENTENCE_FEATURE_DIR}{self.sentence_file_paths[idx]}.npy\")\n",
    "            luke_sentence_feature = np.load(f\"{LUKE_SENTENCE_FEATURE_DIR}{self.sentence_file_paths[idx]}.npy\")\n",
    "            teacher_signal = self.teacher_signals[idx]\n",
    "\n",
    "            return clip_image_feature, clip_sentence_feature, luke_sentence_feature, teacher_signal\n",
    "\n",
    "    def collate_fn_tf(batch):\n",
    "        clip_image_features = torch.Tensor(np.array([B[0] for B in batch]))\n",
    "        clip_sentence_features = torch.Tensor(np.array([B[1] for B in batch]))\n",
    "        luke_sentence_features = torch.Tensor(np.array([B[2] for B in batch]))\n",
    "        teacher_signals = torch.Tensor(np.array([float(B[3]) for B in batch])[..., np.newaxis])\n",
    "        \n",
    "        return clip_image_features, clip_sentence_features, luke_sentence_features, teacher_signals\n",
    "\n",
    "    image_file_numbers = list()\n",
    "    sentence_file_numbers = list()\n",
    "    teacher_signals = list()\n",
    "\n",
    "    for D in boke_datas:\n",
    "        image_file_numbers.append(D[\"image_number\"])\n",
    "        sentence_file_numbers.append(f'boke/{D[\"image_number\"]}/{D[\"boke_number\"]}')\n",
    "        teacher_signals.append(1)\n",
    "\n",
    "    if use_caption:\n",
    "        for D in caption_datas:\n",
    "            image_file_numbers.append(D[\"image_number\"])\n",
    "            sentence_file_numbers.append(f'caption/{D[\"image_number\"]}')\n",
    "            teacher_signals.append(0)\n",
    "    \n",
    "    if use_miss_boke:\n",
    "        miss_boke_datas = list()\n",
    "\n",
    "        # num_ratio_miss_boke回だけ負例を作る\n",
    "        for _ in range(num_ratio_miss_boke):\n",
    "            tmp_idx = np.random.randint(0, len(boke_datas), size = (len(boke_datas), ))\n",
    "            for i, idx in tqdm(enumerate(tmp_idx)):\n",
    "                # ランダムに選んだ大喜利の画像が正例の画像と同じ限り繰り返す\n",
    "                while boke_datas[idx][\"image_number\"] == boke_datas[i][\"image_number\"]:\n",
    "                    idx = np.random.randint(0, len(boke_datas))\n",
    "\n",
    "                miss_boke_datas.append({\n",
    "                    \"boke_path\": f'boke/{boke_datas[idx][\"image_number\"]}/{boke_datas[idx][\"boke_number\"]}',\n",
    "                    \"image_number\": boke_datas[i][\"image_number\"]\n",
    "                })\n",
    "            \n",
    "        for D in miss_boke_datas:\n",
    "            image_file_numbers.append(D[\"image_number\"])\n",
    "            sentence_file_numbers.append(D[\"boke_path\"])\n",
    "            teacher_signals.append(0)\n",
    "    \n",
    "    print(f\"num data: {len(teacher_signals)}\")\n",
    "\n",
    "    tmp = list(zip(image_file_numbers, sentence_file_numbers, teacher_signals))\n",
    "    np.random.shuffle(tmp)\n",
    "    image_file_numbers, sentence_file_numbers, teacher_signals = zip(*tmp)\n",
    "\n",
    "    dataset = LoadNpyDataset(image_file_numbers, sentence_file_numbers, teacher_signals)\n",
    "    dataloader = DataLoader(\n",
    "        dataset, \n",
    "        batch_size = BATCH_SIZE, \n",
    "        num_workers = num_workers, \n",
    "        collate_fn = collate_fn_tf\n",
    "    )\n",
    "\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 大喜利適合判定モデルのクラス\n",
    "class BokeJudgeModel(nn.Module):\n",
    "    def __init__(self, cif_dim = 512, csf_dim = 512, lsf_dim = 768, feature_dim = 1024):\n",
    "        \"\"\"\n",
    "            cif_dim: CLIPの画像の特徴量の次元数\n",
    "            csf_dim: CLIPの文章の特徴量の次元数\n",
    "            lsf_dim: Sentene-LUKEの文章の特徴量の次元数\n",
    "        \"\"\"\n",
    "        super(BokeJudgeModel, self).__init__()\n",
    "        self.cif_dim = cif_dim\n",
    "        self.csf_dim = csf_dim\n",
    "        self.lsf_dim = lsf_dim\n",
    "        \n",
    "        self.fc1 = nn.Linear(cif_dim + csf_dim + lsf_dim, feature_dim)\n",
    "        self.fc2 = nn.Linear(feature_dim, feature_dim)\n",
    "        self.fc3 = nn.Linear(feature_dim, feature_dim)\n",
    "        self.output_layer = nn.Linear(feature_dim, 1)\n",
    "        \n",
    "    def forward(self, cif, csf, lsf):\n",
    "        \"\"\"\n",
    "            cif: CLIPの画像の特徴量\n",
    "            csf: CLIPの文章の特徴量\n",
    "            lsf: Sentence-LUKEの文章の特徴量\n",
    "        \"\"\"\n",
    "        x = torch.cat([cif, csf, lsf], dim = 1)\n",
    "\n",
    "        x = F.leaky_relu(self.fc1(x))\n",
    "        x = F.leaky_relu(self.fc2(x))\n",
    "        x = F.leaky_relu(self.fc3(x))\n",
    "\n",
    "        output = torch.sigmoid(self.output_layer(x))\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 二値分類の精度を計算する関数\n",
    "def calculate_accuracy(teacher_signals, outputs, threshould = 0.5):\n",
    "    \"\"\"\n",
    "        teacher_signals: 教師信号\n",
    "        outputs: モデルの出力\n",
    "        threshould: しきい値\n",
    "    \"\"\"\n",
    "    return ((outputs > threshould).float() == teacher_signals).float().mean()\n",
    "\n",
    "# 1イテレーション学習する関数\n",
    "def train_step(model, optimizer, batch_data, batch_labels):\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(*batch_data)\n",
    "    loss = nn.BCELoss()(outputs, batch_labels)\n",
    "    accuracy = calculate_accuracy(batch_labels, outputs)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss.item(), accuracy.item()\n",
    "\n",
    "# 1イテレーション検証する関数\n",
    "def evaluate(model, batch_data, batch_labels):\n",
    "    with torch.no_grad():\n",
    "        outputs = model(*batch_data)\n",
    "        loss = nn.BCELoss()(outputs, batch_labels)\n",
    "        accuracy = calculate_accuracy(batch_labels, outputs)\n",
    "    return loss.item(), accuracy.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-8-fdf76b1e07f9>:11: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(f\"{RESULT_DIR}model_{len(train_loss_history):03}.pth\"))\n",
      "2297194it [00:01, 1170009.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num data: 4846187\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/15:   0%|          | 0/75722 [00:01<?, ?it/s, train_loss=0.658, train_accuracy=0.625]\n",
      "23204it [00:00, 1776483.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num data: 48952\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:   0%|          | 0/765 [00:00<?, ?it/s, test_loss=0.699, test_accuracy=0.578]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8/15, Train Loss: 0.0000, Train Accuracy: 0.0000, Test Loss: 0.0009, Test Accuracy: 0.0008\n",
      "-------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2297194it [00:01, 1178285.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num data: 4846187\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/15:   0%|          | 0/75722 [00:01<?, ?it/s, train_loss=0.702, train_accuracy=0.547]\n",
      "23204it [00:00, 1541214.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num data: 48952\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:   0%|          | 0/765 [00:00<?, ?it/s, test_loss=0.714, test_accuracy=0.484]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9/15, Train Loss: 0.0000, Train Accuracy: 0.0000, Test Loss: 0.0009, Test Accuracy: 0.0006\n",
      "-------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2297194it [00:01, 1167125.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num data: 4846187\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/15:   0%|          | 0/75722 [00:01<?, ?it/s, train_loss=0.686, train_accuracy=0.578]\n",
      "23204it [00:00, 1541727.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num data: 48952\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:   0%|          | 0/765 [00:00<?, ?it/s, test_loss=0.691, test_accuracy=0.562]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10/15, Train Loss: 0.0000, Train Accuracy: 0.0000, Test Loss: 0.0009, Test Accuracy: 0.0007\n",
      "-------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2297194it [00:02, 1146483.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num data: 4846187\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/15:   0%|          | 0/75722 [00:01<?, ?it/s, train_loss=0.693, train_accuracy=0.547]\n",
      "23204it [00:00, 1485554.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num data: 48952\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:   0%|          | 0/765 [00:00<?, ?it/s, test_loss=0.702, test_accuracy=0.531]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 11/15, Train Loss: 0.0000, Train Accuracy: 0.0000, Test Loss: 0.0009, Test Accuracy: 0.0007\n",
      "-------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2297194it [00:01, 1170393.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num data: 4846187\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/15:   0%|          | 0/75722 [00:01<?, ?it/s, train_loss=0.714, train_accuracy=0.406]\n",
      "23204it [00:00, 1440971.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num data: 48952\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:   0%|          | 0/765 [00:00<?, ?it/s, test_loss=0.691, test_accuracy=0.547]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 12/15, Train Loss: 0.0000, Train Accuracy: 0.0000, Test Loss: 0.0009, Test Accuracy: 0.0007\n",
      "-------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2297194it [00:02, 1103395.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num data: 4846187\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/15:   0%|          | 0/75722 [00:01<?, ?it/s, train_loss=0.674, train_accuracy=0.656]\n",
      "23204it [00:00, 1528363.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num data: 48952\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:   0%|          | 0/765 [00:00<?, ?it/s, test_loss=0.691, test_accuracy=0.516]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 13/15, Train Loss: 0.0000, Train Accuracy: 0.0000, Test Loss: 0.0009, Test Accuracy: 0.0007\n",
      "-------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2297194it [00:02, 1127814.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num data: 4846187\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/15:   0%|          | 0/75722 [00:01<?, ?it/s, train_loss=0.704, train_accuracy=0.359]\n",
      "23204it [00:00, 1519794.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num data: 48952\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:   0%|          | 0/765 [00:00<?, ?it/s, test_loss=0.693, test_accuracy=0.531]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 14/15, Train Loss: 0.0000, Train Accuracy: 0.0000, Test Loss: 0.0009, Test Accuracy: 0.0007\n",
      "-------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2297194it [00:01, 1149510.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num data: 4846187\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/15:   0%|          | 0/75722 [00:01<?, ?it/s, train_loss=0.684, train_accuracy=0.531]\n",
      "23204it [00:00, 1421980.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num data: 48952\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:   0%|          | 0/765 [00:00<?, ?it/s, test_loss=0.693, test_accuracy=0.516]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 15/15, Train Loss: 0.0000, Train Accuracy: 0.0000, Test Loss: 0.0009, Test Accuracy: 0.0007\n",
      "-------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model = BokeJudgeModel(feature_dim = FEATURE_DIM)\n",
    "\n",
    "# 学習履歴がある場合，途中から再開する\n",
    "if os.path.exists(f\"{RESULT_DIR}history.json\"):\n",
    "    with open(f\"{RESULT_DIR}history.json\", \"r\") as f:\n",
    "        a = json.load(f)\n",
    "        train_loss_history = a[\"train_loss\"]\n",
    "        train_accuracy_history = a[\"train_accuracy\"]\n",
    "        test_loss_history = a[\"test_loss\"]\n",
    "        test_accuracy_history = a[\"test_accuracy\"]\n",
    "    model.load_state_dict(torch.load(f\"{RESULT_DIR}model_{len(train_loss_history):03}.pth\"))\n",
    "    SATRT_EPOCH = len(train_loss_history)\n",
    "else:\n",
    "    train_loss_history = []\n",
    "    train_accuracy_history = []\n",
    "    test_loss_history = []\n",
    "    test_accuracy_history = []\n",
    "    SATRT_EPOCH = 0\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model.to(device)\n",
    "optimizer = optim.AdamW(model.parameters(), lr = LEARNING_RATE)\n",
    "\n",
    "for epoch in range(SATRT_EPOCH, EPOCH):\n",
    "\n",
    "    # train\n",
    "    train_loss_obj = 0.0\n",
    "    train_accuracy_obj = 0.0\n",
    "    model.train()\n",
    "    train_dataloader = make_dataloader(train_boke_datas, train_caption_datas,\n",
    "                                       use_caption = USE_CAPTION, use_miss_boke = USE_MISS_BOKE, num_ratio_miss_boke = NUM_RATIO_MISS_BOKE,\n",
    "                                       num_workers = NUM_WORKERS)\n",
    "    pb = tqdm(train_dataloader, desc = f\"Epoch {epoch+1}/{EPOCH}\")\n",
    "    \n",
    "    for CIF, CSF, LSF, TS in pb:\n",
    "        CIF = CIF.to(device)\n",
    "        CSF = CSF.to(device)\n",
    "        LSF = LSF.to(device)\n",
    "        TS = TS.to(device)\n",
    "\n",
    "        loss, accuracy = train_step(model, optimizer, (CIF, CSF, LSF), TS)\n",
    "        train_loss_obj += loss\n",
    "        train_accuracy_obj += accuracy\n",
    "        pb.set_postfix({\"train_loss\": train_loss_obj / (pb.n + 1), \"train_accuracy\": train_accuracy_obj / (pb.n + 1)})\n",
    "        break\n",
    "    train_loss = train_loss_obj / len(train_dataloader)\n",
    "    train_accuracy = train_accuracy_obj / len(train_dataloader)\n",
    "\n",
    "    # test\n",
    "    test_loss_obj = 0.0\n",
    "    test_accuracy_obj = 0.0\n",
    "    model.eval()\n",
    "    test_dataloader = make_dataloader(test_boke_datas, test_caption_datas,\n",
    "                                      use_caption = USE_CAPTION, use_miss_boke = USE_MISS_BOKE, num_ratio_miss_boke = NUM_RATIO_MISS_BOKE,\n",
    "                                       num_workers = NUM_WORKERS)\n",
    "    pb = tqdm(test_dataloader, desc = \"Evaluating\")\n",
    "\n",
    "    for CIF, CSF, LSF, TS in pb:\n",
    "        CIF = CIF.to(device)\n",
    "        CSF = CSF.to(device)\n",
    "        LSF = LSF.to(device)\n",
    "        TS = TS.to(device)\n",
    "        \n",
    "        loss, accuracy = evaluate(model, (CIF, CSF, LSF), TS)\n",
    "        test_loss_obj += loss\n",
    "        test_accuracy_obj += accuracy\n",
    "        pb.set_postfix({\"test_loss\": test_loss_obj / (pb.n + 1), \"test_accuracy\": test_accuracy_obj / (pb.n + 1)})\n",
    "        break\n",
    "    test_loss = test_loss_obj / len(test_dataloader)\n",
    "    test_accuracy = test_accuracy_obj / len(test_dataloader)\n",
    "\n",
    "    print(f\"Epoch: {epoch+1}/{EPOCH}, \"\n",
    "          f\"Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, \"\n",
    "          f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}\")\n",
    "    print(\"-\" * 25)\n",
    "\n",
    "    train_loss_history.append(train_loss)\n",
    "    train_accuracy_history.append(train_accuracy)\n",
    "    test_loss_history.append(test_loss)\n",
    "    test_accuracy_history.append(test_accuracy)\n",
    "\n",
    "    torch.save(model.state_dict(), f\"{RESULT_DIR}model_{len(train_loss_history):03}.pth\")\n",
    "    if os.path.exists(f\"{RESULT_DIR}model_{len(train_loss_history) - 1:03}.pth\"):\n",
    "        os.remove(f\"{RESULT_DIR}model_{len(train_loss_history) - 1:03}.pth\")\n",
    "\n",
    "    # 検証精度を更新した場合、重みを保存\n",
    "    if max(test_accuracy_history) == test_accuracy:\n",
    "        torch.save(model.state_dict(), f\"{RESULT_DIR}best_model.pth\")\n",
    "\n",
    "    # 学習結果を保存\n",
    "    with open(f\"{RESULT_DIR}history.json\", \"w\") as f:\n",
    "        json.dump({\n",
    "            \"train_loss\": train_loss_history,\n",
    "            \"train_accuracy\": train_accuracy_history,\n",
    "            \"test_loss\": test_loss_history,\n",
    "            \"test_accuracy\": test_accuracy_history\n",
    "        }, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 学習結果を描画\n",
    "fig = plt.figure(figsize = (10, 5))\n",
    "ax = fig.add_subplot(1, 2, 1)\n",
    "ax.plot(train_loss_history, label = \"train\")\n",
    "ax.plot(test_loss_history, label = \"test\")\n",
    "ax.set_xlabel(\"epoch\")\n",
    "ax.set_ylabel(\"loss\")\n",
    "ax.legend()\n",
    "ax.grid()\n",
    "\n",
    "ax = fig.add_subplot(1, 2, 2)\n",
    "ax.plot(train_accuracy_history, label = \"train\")\n",
    "ax.plot(test_accuracy_history, label = \"test\")\n",
    "ax.set_xlabel(\"epoch\")\n",
    "ax.set_ylabel(\"accuracy\")\n",
    "ax.legend()\n",
    "ax.grid()\n",
    "\n",
    "fig.savefig(f\"{RESULT_DIR}history.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentence-LUKEのクラス\n",
    "class SentenceLukeJapanese:\n",
    "    def __init__(self, device = None):\n",
    "        self.tokenizer = MLukeTokenizer.from_pretrained(\"sonoisa/sentence-luke-japanese-base-lite\")\n",
    "        self.model = LukeModel.from_pretrained(\"sonoisa/sentence-luke-japanese-base-lite\",\n",
    "                                               torch_dtype = torch.float16)\n",
    "        self.model.eval()\n",
    "\n",
    "        if device is None:\n",
    "            device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        self.device = torch.device(device)\n",
    "        self.model.to(device)\n",
    "\n",
    "    def _mean_pooling(self, model_output, attention_mask):\n",
    "        token_embeddings = model_output[0] #First element of model_output contains all token embeddings\n",
    "        input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "        return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def encode(self, sentences, batch_size = 256):\n",
    "        all_embeddings = []\n",
    "        iterator = range(0, len(sentences), batch_size)\n",
    "        for batch_idx in iterator:\n",
    "            batch = sentences[batch_idx:batch_idx + batch_size]\n",
    "\n",
    "            encoded_input = self.tokenizer.batch_encode_plus(batch, padding=\"longest\",\n",
    "                                           truncation=True, return_tensors=\"pt\").to(self.device)\n",
    "            model_output = self.model(**encoded_input)\n",
    "            sentence_embeddings = self._mean_pooling(model_output, encoded_input[\"attention_mask\"]).to('cpu')\n",
    "\n",
    "            all_embeddings.extend(sentence_embeddings)\n",
    "\n",
    "        return torch.stack(all_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 大喜利適合判定AI\n",
    "class BokeJugeAI:\n",
    "    def __init__(self, weight_path):\n",
    "        \"\"\"\n",
    "            weight_path: 大喜利適合判定モデルの学習済みの重みのパス\n",
    "        \"\"\"\n",
    "        # 大喜利適合判定AIの読み込み\n",
    "        self.boke_judge_model = BokeJudgeModel()\n",
    "        self.boke_judge_model.load_state_dict(torch.load(weight_path))\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.boke_judge_model.to(self.device)\n",
    "        self.boke_judge_model.eval()\n",
    "\n",
    "        # CLIP\n",
    "        self.clip_model, self.clip_preprocesser = ja_clip.load(\"rinna/japanese-clip-vit-b-16\",\n",
    "                                             cache_dir=\"/tmp/japanese_clip\",\n",
    "                                             torch_dtype = torch.float16,\n",
    "                                             device = self.device)\n",
    "        self.clip_tokenizer = ja_clip.load_tokenizer()\n",
    "\n",
    "        # Sentence-LUKE\n",
    "        self.luke_model = SentenceLukeJapanese()\n",
    "\n",
    "    def __call__(self, image_path, sentence):\n",
    "        \"\"\"\n",
    "            image_path: 判定したい大喜利のお題画像\n",
    "            sentence: 判定したい大喜利\n",
    "        \"\"\"\n",
    "        # CLIPによる特徴量への変換\n",
    "        tokenized_sentences = ja_clip.tokenize(\n",
    "            texts = [sentence],\n",
    "            max_seq_len = 77,\n",
    "            device = self.device,\n",
    "            tokenizer = self.clip_tokenizer,\n",
    "            )\n",
    "        image = Image.open(image_path)\n",
    "        preprcessed_image = self.clip_preprocesser(image).unsqueeze(0).to(self.device)\n",
    "        with torch.no_grad():\n",
    "            clip_image_features = self.clip_model.get_image_features(preprcessed_image)\n",
    "            clip_sentence_features = self.clip_model.get_text_features(**tokenized_sentences)\n",
    "\n",
    "        # Sentence-LUKEによる特徴量への変換\n",
    "        luke_sentence_feature = self.luke_model.encode([sentence])\n",
    "\n",
    "        # 大喜利適合判定AIの推論\n",
    "        with torch.no_grad():\n",
    "            outputs = self.boke_judge_model(clip_image_features,\n",
    "                                        clip_sentence_features,\n",
    "                                        luke_sentence_feature.to(self.device))\n",
    "\n",
    "        return outputs.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# テストデータでモデルを評価する関数\n",
    "def evaluate_model(weight_path, boke_data_path, caption_data_path,\n",
    "                   thresholds = [0.1]):#, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]):\n",
    "    \"\"\"\n",
    "        weight_path: 大喜利適合判定モデルの学習済みの重みのパス\n",
    "        boke_data_path: {\"image_number\":お題画像の番号，\"boke_number\":image_numberの画像の何番目の大喜利か}のリストのjsonファイルのパス\n",
    "        caption_data_path: {\"image_number\":お題画像の番号}のリストのjsonファイルのパス\n",
    "    \"\"\"\n",
    "    def calculate_accuracy(predictions, threshold = 0.5):\n",
    "        return sum(np.array(predictions) > threshold) / len(predictions)\n",
    "\n",
    "    boke_judge_ai = BokeJugeAI(weight_path)\n",
    "\n",
    "    # evaluate for caption\n",
    "    print(\"evaluating caption...\")\n",
    "    predictions_for_caption = list()\n",
    "    with open(caption_data_path, \"r\") as f:\n",
    "        test_caption_datas = json.load(f)\n",
    "    \n",
    "    for D in tqdm(test_caption_datas):\n",
    "        image_number = D[\"image_number\"]\n",
    "        with open(f\"{DATA_DIR}{image_number}.json\", \"r\") as f:\n",
    "            ja_caption = json.load(f)[\"image_information\"][\"ja_caption\"]\n",
    "        judge = boke_judge_ai(f\"../../datas/boke_image/{image_number}.jpg\",\n",
    "                            ja_caption) \n",
    "        predictions_for_caption.append(judge[0][0])\n",
    "    \n",
    "    # evaluate for boke\n",
    "    print(\"evaluating boke...\")\n",
    "    predictions_for_boke = list()\n",
    "    with open(boke_data_path, \"r\") as f:\n",
    "        test_boke_datas = json.load(f)\n",
    "\n",
    "    for D in tqdm(test_boke_datas):\n",
    "        image_number = D[\"image_number\"]\n",
    "        boke_number = D[\"boke_number\"]\n",
    "        with open(f\"{DATA_DIR}{image_number}.json\", \"r\") as f:\n",
    "            boke = json.load(f)[\"bokes\"][boke_number][\"boke\"]\n",
    "        judge = boke_judge_ai(f\"{IMAGE_DIR}{image_number}.jpg\",\n",
    "                            boke) \n",
    "        predictions_for_boke.append(judge[0][0])\n",
    "\n",
    "    # evaluate for miss boke\n",
    "    print(\"evaluating miss boke...\")\n",
    "    predictions_for_miss_boke = list()\n",
    "    test_miss_boke_datas = list()\n",
    "    tmp_idx = np.random.randint(0, len(test_boke_datas), size = (len(test_boke_datas), ))\n",
    "    for i, idx in tqdm(enumerate(tmp_idx)):\n",
    "        # ランダムに選んだ大喜利の画像が正例の画像と同じ限り繰り返す\n",
    "        while test_boke_datas[idx][\"image_number\"] == test_boke_datas[i][\"image_number\"]:\n",
    "            idx = np.random.randint(0, len(test_boke_datas))\n",
    "\n",
    "        test_miss_boke_datas.append({\n",
    "            \"boke_number\": test_boke_datas[i][\"boke_number\"],\n",
    "            \"original_image_number\": test_boke_datas[i][\"image_number\"],\n",
    "            \"miss_image_number\": test_boke_datas[idx][\"image_number\"]\n",
    "        })\n",
    "\n",
    "    for D in tqdm(test_miss_boke_datas):\n",
    "        original_image_number = D[\"original_image_number\"]\n",
    "        miss_image_number = D[\"miss_image_number\"]\n",
    "        boke_number = D[\"boke_number\"]\n",
    "        with open(f\"{DATA_DIR}{original_image_number}.json\", \"r\") as f:\n",
    "            boke = json.load(f)[\"bokes\"][boke_number][\"boke\"]\n",
    "        judge = boke_judge_ai(f\"{IMAGE_DIR}{miss_image_number}.jpg\",\n",
    "                            boke) \n",
    "        predictions_for_miss_boke.append(judge[0][0])\n",
    "\n",
    "    result_dict = dict()\n",
    "    for T in thresholds:\n",
    "        result_dict[T] = {\n",
    "            \"caption\": 1 - calculate_accuracy(predictions_for_caption, T),\n",
    "            \"boke\": calculate_accuracy(predictions_for_boke, T),\n",
    "            \"miss_boke\": 1 - calculate_accuracy(predictions_for_miss_boke, T)\n",
    "        }\n",
    "    \n",
    "    return result_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-13-9b97ec0fe3da>:9: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.boke_judge_model.load_state_dict(torch.load(weight_path))\n",
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating caption...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2544/2544 [01:14<00:00, 34.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating boke...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 23204/23204 [07:42<00:00, 50.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating miss boke...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23204it [00:00, 1699874.77it/s]\n",
      " 40%|████      | 9365/23204 [03:02<04:33, 50.61it/s]"
     ]
    }
   ],
   "source": [
    "\n",
    "# テストデータでモデルを評価\n",
    "evaluate_result_dict = evaluate_model(f\"{RESULT_DIR}best_model.pth\",\n",
    "                                      boke_data_path = f\"{RESULT_DIR}test_boke_datas.json\", \n",
    "                                      caption_data_path = f\"{RESULT_DIR}test_caption_datas.json\")\n",
    "with open(f\"{RESULT_DIR}evaluation_result.json\", \"w\") as f:\n",
    "    json.dump(evaluate_result_dict, f)\n",
    "\n",
    "x = list()\n",
    "caption_evaluations = list()\n",
    "boke_evaluations = list()\n",
    "miss_boke_evaluations = list()\n",
    "for K, V in evaluate_result_dict.items():\n",
    "    x.append(K)\n",
    "    caption_evaluations.append(V[\"caption\"])\n",
    "    boke_evaluations.append(V[\"boke\"])\n",
    "    miss_boke_evaluations.append(V[\"miss_boke\"])\n",
    "\n",
    "fig = plt.figure(figsize = (5, 5))\n",
    "ax = fig.add_subplot()\n",
    "ax.plot(x, caption_evaluations, label = \"caption\", color = \"red\")\n",
    "ax.plot(x, boke_evaluations, label = \"boke\", color = \"blue\")\n",
    "ax.plot(x, miss_boke_evaluations, label = \"miss boke\", color = \"green\")\n",
    "ax.scatter(x, caption_evaluations, color = \"red\")\n",
    "ax.scatter(x, boke_evaluations, color = \"blue\")\n",
    "ax.scatter(x, miss_boke_evaluations, color = \"green\")\n",
    "ax.legend()\n",
    "ax.grid()\n",
    "ax.set_ylabel(\"accuracy\")\n",
    "ax.set_xlabel(\"threshold\")\n",
    "\n",
    "fig.savefig(f\"{RESULT_DIR}evaluation_result.png\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Colab_20241111",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
