{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import japanese_clip as ja_clip\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from transformers import MLukeTokenizer, LukeModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-12-05 12:44:08--  https://www.shonan-it.ac.jp/media/20241126_g03.jpg\n",
      "Resolving www.shonan-it.ac.jp (www.shonan-it.ac.jp)... 150.60.144.101, 10.2.10.10, 10.2.10.20\n",
      "Connecting to www.shonan-it.ac.jp (www.shonan-it.ac.jp)|150.60.144.101|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 56759 (55K) [image/jpeg]\n",
      "Saving to: ‘test_image.jpg’\n",
      "\n",
      "test_image.jpg      100%[===================>]  55.43K  --.-KB/s    in 0.04s   \n",
      "\n",
      "2024-12-05 12:44:08 (1.49 MB/s) - ‘test_image.jpg’ saved [56759/56759]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://www.shonan-it.ac.jp/media/20241126_g03.jpg -O test_image.jpg\n",
    "test_image_path = \"test_image.jpg\"\n",
    "test_sentence = \"2024年11月14日、「湘南工科大学 産学交流フォーラム2024」が開催され、大学院生による研究発表に対する「優秀プレゼンテーション賞」の表彰が初めて実施されました。\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((512,), (512,))"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "clip_model, clip_preprocesser = ja_clip.load(\"rinna/japanese-clip-vit-b-16\", \n",
    "                                             cache_dir=\"/tmp/japanese_clip\", \n",
    "                                             torch_dtype = torch.float16,\n",
    "                                             device = device)\n",
    "clip_tokenizer = ja_clip.load_tokenizer()\n",
    "\n",
    "encoded_sentences = ja_clip.tokenize(\n",
    "        texts = [test_sentence],\n",
    "        max_seq_len = 77,\n",
    "        device = device,\n",
    "        tokenizer = clip_tokenizer,\n",
    "    )\n",
    "image = Image.open(test_image_path)\n",
    "preprcessed_image = clip_preprocesser(image).unsqueeze(0).to(device)\n",
    "with torch.no_grad():\n",
    "    clip_test_image_features = clip_model.get_image_features(preprcessed_image)\n",
    "    clip_test_image_feature = clip_test_image_features.cpu().numpy()[0]\n",
    "    clip_test_sentence_features = clip_model.get_text_features(**encoded_sentences)\n",
    "    clip_test_sentence_feature = clip_test_sentence_features.cpu().numpy()[0]\n",
    "\n",
    "\n",
    "clip_test_image_feature.shape, clip_test_sentence_feature.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentenceLukeJapanese:\n",
    "    def __init__(self, device = None):\n",
    "        self.tokenizer = MLukeTokenizer.from_pretrained(\"sonoisa/sentence-luke-japanese-base-lite\")\n",
    "        self.model = LukeModel.from_pretrained(\"sonoisa/sentence-luke-japanese-base-lite\",\n",
    "                                               torch_dtype = torch.float16)\n",
    "        self.model.eval()\n",
    "\n",
    "        if device is None:\n",
    "            device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        self.device = torch.device(device)\n",
    "        self.model.to(device)\n",
    "\n",
    "    def _mean_pooling(self, model_output, attention_mask):\n",
    "        token_embeddings = model_output[0] #First element of model_output contains all token embeddings\n",
    "        input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "        return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def encode(self, sentences, batch_size = 256):\n",
    "        all_embeddings = []\n",
    "        iterator = range(0, len(sentences), batch_size)\n",
    "        for batch_idx in iterator:\n",
    "            batch = sentences[batch_idx:batch_idx + batch_size]\n",
    "\n",
    "            encoded_input = self.tokenizer.batch_encode_plus(batch, padding=\"longest\",\n",
    "                                           truncation=True, return_tensors=\"pt\").to(self.device)\n",
    "            model_output = self.model(**encoded_input)\n",
    "            sentence_embeddings = self._mean_pooling(model_output, encoded_input[\"attention_mask\"]).to('cpu')\n",
    "\n",
    "            all_embeddings.extend(sentence_embeddings)\n",
    "\n",
    "        return torch.stack(all_embeddings)\n",
    "\n",
    "luke_model = SentenceLukeJapanese()\n",
    "luke_test_sentence_feature = luke_model.encode([test_sentence]).cpu().numpy()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomModel(nn.Module):\n",
    "    def __init__(self, cif_dim, csf_dim, lsf_dim):\n",
    "        super(CustomModel, self).__init__()\n",
    "        # 入力次元を設定\n",
    "        self.cif_dim = cif_dim\n",
    "        self.csf_dim = csf_dim\n",
    "        self.lsf_dim = lsf_dim\n",
    "        \n",
    "        # 全結合層\n",
    "        self.fc1 = nn.Linear(cif_dim + csf_dim + lsf_dim, 1024)\n",
    "        self.fc2 = nn.Linear(1024, 1024)\n",
    "        self.fc3 = nn.Linear(1024, 1024)\n",
    "        self.output_layer = nn.Linear(1024, 1)\n",
    "        \n",
    "    def forward(self, cif, csf, lsf):\n",
    "        # 入力を結合\n",
    "        x = torch.cat([cif, csf, lsf], dim=1)\n",
    "        # 全結合層 + LeakyReLU\n",
    "        x = F.leaky_relu(self.fc1(x))\n",
    "        x = F.leaky_relu(self.fc2(x))\n",
    "        x = F.leaky_relu(self.fc3(x))\n",
    "        # 出力層 + シグモイド\n",
    "        output = torch.sigmoid(self.output_layer(x))\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-12-05 13:05:51--  https://d2dcan0armyq93.cloudfront.net/photo/odai/600/04950fa024255b5c910abbc35650f2d3_600.jpg\n",
      "Resolving d2dcan0armyq93.cloudfront.net (d2dcan0armyq93.cloudfront.net)... 13.249.166.90, 13.249.166.231, 13.249.166.109, ...\n",
      "Connecting to d2dcan0armyq93.cloudfront.net (d2dcan0armyq93.cloudfront.net)|13.249.166.90|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 41849 (41K) [image/jpeg]\n",
      "Saving to: ‘test_image.jpg’\n",
      "\n",
      "test_image.jpg      100%[===================>]  40.87K  --.-KB/s    in 0.004s  \n",
      "\n",
      "2024-12-05 13:05:54 (10.7 MB/s) - ‘test_image.jpg’ saved [41849/41849]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://d2dcan0armyq93.cloudfront.net/photo/odai/600/04950fa024255b5c910abbc35650f2d3_600.jpg -O test_image.jpg\n",
    "test_image_path = \"test_image.jpg\"\n",
    "test_sentence = \"今日もいい天気\" # \"藤井聡太普段\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-12-05 15:49:52--  https://d2dcan0armyq93.cloudfront.net/photo/odai/600/a7d1e26dc93ee0a4d3b16f80dd0954ea_600.jpg\n",
      "Resolving d2dcan0armyq93.cloudfront.net (d2dcan0armyq93.cloudfront.net)... 13.249.166.109, 13.249.166.197, 13.249.166.231, ...\n",
      "Connecting to d2dcan0armyq93.cloudfront.net (d2dcan0armyq93.cloudfront.net)|13.249.166.109|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 34341 (34K) [image/jpeg]\n",
      "Saving to: ‘test_image.jpg’\n",
      "\n",
      "test_image.jpg      100%[===================>]  33.54K  --.-KB/s    in 0.001s  \n",
      "\n",
      "2024-12-05 15:49:52 (50.9 MB/s) - ‘test_image.jpg’ saved [34341/34341]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://d2dcan0armyq93.cloudfront.net/photo/odai/600/a7d1e26dc93ee0a4d3b16f80dd0954ea_600.jpg -O test_image.jpg\n",
    "test_image_path = \"test_image.jpg\"\n",
    "test_sentence = \"こいつ誰だ\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-12-05 15:49:22--  https://d2dcan0armyq93.cloudfront.net/photo/odai/600/fd8eb8d243d6fe080067d9483ede42fe_600.jpg\n",
      "Resolving d2dcan0armyq93.cloudfront.net (d2dcan0armyq93.cloudfront.net)... 13.249.166.109, 13.249.166.197, 13.249.166.231, ...\n",
      "Connecting to d2dcan0armyq93.cloudfront.net (d2dcan0armyq93.cloudfront.net)|13.249.166.109|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 42570 (42K) [image/jpeg]\n",
      "Saving to: ‘test_image.jpg’\n",
      "\n",
      "test_image.jpg      100%[===================>]  41.57K  --.-KB/s    in 0.004s  \n",
      "\n",
      "2024-12-05 15:49:22 (10.8 MB/s) - ‘test_image.jpg’ saved [42570/42570]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://d2dcan0armyq93.cloudfront.net/photo/odai/600/fd8eb8d243d6fe080067d9483ede42fe_600.jpg -O test_image.jpg\n",
    "test_image_path = \"test_image.jpg\"\n",
    "test_sentence = \"はい、アーンして\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-12-05 15:04:58--  https://d2dcan0armyq93.cloudfront.net/photo/odai/600/89ed8f0f21e303e5c76e77ecf81c88e9_600.jpg\n",
      "Resolving d2dcan0armyq93.cloudfront.net (d2dcan0armyq93.cloudfront.net)... 13.249.166.197, 13.249.166.90, 13.249.166.231, ...\n",
      "Connecting to d2dcan0armyq93.cloudfront.net (d2dcan0armyq93.cloudfront.net)|13.249.166.197|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 35766 (35K) [image/jpeg]\n",
      "Saving to: ‘test_image.jpg’\n",
      "\n",
      "test_image.jpg      100%[===================>]  34.93K  --.-KB/s    in 0s      \n",
      "\n",
      "2024-12-05 15:04:58 (287 MB/s) - ‘test_image.jpg’ saved [35766/35766]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://d2dcan0armyq93.cloudfront.net/photo/odai/600/89ed8f0f21e303e5c76e77ecf81c88e9_600.jpg -O test_image.jpg\n",
    "test_image_path = \"test_image.jpg\"\n",
    "test_sentence = \"ゴジラになれますように\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-12-05 15:32:01--  https://www.shonan-it.ac.jp/media/20241126_g03.jpg\n",
      "Resolving www.shonan-it.ac.jp (www.shonan-it.ac.jp)... 150.60.144.101, 10.2.10.10, 10.2.10.20\n",
      "Connecting to www.shonan-it.ac.jp (www.shonan-it.ac.jp)|150.60.144.101|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 56759 (55K) [image/jpeg]\n",
      "Saving to: ‘test_image.jpg’\n",
      "\n",
      "test_image.jpg      100%[===================>]  55.43K  --.-KB/s    in 0.04s   \n",
      "\n",
      "2024-12-05 15:32:01 (1.48 MB/s) - ‘test_image.jpg’ saved [56759/56759]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://www.shonan-it.ac.jp/media/20241126_g03.jpg -O test_image.jpg\n",
    "test_image_path = \"test_image.jpg\"\n",
    "test_sentence = \"AIはボケられるのか⁉ ～AIを用いた画像に対する大喜利生成～\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-12-05 15:33:59--  https://www.shonan-it.ac.jp/media/20241128_g01.jpg\n",
      "Resolving www.shonan-it.ac.jp (www.shonan-it.ac.jp)... 150.60.144.101, 10.2.10.10, 10.2.10.20\n",
      "Connecting to www.shonan-it.ac.jp (www.shonan-it.ac.jp)|150.60.144.101|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 46813 (46K) [image/jpeg]\n",
      "Saving to: ‘test_image.jpg’\n",
      "\n",
      "test_image.jpg      100%[===================>]  45.72K  --.-KB/s    in 0.04s   \n",
      "\n",
      "2024-12-05 15:33:59 (1.25 MB/s) - ‘test_image.jpg’ saved [46813/46813]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://www.shonan-it.ac.jp/media/20241128_g01.jpg -O test_image.jpg\n",
    "test_image_path = \"test_image.jpg\"\n",
    "test_sentence = \"AIはボケられるのか⁉ ～AIを用いた画像に対する大喜利生成～\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-12-05 15:56:14--  https://www.shonan-it.ac.jp/media/20241204_w01.jpg\n",
      "Resolving www.shonan-it.ac.jp (www.shonan-it.ac.jp)... 150.60.144.101, 10.2.10.10, 10.2.10.20\n",
      "Connecting to www.shonan-it.ac.jp (www.shonan-it.ac.jp)|150.60.144.101|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 77508 (76K) [image/jpeg]\n",
      "Saving to: ‘test_image.jpg’\n",
      "\n",
      "test_image.jpg      100%[===================>]  75.69K  --.-KB/s    in 0.05s   \n",
      "\n",
      "2024-12-05 15:56:14 (1.37 MB/s) - ‘test_image.jpg’ saved [77508/77508]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://www.shonan-it.ac.jp/media/20241204_w01.jpg -O test_image.jpg\n",
    "test_image_path = \"test_image.jpg\"\n",
    "test_sentence = \"自宅の冷蔵庫を開けると、そこにはりんごとみかんとぶどうがありました\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-12-05 15:54:45--  https://d2dcan0armyq93.cloudfront.net/photo/odai/600/c42184648bce2e291c7ccc36495b34e7_600.jpg\n",
      "Resolving d2dcan0armyq93.cloudfront.net (d2dcan0armyq93.cloudfront.net)... 13.249.166.109, 13.249.166.231, 13.249.166.90, ...\n",
      "Connecting to d2dcan0armyq93.cloudfront.net (d2dcan0armyq93.cloudfront.net)|13.249.166.109|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 29743 (29K) [image/jpeg]\n",
      "Saving to: ‘test_image.jpg’\n",
      "\n",
      "test_image.jpg      100%[===================>]  29.05K  --.-KB/s    in 0.001s  \n",
      "\n",
      "2024-12-05 15:54:45 (39.0 MB/s) - ‘test_image.jpg’ saved [29743/29743]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://d2dcan0armyq93.cloudfront.net/photo/odai/600/c42184648bce2e291c7ccc36495b34e7_600.jpg -O test_image.jpg\n",
    "test_image_path = \"test_image.jpg\"\n",
    "test_sentence = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-89-2fce6cdfc351>:8: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(f\"{RESULT_DIR}model_weights.pth\"))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0.4938]], device='cuda:0')"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EXPERIENCE_NUMBER = \"003\"\n",
    "RESULT_DIR = f\"../../results/Boke_Judge/{EXPERIENCE_NUMBER}/\"\n",
    "model_path = f\"{RESULT_DIR}model.weights.pth\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = CustomModel(512, 512, 768)\n",
    "\n",
    "model.load_state_dict(torch.load(f\"{RESULT_DIR}model_weights.pth\"))\n",
    "model.eval()\n",
    "model.to(device)\n",
    "\n",
    "encoded_sentences = ja_clip.tokenize(\n",
    "        texts = [test_sentence],\n",
    "        max_seq_len = 77,\n",
    "        device = device,\n",
    "        tokenizer = clip_tokenizer,\n",
    "    )\n",
    "image = Image.open(test_image_path)\n",
    "preprcessed_image = clip_preprocesser(image).unsqueeze(0).to(device)\n",
    "with torch.no_grad():\n",
    "    clip_test_image_features = clip_model.get_image_features(preprcessed_image)\n",
    "    clip_test_image_feature = clip_test_image_features.cpu().numpy()[0]\n",
    "    clip_test_sentence_features = clip_model.get_text_features(**encoded_sentences)\n",
    "    clip_test_sentence_feature = clip_test_sentence_features.cpu().numpy()[0]\n",
    "\n",
    "luke_test_sentence_feature = luke_model.encode([test_sentence]).cpu().numpy()[0]\n",
    "\n",
    "with torch.no_grad():\n",
    "    clip_test_image_feature = torch.tensor(clip_test_image_feature[np.newaxis])\n",
    "    clip_test_sentence_feature = torch.tensor(clip_test_sentence_feature[np.newaxis])\n",
    "    luke_test_sentence_feature = torch.tensor(luke_test_sentence_feature[np.newaxis])\n",
    "\n",
    "    clip_test_image_feature = clip_test_image_feature.to(device)\n",
    "    clip_test_sentence_feature = clip_test_sentence_feature.to(device)\n",
    "    luke_test_sentence_feature = luke_test_sentence_feature.to(device)\n",
    "\n",
    "    outputs = model(clip_test_image_feature,\n",
    "                    clip_test_sentence_feature,\n",
    "                    luke_test_sentence_feature)\n",
    "\n",
    "outputs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Colab_20241111",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
