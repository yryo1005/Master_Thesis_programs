{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import argparse\n",
    "import matplotlib.pyplot as plt\n",
    "import japanize_matplotlib\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import japanese_clip as ja_clip\n",
    "from transformers import MLukeTokenizer, LukeModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BokeJudgeModel(nn.Module):\n",
    "    def __init__(self, cif_dim = 512, csf_dim = 512, lsf_dim = 768, feature_dim = 1024):\n",
    "        \"\"\"\n",
    "            cif_dim: CLIPの画像の特徴量の次元数\n",
    "            csf_dim: CLIPの文章の特徴量の次元数\n",
    "            lsf_dim: Sentene-LUKEの文章の特徴量の次元数\n",
    "        \"\"\"\n",
    "        super(BokeJudgeModel, self).__init__()\n",
    "        self.cif_dim = cif_dim\n",
    "        self.csf_dim = csf_dim\n",
    "        self.lsf_dim = lsf_dim\n",
    "        \n",
    "        self.fc1 = nn.Linear(cif_dim + csf_dim + lsf_dim, feature_dim)\n",
    "        self.fc2 = nn.Linear(feature_dim, feature_dim)\n",
    "        self.fc3 = nn.Linear(feature_dim, feature_dim)\n",
    "        self.output_layer = nn.Linear(feature_dim, 1)\n",
    "        \n",
    "    def forward(self, cif, csf, lsf):\n",
    "        \"\"\"\n",
    "            cif: CLIPの画像の特徴量\n",
    "            csf: CLIPの文章の特徴量\n",
    "            lsf: Sentence-LUKEの文章の特徴量\n",
    "        \"\"\"\n",
    "        x = torch.cat([cif, csf, lsf], dim = 1)\n",
    "\n",
    "        x = F.leaky_relu(self.fc1(x))\n",
    "        x = F.leaky_relu(self.fc2(x))\n",
    "        x = F.leaky_relu(self.fc3(x))\n",
    "\n",
    "        output = torch.sigmoid(self.output_layer(x))\n",
    "        return output\n",
    "\n",
    "class SentenceLukeJapanese:\n",
    "    def __init__(self, device = None):\n",
    "        self.tokenizer = MLukeTokenizer.from_pretrained(\"sonoisa/sentence-luke-japanese-base-lite\")\n",
    "        self.model = LukeModel.from_pretrained(\"sonoisa/sentence-luke-japanese-base-lite\",\n",
    "                                               torch_dtype = torch.float16)\n",
    "        self.model.eval()\n",
    "\n",
    "        if device is None:\n",
    "            device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        self.device = torch.device(device)\n",
    "        self.model.to(device)\n",
    "\n",
    "    def _mean_pooling(self, model_output, attention_mask):\n",
    "        token_embeddings = model_output[0] #First element of model_output contains all token embeddings\n",
    "        input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "        return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def encode(self, sentences, batch_size = 256):\n",
    "        all_embeddings = []\n",
    "        iterator = range(0, len(sentences), batch_size)\n",
    "        for batch_idx in iterator:\n",
    "            batch = sentences[batch_idx:batch_idx + batch_size]\n",
    "\n",
    "            encoded_input = self.tokenizer.batch_encode_plus(batch, padding=\"longest\",\n",
    "                                           truncation=True, return_tensors=\"pt\").to(self.device)\n",
    "            model_output = self.model(**encoded_input)\n",
    "            sentence_embeddings = self._mean_pooling(model_output, encoded_input[\"attention_mask\"]).to('cpu')\n",
    "\n",
    "            all_embeddings.extend(sentence_embeddings)\n",
    "\n",
    "        return torch.stack(all_embeddings)\n",
    "\n",
    "class BokeJugeAI:\n",
    "    def __init__(self, weight_path, feature_dim):\n",
    "        \"\"\"\n",
    "            weight_path: 大喜利適合判定モデルの学習済みの重みのパス\n",
    "        \"\"\"\n",
    "        # 大喜利適合判定AIの読み込み\n",
    "        self.boke_judge_model = BokeJudgeModel(feature_dim = feature_dim)\n",
    "        self.boke_judge_model.load_state_dict(torch.load(weight_path))\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.boke_judge_model.to(self.device)\n",
    "        self.boke_judge_model.eval()\n",
    "\n",
    "        # CLIP\n",
    "        self.clip_model, self.clip_preprocesser = ja_clip.load(\"rinna/japanese-clip-vit-b-16\",\n",
    "                                             cache_dir=\"/tmp/japanese_clip\",\n",
    "                                             torch_dtype = torch.float16,\n",
    "                                             device = self.device)\n",
    "        self.clip_tokenizer = ja_clip.load_tokenizer()\n",
    "\n",
    "        # Sentence-LUKE\n",
    "        self.luke_model = SentenceLukeJapanese()\n",
    "\n",
    "    def __call__(self, image_path, sentence):\n",
    "        \"\"\"\n",
    "            image_path: 判定したい大喜利のお題画像\n",
    "            sentence: 判定したい大喜利\n",
    "        \"\"\"\n",
    "        # CLIPによる特徴量への変換\n",
    "        tokenized_sentences = ja_clip.tokenize(\n",
    "            texts = [sentence],\n",
    "            max_seq_len = 77,\n",
    "            device = self.device,\n",
    "            tokenizer = self.clip_tokenizer,\n",
    "            )\n",
    "        image = Image.open(image_path)\n",
    "        preprcessed_image = self.clip_preprocesser(image).unsqueeze(0).to(self.device)\n",
    "        with torch.no_grad():\n",
    "            clip_image_features = self.clip_model.get_image_features(preprcessed_image)\n",
    "            clip_sentence_features = self.clip_model.get_text_features(**tokenized_sentences)\n",
    "\n",
    "        # Sentence-LUKEによる特徴量への変換\n",
    "        luke_sentence_feature = self.luke_model.encode([sentence])\n",
    "\n",
    "        # 大喜利適合判定AIの推論\n",
    "        with torch.no_grad():\n",
    "            outputs = self.boke_judge_model(clip_image_features,\n",
    "                                        clip_sentence_features,\n",
    "                                        luke_sentence_feature.to(self.device))\n",
    "\n",
    "        return outputs.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-2-25a8a327134a>:73: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.boke_judge_model.load_state_dict(torch.load(weight_path))\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/home/user/workspace/Master_Thesis/results/Boke_Judge/False_False_False_True_True_1_15_64_0.0001_1024/best_model.pth'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-0a602b4cf8f7>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m boke_judge_AI = BokeJugeAI(weight_path = \"/home/user/workspace/Master_Thesis/results/Boke_Judge/False_False_False_True_True_1_15_64_0.0001_1024/best_model.pth\", \n\u001b[0m\u001b[1;32m      2\u001b[0m                            feature_dim = 1024)\n",
      "\u001b[0;32m<ipython-input-2-25a8a327134a>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, weight_path, feature_dim)\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[0;31m# 大喜利適合判定AIの読み込み\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mboke_judge_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBokeJudgeModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeature_dim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeature_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mboke_judge_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cuda\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"cpu\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mboke_judge_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/user/.conda/envs/Colab_20241111/lib/python3.10/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1317\u001b[0m         \u001b[0mpickle_load_args\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"encoding\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1319\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1320\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_is_zipfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m             \u001b[0;31m# The zipfile reader is going to advance the current file position.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/user/.conda/envs/Colab_20241111/lib/python3.10/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    657\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    658\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_is_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 659\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    660\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    661\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m\"w\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/user/.conda/envs/Colab_20241111/lib/python3.10/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    638\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_opener\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    639\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 640\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    641\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    642\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/home/user/workspace/Master_Thesis/results/Boke_Judge/False_False_False_True_True_1_15_64_0.0001_1024/best_model.pth'"
     ]
    }
   ],
   "source": [
    "boke_judge_AI = BokeJugeAI(weight_path = \"/home/user/workspace/Master_Thesis/results/Boke_Judge/False_False_False_True_True_1_15_64_0.0001_1024/best_model.pth\", \n",
    "                           feature_dim = 1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"generated_ohgiri.json\", \"r\") as f:\n",
    "    generated_ohgiri = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "methods = [\n",
    "    \"human\",\n",
    "    \"caption\",\n",
    "    \"Neural_Joking_Machine\",\n",
    "    \"GUMI_AE\",\n",
    "    \"GUMI_AMAE_1.0\",\n",
    "    \"GUMI_AMAE_10.0\",\n",
    "    \"GUMI_AMAE_100.0\",\n",
    "    \"GUMI_AMAE_1000.0\",\n",
    "    \"GUMI_AMAE_10000.0\",\n",
    "    \"GUMI_T_3\"\n",
    "]\n",
    "\n",
    "IMAGE_DIR = \"../../datas/boke_image/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:04<00:00, 48.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "human\n",
      "0.66973084 0.041310646\n",
      "==========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:03<00:00, 53.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "caption\n",
      "0.008578465 0.0043417197\n",
      "==========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:03<00:00, 52.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural_Joking_Machine\n",
      "0.5563391 0.06729085\n",
      "==========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:03<00:00, 53.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GUMI_AE\n",
      "0.43245643 0.061992027\n",
      "==========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:03<00:00, 52.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GUMI_AMAE_1.0\n",
      "0.48513505 0.07024258\n",
      "==========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:03<00:00, 54.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GUMI_AMAE_10.0\n",
      "0.4163568 0.038962036\n",
      "==========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:03<00:00, 52.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GUMI_AMAE_100.0\n",
      "0.45307162 0.05997465\n",
      "==========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:03<00:00, 53.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GUMI_AMAE_1000.0\n",
      "0.4772019 0.06377521\n",
      "==========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:03<00:00, 52.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GUMI_AMAE_10000.0\n",
      "0.43219215 0.050536383\n",
      "==========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:03<00:00, 52.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GUMI_T_3\n",
      "0.6949591 0.058050375\n",
      "==========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for M in methods:\n",
    "    \n",
    "    preds = list()\n",
    "    for O in tqdm(generated_ohgiri):\n",
    "        image_path = f\"{IMAGE_DIR}{O['image_number']}.jpg\"\n",
    "        pred = boke_judge_AI(image_path, O[M])\n",
    "        preds.append(pred)\n",
    "\n",
    "    print(M)\n",
    "    print(np.average(preds), np.var(preds))\n",
    "    print(\"=\" * 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:04<00:00, 47.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6855812 0.04377045\n",
      "0.6538805 0.03834836\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "preds_high = list()\n",
    "preds_low = list()\n",
    "preds = list()\n",
    "stars = list()\n",
    "for O in tqdm(generated_ohgiri):\n",
    "    image_path = f\"{IMAGE_DIR}{O['image_number']}.jpg\"\n",
    "    pred = boke_judge_AI(image_path, O[\"human\"])\n",
    "    \n",
    "    if O[\"star\"] > 10:\n",
    "        preds_high.append(pred)\n",
    "    else:\n",
    "        preds_low.append(pred)\n",
    "\n",
    "    preds.append(pred)\n",
    "    stars.append(O[\"star\"])\n",
    "\n",
    "print(np.average(preds_high), np.var(preds_high))\n",
    "print(np.average(preds_low), np.var(preds_low))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x7f8755776950>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot()\n",
    "ax.scatter(preds, stars)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
