{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import shutil\n",
    "\n",
    "# shutil.rmtree(\"../datas/encoded/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "import numpy as np\n",
    "import japanese_clip as ja_clip\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from transformers import MLukeTokenizer, LukeModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "clip_model, clip_preprocesser = ja_clip.load(\"rinna/japanese-clip-vit-b-16\", \n",
    "                                             cache_dir=\"/tmp/japanese_clip\", \n",
    "                                             torch_dtype = torch.float16,\n",
    "                                             device = device)\n",
    "clip_tokenizer = ja_clip.load_tokenizer()\n",
    "\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentenceLukeJapanese:\n",
    "    def __init__(self, device = None):\n",
    "        self.tokenizer = MLukeTokenizer.from_pretrained(\"sonoisa/sentence-luke-japanese-base-lite\")\n",
    "        self.model = LukeModel.from_pretrained(\"sonoisa/sentence-luke-japanese-base-lite\",\n",
    "                                               torch_dtype = torch.float16)\n",
    "        self.model.eval()\n",
    "\n",
    "        if device is None:\n",
    "            device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        self.device = torch.device(device)\n",
    "        self.model.to(device)\n",
    "\n",
    "    def _mean_pooling(self, model_output, attention_mask):\n",
    "        token_embeddings = model_output[0] #First element of model_output contains all token embeddings\n",
    "        input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "        return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def encode(self, sentences, batch_size = 256):\n",
    "        all_embeddings = []\n",
    "        iterator = range(0, len(sentences), batch_size)\n",
    "        for batch_idx in iterator:\n",
    "            batch = sentences[batch_idx:batch_idx + batch_size]\n",
    "\n",
    "            encoded_input = self.tokenizer.batch_encode_plus(batch, padding=\"longest\",\n",
    "                                           truncation=True, return_tensors=\"pt\").to(self.device)\n",
    "            model_output = self.model(**encoded_input)\n",
    "            sentence_embeddings = self._mean_pooling(model_output, encoded_input[\"attention_mask\"]).to('cpu')\n",
    "\n",
    "            all_embeddings.extend(sentence_embeddings)\n",
    "\n",
    "        return torch.stack(all_embeddings)\n",
    "\n",
    "luke_model = SentenceLukeJapanese()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(668970, 668982)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DATA_DIR = \"../datas/boke_data_assemble/\"\n",
    "IMAGE_DIR = \"../datas/boke_image/\"\n",
    "\n",
    "TARGET_DIR = \"../datas/encoded/\"\n",
    "TARGET_CLIP_IMAGE_FEATURE_DIR = f\"{TARGET_DIR}clip_image_feature/\"\n",
    "TARGET_CLIP_SENTENCE_FEATURE_DIR = f\"{TARGET_DIR}clip_sentence_feature/\"\n",
    "TARGET_CLIP_BOKE_FEATURE_DIR = f\"{TARGET_DIR}clip_sentence_feature/boke/\"\n",
    "TARGET_CLIP_CAPTION_FEATURE_DIR = f\"{TARGET_DIR}clip_sentence_feature/caption/\"\n",
    "TARGET_LUKE_SENTENCE_FEATURE_DIR = f\"{TARGET_DIR}luke_sentence_feature/\"\n",
    "TARGET_LUKE_BOKE_FEATURE_DIR = f\"{TARGET_DIR}luke_sentence_feature/boke/\"\n",
    "TARGET_LUKE_CAPTION_FEATURE_DIR = f\"{TARGET_DIR}luke_sentence_feature/caption/\"\n",
    "\n",
    "tmp = [TARGET_DIR, \n",
    "       TARGET_CLIP_IMAGE_FEATURE_DIR, \n",
    "       TARGET_CLIP_SENTENCE_FEATURE_DIR, TARGET_CLIP_BOKE_FEATURE_DIR, TARGET_CLIP_CAPTION_FEATURE_DIR,\n",
    "       TARGET_LUKE_SENTENCE_FEATURE_DIR, TARGET_LUKE_BOKE_FEATURE_DIR, TARGET_LUKE_CAPTION_FEATURE_DIR]\n",
    "for D in tmp:\n",
    "    if not os.path.exists(D): os.mkdir(D)\n",
    "\n",
    "len(os.listdir(DATA_DIR)), len(os.listdir(IMAGE_DIR))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 272/668970 [00:08<5:25:23, 34.25it/s]"
     ]
    }
   ],
   "source": [
    "for JP in tqdm(os.listdir(DATA_DIR)):\n",
    "    N = int(JP.split(\".\")[0])\n",
    "    if os.path.exists(f\"{TARGET_LUKE_SENTENCE_FEATURE_DIR}2{N:07}.npy\"):\n",
    "        continue\n",
    "    if N == 797222: continue\n",
    "\n",
    "    image_path = f\"{IMAGE_DIR}{N}.jpg\"\n",
    "    if not os.path.exists(image_path): continue\n",
    "\n",
    "    with open(f\"{DATA_DIR}{JP}\", \"r\") as f:\n",
    "        a = json.load(f)\n",
    "\n",
    "    bokes = [A[\"boke\"] for A in a[\"bokes\"]]\n",
    "    caption = a[\"image_information\"][\"ja_caption\"]\n",
    "    sentences = bokes + [caption]\n",
    "\n",
    "    encoded_sentences = ja_clip.tokenize(\n",
    "        texts = sentences,\n",
    "        max_seq_len = 77,\n",
    "        device = device,\n",
    "        tokenizer = clip_tokenizer,\n",
    "    )\n",
    "    image = Image.open(image_path)\n",
    "    preprcessed_image = clip_preprocesser(image).unsqueeze(0).to(device)\n",
    "    with torch.no_grad():\n",
    "        clip_image_feature = clip_model.get_image_features(preprcessed_image)\n",
    "        clip_image_feature = clip_image_feature.cpu().numpy()[0]\n",
    "        clip_sentence_features = clip_model.get_text_features(**encoded_sentences)\n",
    "        clip_sentence_features = clip_sentence_features.cpu().numpy()\n",
    "    clip_boke_features = clip_sentence_features[:-1]\n",
    "    clip_caption_feature = clip_sentence_features[-1]\n",
    "\n",
    "    luke_sentence_features = luke_model.encode(sentences).cpu().numpy()\n",
    "    luke_boke_features = luke_sentence_features[:-1]\n",
    "    luke_caption_feature = luke_sentence_features[-1]\n",
    "\n",
    "    for i in range(len(bokes)):\n",
    "        np.save(f\"{TARGET_CLIP_BOKE_FEATURE_DIR}{N:08}{i:06}\", clip_boke_features[i])\n",
    "        np.save(f\"{TARGET_LUKE_BOKE_FEATURE_DIR}{N:08}{i:06}\", luke_boke_features[i])\n",
    "\n",
    "    np.save(f\"{TARGET_CLIP_IMAGE_FEATURE_DIR}{N:08}\", clip_image_feature)\n",
    "    np.save(f\"{TARGET_CLIP_CAPTION_FEATURE_DIR}{N:08}\", clip_caption_feature)\n",
    "    np.save(f\"{TARGET_LUKE_CAPTION_FEATURE_DIR}{N:08}\", luke_caption_feature)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Colab_20241111",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
