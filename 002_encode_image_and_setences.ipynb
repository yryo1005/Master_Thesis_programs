{
 "cells": [
  {
   "cell_type": "code",
<<<<<<< Updated upstream
   "execution_count": 6,
=======
   "execution_count": 1,
>>>>>>> Stashed changes
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "shutil.rmtree(\"../datas/encoded/\")"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< Updated upstream
   "execution_count": 1,
=======
   "execution_count": 2,
>>>>>>> Stashed changes
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "import numpy as np\n",
    "import japanese_clip as ja_clip\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from transformers import MLukeTokenizer, LukeModel"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< Updated upstream
   "execution_count": 2,
=======
   "execution_count": 3,
>>>>>>> Stashed changes
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
<<<<<<< Updated upstream
     "execution_count": 2,
=======
     "execution_count": 3,
>>>>>>> Stashed changes
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "clip_model, clip_preprocesser = ja_clip.load(\"rinna/japanese-clip-vit-b-16\", \n",
    "                                             cache_dir=\"/tmp/japanese_clip\", \n",
    "                                             torch_dtype = torch.float16,\n",
    "                                             device = device)\n",
    "clip_tokenizer = ja_clip.load_tokenizer()\n",
    "\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< Updated upstream
   "execution_count": 3,
=======
   "execution_count": 4,
>>>>>>> Stashed changes
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a34047b2ad7b49aea348f88255608056",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/2.17k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b4a9396089f408db512fb8d3b41e167",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentencepiece.bpe.model:   0%|          | 0.00/842k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1b57f1f1b864fbeba9f309cde97b365",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "entity_vocab.json:   0%|          | 0.00/62.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98f2c827e6114cb1907d76a5b50d8f02",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/40.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3e8ad5e0baf46278adc48f3a59be649",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/595 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6bc8163bfb7148fdb18bb1219815b92d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/847 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d31153ca741349d586b49b9f531f1749",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/532M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class SentenceLukeJapanese:\n",
    "    def __init__(self, device = None):\n",
    "        self.tokenizer = MLukeTokenizer.from_pretrained(\"sonoisa/sentence-luke-japanese-base-lite\")\n",
    "        self.model = LukeModel.from_pretrained(\"sonoisa/sentence-luke-japanese-base-lite\",\n",
    "                                               torch_dtype = torch.float16)\n",
    "        self.model.eval()\n",
    "\n",
    "        if device is None:\n",
    "            device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        self.device = torch.device(device)\n",
    "        self.model.to(device)\n",
    "\n",
    "    def _mean_pooling(self, model_output, attention_mask):\n",
    "        token_embeddings = model_output[0] #First element of model_output contains all token embeddings\n",
    "        input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "        return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def encode(self, sentences, batch_size = 256):\n",
    "        all_embeddings = []\n",
    "        iterator = range(0, len(sentences), batch_size)\n",
    "        for batch_idx in iterator:\n",
    "            batch = sentences[batch_idx:batch_idx + batch_size]\n",
    "\n",
    "            encoded_input = self.tokenizer.batch_encode_plus(batch, padding=\"longest\",\n",
    "                                           truncation=True, return_tensors=\"pt\").to(self.device)\n",
    "            model_output = self.model(**encoded_input)\n",
    "            sentence_embeddings = self._mean_pooling(model_output, encoded_input[\"attention_mask\"]).to('cpu')\n",
    "\n",
    "            all_embeddings.extend(sentence_embeddings)\n",
    "\n",
    "        return torch.stack(all_embeddings)\n",
    "\n",
    "luke_model = SentenceLukeJapanese()"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< Updated upstream
   "execution_count": 4,
=======
   "execution_count": 5,
>>>>>>> Stashed changes
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(668970, 668982)"
      ]
     },
<<<<<<< Updated upstream
     "execution_count": 4,
=======
     "execution_count": 5,
>>>>>>> Stashed changes
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DATA_DIR = \"../datas/boke_data_assemble/\"\n",
    "IMAGE_DIR = \"../datas/boke_image/\"\n",
    "\n",
    "TARGET_DIR = \"../datas/encoded/\"\n",
    "TARGET_CLIP_IMAGE_FEATURE_DIR = f\"{TARGET_DIR}clip_image_feature/\"\n",
    "TARGET_CLIP_SENTENCE_FEATURE_DIR = f\"{TARGET_DIR}clip_sentence_feature/\"\n",
    "TARGET_LUKE_SENTENCE_FEATURE_DIR = f\"{TARGET_DIR}luke_sentence_feature/\"\n",
    "\n",
    "tmp = [TARGET_DIR, \n",
    "       TARGET_CLIP_IMAGE_FEATURE_DIR, TARGET_CLIP_SENTENCE_FEATURE_DIR,\n",
    "       TARGET_LUKE_SENTENCE_FEATURE_DIR]\n",
    "for D in tmp:\n",
    "    if not os.path.exists(D): os.mkdir(D)\n",
    "\n",
    "len(os.listdir(DATA_DIR)), len(os.listdir(IMAGE_DIR))"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< Updated upstream
   "execution_count": 5,
=======
   "execution_count": 6,
>>>>>>> Stashed changes
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
<<<<<<< Updated upstream
      " 93%|█████████▎| 623011/668970 [8:43:16<38:36, 19.84it/s]   \n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "[Errno 28] No space left on device: '../datas/encoded/clip_sentence_feature/1070953300105.npy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-ea158cc94d86>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbokes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m         \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{TARGET_CLIP_SENTENCE_FEATURE_DIR}1{N:07}{i:05}\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclip_boke_features\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m         \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{TARGET_LUKE_SENTENCE_FEATURE_DIR}1{N:07}{i:05}\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mluke_boke_features\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/user/.conda/envs/Colab_20241111/lib/python3.10/site-packages/numpy/lib/npyio.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(file, arr, allow_pickle, fix_imports)\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.npy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    541\u001b[0m             \u001b[0mfile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfile\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'.npy'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 542\u001b[0;31m         \u001b[0mfile_ctx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"wb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    543\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    544\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mfile_ctx\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfid\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: [Errno 28] No space left on device: '../datas/encoded/clip_sentence_feature/1070953300105.npy'"
=======
      " 58%|█████▊    | 388993/668970 [4:45:12<3:44:23, 20.80it/s] "
>>>>>>> Stashed changes
     ]
    }
   ],
   "source": [
    "for JP in tqdm(os.listdir(DATA_DIR)):\n",
    "    N = int(JP.split(\".\")[0])\n",
    "    if os.path.exists(f\"{TARGET_LUKE_SENTENCE_FEATURE_DIR}2{N:07}.npy\"):\n",
    "        continue\n",
    "\n",
    "    image_path = f\"{IMAGE_DIR}{N}.jpg\"\n",
    "    if not os.path.exists(image_path): continue\n",
    "\n",
    "    with open(f\"{DATA_DIR}{JP}\", \"r\") as f:\n",
    "        a = json.load(f)\n",
    "\n",
    "    bokes = [A[\"boke\"] for A in a[\"bokes\"]]\n",
    "    caption = a[\"image_information\"][\"ja_caption\"]\n",
    "    sentences = bokes + [caption]\n",
    "\n",
    "    encoded_sentences = ja_clip.tokenize(\n",
    "        texts = sentences,\n",
    "        max_seq_len = 77,\n",
    "        device = device,\n",
    "        tokenizer = clip_tokenizer,\n",
    "    )\n",
    "    image = Image.open(image_path)\n",
    "    preprcessed_image = clip_preprocesser(image).unsqueeze(0).to(device)\n",
    "    with torch.no_grad():\n",
    "        clip_image_feature = clip_model.get_image_features(preprcessed_image)\n",
    "        clip_image_feature = clip_image_feature.cpu().numpy()[0]\n",
    "        clip_sentence_features = clip_model.get_text_features(**encoded_sentences)\n",
    "        clip_sentence_features = clip_sentence_features.cpu().numpy()\n",
    "    clip_boke_features = clip_sentence_features[:-1]\n",
    "    clip_caption_feature = clip_sentence_features[-1]\n",
    "\n",
    "    luke_sentence_features = luke_model.encode(sentences).cpu().numpy()\n",
    "    luke_boke_features = luke_sentence_features[:-1]\n",
    "    luke_caption_feature = luke_sentence_features[-1]\n",
    "\n",
    "    for i in range(len(bokes)):\n",
    "        np.save(f\"{TARGET_CLIP_SENTENCE_FEATURE_DIR}1{N:07}{i:05}\", clip_boke_features[i])\n",
    "        np.save(f\"{TARGET_LUKE_SENTENCE_FEATURE_DIR}1{N:07}{i:05}\", luke_boke_features[i])\n",
    "\n",
    "    np.save(f\"{TARGET_CLIP_IMAGE_FEATURE_DIR}3{N:07}\", clip_image_feature)\n",
    "    np.save(f\"{TARGET_CLIP_SENTENCE_FEATURE_DIR}2{N:07}\", clip_caption_feature)\n",
    "    np.save(f\"{TARGET_LUKE_SENTENCE_FEATURE_DIR}2{N:07}\", luke_caption_feature)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Colab_20241111",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
