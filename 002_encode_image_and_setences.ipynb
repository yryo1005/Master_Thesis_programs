{
 "cells": [
  {
   "cell_type": "code",
<<<<<<< Updated upstream
   "execution_count": 30,
=======
   "execution_count": 7,
>>>>>>> Stashed changes
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "shutil.rmtree(\"../datas/encoded/\")"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< Updated upstream
   "execution_count": 22,
=======
   "execution_count": 2,
>>>>>>> Stashed changes
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "import numpy as np\n",
    "import japanese_clip as ja_clip\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from transformers import MLukeTokenizer, LukeModel"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< Updated upstream
   "execution_count": 23,
=======
   "execution_count": 3,
>>>>>>> Stashed changes
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
<<<<<<< Updated upstream
       "model_id": "99a9d9c1636144cbb4f9f5998f16f852",
=======
       "model_id": "3385dc1161ea4dd78bf53cab64cde379",
>>>>>>> Stashed changes
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
<<<<<<< Updated upstream
       "tokenizer_config.json:   0%|          | 0.00/259 [00:00<?, ?B/s]"
=======
       "config.json:   0%|          | 0.00/4.24k [00:00<?, ?B/s]"
>>>>>>> Stashed changes
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
<<<<<<< Updated upstream
       "model_id": "f71634b7501146e1b1fbeccda1856f65",
=======
       "model_id": "05368e6742214f1589d110727cd2b30b",
>>>>>>> Stashed changes
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
<<<<<<< Updated upstream
       "spiece.model:   0%|          | 0.00/806k [00:00<?, ?B/s]"
=======
       "model.safetensors:   0%|          | 0.00/787M [00:00<?, ?B/s]"
>>>>>>> Stashed changes
      ]
     },
     "metadata": {},
     "output_type": "display_data"
<<<<<<< Updated upstream
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a97a8e4e3f4d43cfa573c593470bd508",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/153 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
=======
>>>>>>> Stashed changes
    },
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
<<<<<<< Updated upstream
     "execution_count": 23,
=======
     "execution_count": 3,
>>>>>>> Stashed changes
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "clip_model, clip_preprocesser = ja_clip.load(\"rinna/japanese-clip-vit-b-16\", \n",
    "                                             cache_dir=\"/tmp/japanese_clip\", \n",
    "                                             torch_dtype = torch.float16,\n",
    "                                             device = device)\n",
    "clip_tokenizer = ja_clip.load_tokenizer()\n",
    "\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< Updated upstream
   "execution_count": 24,
=======
   "execution_count": 4,
>>>>>>> Stashed changes
   "metadata": {},
<<<<<<< Updated upstream
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ce58436a26548d19e943bc5c5a724e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/2.17k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74c7f0fd8fb145efa4b196f6aa6a0614",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentencepiece.bpe.model:   0%|          | 0.00/842k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dbe7af75bd194b97897f4f7909cb9781",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "entity_vocab.json:   0%|          | 0.00/62.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "674139c6779b48a18235691b42c2afd1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/40.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c68f2968be04f80b45238be3876a9ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/595 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8b771029cfb46dda3cacd586ecccea9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/847 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34b78e4f07594e5f9ea45e7769a9811e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/532M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
=======
   "outputs": [],
>>>>>>> Stashed changes
   "source": [
    "class SentenceLukeJapanese:\n",
    "    def __init__(self, device = None):\n",
    "        self.tokenizer = MLukeTokenizer.from_pretrained(\"sonoisa/sentence-luke-japanese-base-lite\")\n",
    "        self.model = LukeModel.from_pretrained(\"sonoisa/sentence-luke-japanese-base-lite\",\n",
    "                                               torch_dtype = torch.float16)\n",
    "        self.model.eval()\n",
    "\n",
    "        if device is None:\n",
    "            device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        self.device = torch.device(device)\n",
    "        self.model.to(device)\n",
    "\n",
    "    def _mean_pooling(self, model_output, attention_mask):\n",
    "        token_embeddings = model_output[0] #First element of model_output contains all token embeddings\n",
    "        input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "        return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def encode(self, sentences, batch_size = 256):\n",
    "        all_embeddings = []\n",
    "        iterator = range(0, len(sentences), batch_size)\n",
    "        for batch_idx in iterator:\n",
    "            batch = sentences[batch_idx:batch_idx + batch_size]\n",
    "\n",
    "            encoded_input = self.tokenizer.batch_encode_plus(batch, padding=\"longest\",\n",
    "                                           truncation=True, return_tensors=\"pt\").to(self.device)\n",
    "            model_output = self.model(**encoded_input)\n",
    "            sentence_embeddings = self._mean_pooling(model_output, encoded_input[\"attention_mask\"]).to('cpu')\n",
    "\n",
    "            all_embeddings.extend(sentence_embeddings)\n",
    "\n",
    "        return torch.stack(all_embeddings)\n",
    "\n",
    "luke_model = SentenceLukeJapanese()"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< Updated upstream
<<<<<<< Updated upstream
   "execution_count": 4,
=======
   "execution_count": 25,
>>>>>>> Stashed changes
=======
   "execution_count": 5,
>>>>>>> Stashed changes
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(668970, 668982)"
      ]
     },
<<<<<<< Updated upstream
<<<<<<< Updated upstream
     "execution_count": 4,
=======
     "execution_count": 25,
>>>>>>> Stashed changes
=======
     "execution_count": 5,
>>>>>>> Stashed changes
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DATA_DIR = \"../datas/boke_data_assemble/\"\n",
    "IMAGE_DIR = \"../datas/boke_image/\"\n",
    "\n",
    "TARGET_DIR = \"../datas/encoded/\"\n",
    "TARGET_CLIP_IMAGE_FEATURE_DIR = f\"{TARGET_DIR}clip_image_feature/\"\n",
    "TARGET_CLIP_SENTENCE_FEATURE_DIR = f\"{TARGET_DIR}clip_sentence_feature/\"\n",
    "TARGET_LUKE_SENTENCE_FEATURE_DIR = f\"{TARGET_DIR}luke_sentence_feature/\"\n",
    "\n",
    "tmp = [TARGET_DIR, \n",
    "       TARGET_CLIP_IMAGE_FEATURE_DIR, TARGET_CLIP_SENTENCE_FEATURE_DIR,\n",
    "       TARGET_LUKE_SENTENCE_FEATURE_DIR]\n",
    "for D in tmp:\n",
    "    if not os.path.exists(D): os.mkdir(D)\n",
    "\n",
    "len(os.listdir(DATA_DIR)), len(os.listdir(IMAGE_DIR))"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< Updated upstream
   "execution_count": 5,
=======
   "execution_count": 6,
>>>>>>> Stashed changes
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
<<<<<<< Updated upstream
<<<<<<< Updated upstream
      "100%|██████████| 602566/602566 [6:11:50<00:00, 27.01it/s]   \n"
=======
      " 93%|█████████▎| 620783/668970 [6:08:23<28:35, 28.08it/s]   \n"
=======
      " 98%|█████████▊| 655236/668970 [4:53:05<06:08, 37.26it/s]   \n"
>>>>>>> Stashed changes
     ]
    },
    {
     "ename": "OSError",
<<<<<<< Updated upstream
     "evalue": "[Errno 28] No space left on device: '../datas/encoded/clip_sentence_feature/1069775600003.npy'",
=======
     "evalue": "[Errno 28] No space left on device: '../datas/encoded/clip_sentence_feature/boke/00215587000144.npy'",
>>>>>>> Stashed changes
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
<<<<<<< Updated upstream
      "\u001b[0;32m<ipython-input-5-ea158cc94d86>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbokes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m         \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{TARGET_CLIP_SENTENCE_FEATURE_DIR}1{N:07}{i:05}\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclip_boke_features\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m         \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{TARGET_LUKE_SENTENCE_FEATURE_DIR}1{N:07}{i:05}\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mluke_boke_features\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/user/.conda/envs/Colab_20241111/lib/python3.10/site-packages/numpy/lib/npyio.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(file, arr, allow_pickle, fix_imports)\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.npy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    541\u001b[0m             \u001b[0mfile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfile\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'.npy'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 542\u001b[0;31m         \u001b[0mfile_ctx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"wb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    543\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    544\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mfile_ctx\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfid\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: [Errno 28] No space left on device: '../datas/encoded/clip_sentence_feature/1069775600003.npy'"
>>>>>>> Stashed changes
=======
      "\u001b[0;32m<ipython-input-6-fe0af021b18a>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbokes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m         \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{TARGET_CLIP_BOKE_FEATURE_DIR}{N:08}{i:06}\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclip_boke_features\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m         \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{TARGET_LUKE_BOKE_FEATURE_DIR}{N:08}{i:06}\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mluke_boke_features\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/user/.conda/envs/Colab_20241111/lib/python3.10/site-packages/numpy/lib/npyio.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(file, arr, allow_pickle, fix_imports)\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.npy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    541\u001b[0m             \u001b[0mfile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfile\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'.npy'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 542\u001b[0;31m         \u001b[0mfile_ctx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"wb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    543\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    544\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mfile_ctx\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfid\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: [Errno 28] No space left on device: '../datas/encoded/clip_sentence_feature/boke/00215587000144.npy'"
>>>>>>> Stashed changes
     ]
    }
   ],
   "source": [
    "for JP in tqdm(os.listdir(DATA_DIR)):\n",
    "    N = int(JP.split(\".\")[0])\n",
    "    if os.path.exists(f\"{TARGET_LUKE_SENTENCE_FEATURE_DIR}2{N:07}.npy\"):\n",
    "        continue\n",
    "\n",
    "    image_path = f\"{IMAGE_DIR}{N}.jpg\"\n",
    "    if not os.path.exists(image_path): continue\n",
    "\n",
    "    with open(f\"{DATA_DIR}{JP}\", \"r\") as f:\n",
    "        a = json.load(f)\n",
    "\n",
    "    bokes = [A[\"boke\"] for A in a[\"bokes\"]]\n",
    "    caption = a[\"image_information\"][\"ja_caption\"]\n",
    "    sentences = bokes + [caption]\n",
    "\n",
    "    encoded_sentences = ja_clip.tokenize(\n",
    "        texts = sentences,\n",
    "        max_seq_len = 77,\n",
    "        device = device,\n",
    "        tokenizer = clip_tokenizer,\n",
    "    )\n",
    "    image = Image.open(image_path)\n",
    "    preprcessed_image = clip_preprocesser(image).unsqueeze(0).to(device)\n",
    "    with torch.no_grad():\n",
    "        clip_image_feature = clip_model.get_image_features(preprcessed_image)\n",
    "        clip_image_feature = clip_image_feature.cpu().numpy()[0]\n",
    "        clip_sentence_features = clip_model.get_text_features(**encoded_sentences)\n",
    "        clip_sentence_features = clip_sentence_features.cpu().numpy()\n",
    "    clip_boke_features = clip_sentence_features[:-1]\n",
    "    clip_caption_feature = clip_sentence_features[-1]\n",
    "\n",
    "    luke_sentence_features = luke_model.encode(sentences).cpu().numpy()\n",
    "    luke_boke_features = luke_sentence_features[:-1]\n",
    "    luke_caption_feature = luke_sentence_features[-1]\n",
    "\n",
    "    for i in range(len(bokes)):\n",
    "        np.save(f\"{TARGET_CLIP_SENTENCE_FEATURE_DIR}1{N:07}{i:05}\", clip_boke_features[i])\n",
    "        np.save(f\"{TARGET_LUKE_SENTENCE_FEATURE_DIR}1{N:07}{i:05}\", luke_boke_features[i])\n",
    "\n",
    "    np.save(f\"{TARGET_CLIP_IMAGE_FEATURE_DIR}3{N:07}\", clip_image_feature)\n",
    "    np.save(f\"{TARGET_CLIP_SENTENCE_FEATURE_DIR}2{N:07}\", clip_caption_feature)\n",
    "    np.save(f\"{TARGET_LUKE_SENTENCE_FEATURE_DIR}2{N:07}\", luke_caption_feature)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Colab_20241111",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
