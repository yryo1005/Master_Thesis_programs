{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 001\n",
    "\n",
    "現実写真のみ、文字なし"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import shutil\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from transformers import ViTImageProcessor, ViTModel\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import subprocess\n",
    "if not os.path.exists(\"Japanese_BPEEncoder_V2\"):\n",
    "    subprocess.run([\"git\", \"clone\", \"https://github.com/tanreinama/Japanese-BPEEncoder_V2.git\", \"Japanese_BPEEncoder_V2\"])\n",
    "from Japanese_BPEEncoder_V2.encode_swe import SWEEncoder_ja"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXPERIENCE_NUMBER = \"001\"\n",
    "\n",
    "# PCによって変更する\n",
    "NUM_WORKERS = 20\n",
    "# データセットが既に存在する場合に，再度作り直すか\n",
    "RESET_DATA = False\n",
    "\n",
    "# 現実写真以外を使用するか\n",
    "USE_UNREAL_IMAGE = False\n",
    "# 文字を含む画像を使用するか\n",
    "USE_WORD_IMAGE = False\n",
    "# 固有名詞を含む大喜利を使用するか\n",
    "USE_UNIQUE_NOUN_BOKE = False\n",
    "\n",
    "# 大喜利の最小の星の数\n",
    "MIN_STAR = 0\n",
    "# 単語の最小出現回数\n",
    "MIN_APPER_WORD = 32\n",
    "# 大喜利の最小単語数\n",
    "MIN_SENTENCE_LENGTH = 4\n",
    "# 大喜利の最大単語数\n",
    "MAX_SENTENCE_LENGTH = 31\n",
    "\n",
    "RESULT_DIR = f\"../../results/GUMI_T/{EXPERIENCE_NUMBER}/\"\n",
    "if not os.path.exists(\"../../results/GUMI_T/\"):\n",
    "    os.mkdir(\"../../results/GUMI_T/\")\n",
    "if not os.path.exists(RESULT_DIR):\n",
    "    os.mkdir(RESULT_DIR)\n",
    "\n",
    "# \n",
    "BATCH_SIZE_FOR_AUTOENCODER = 32\n",
    "EPOCH_FOR_AUTOENCODER = 25\n",
    "LEARNING_RATO_FOR_AUTOENCODER = 0.0001\n",
    "IMAGE_HEIGHT = 128\n",
    "IMAGE_WIDTH = 128\n",
    "IMAGE_FEATURE_DIM = 16384\n",
    "\n",
    "BATCH_SIZE_FOR_GENERATOR = 128\n",
    "EPOCH_FOR_GENERATOR = 25\n",
    "LEARNING_RATO_FOR_GENERATOR = 0.001\n",
    "\n",
    "DATA_DIR = \"../../datas/boke_data_assemble/\"\n",
    "IMAGE_DIR = \"../../datas/boke_image/\"\n",
    "\n",
    "IMAGE_FEATURE_DIR = \"../../datas/encoded/vit_image_feature/\"\n",
    "if not os.path.exists(IMAGE_FEATURE_DIR):\n",
    "    os.mkdir(IMAGE_FEATURE_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 画像の前処理\n",
    "image_preprocess =  ViTImageProcessor.from_pretrained('google/vit-base-patch16-224-in21k')\n",
    "\n",
    "# 画像のデータローダを作る関数\n",
    "def make_image_dataloader(image_paths, batch_size, num_workers = 4):\n",
    "\n",
    "    class LoadImageDataset(Dataset):\n",
    "        def __init__(self, image_paths):\n",
    "            \"\"\"\n",
    "                image_paths: 画像のパスからなるリスト\n",
    "            \"\"\"\n",
    "            self.image_paths = image_paths\n",
    "\n",
    "        def __len__(self):\n",
    "            return len(self.image_paths)\n",
    "\n",
    "        def __getitem__(self, idx):\n",
    "            image = Image.open(image_paths[idx]).convert(\"RGB\")\n",
    "\n",
    "            return image, self.image_paths[idx]\n",
    "    \n",
    "    def collate_fn_tf(batch):\n",
    "        images = image_preprocess(images = [B[0] for B in batch], \n",
    "                                  return_tensors = \"pt\")\n",
    "        image_numbers = [B[1] for B in batch]\n",
    "\n",
    "        return images, image_numbers\n",
    "\n",
    "    print(f\"num data: {len(image_paths)}\")\n",
    "\n",
    "    dataset = LoadImageDataset(image_paths)\n",
    "    dataloader = DataLoader(\n",
    "        dataset, \n",
    "        batch_size = batch_size, \n",
    "        num_workers = num_workers, \n",
    "        collate_fn = collate_fn_tf\n",
    "    )\n",
    "\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "学習に用いる大喜利の数: 2030920\n",
      " 検証に用いる大喜利の数: 20515\n",
      " 使用する画像の数: 244286\n",
      " 単語の種類: 16705\n"
     ]
    }
   ],
   "source": [
    "# データセットの作成\n",
    "if not os.path.exists(f\"{RESULT_DIR}index_to_word.json\") or RESET_DATA:\n",
    "    # tokenizer\n",
    "    with open('Japanese_BPEEncoder_V2/ja-swe32kfix.txt') as f:\n",
    "        bpe = f.read().split('\\n')\n",
    "\n",
    "    with open('Japanese_BPEEncoder_V2/emoji.json') as f:\n",
    "        emoji = json.loads(f.read())\n",
    "\n",
    "    tokenizer = SWEEncoder_ja(bpe, emoji)\n",
    "\n",
    "    tmp = list()\n",
    "    word_count_dict = dict()\n",
    "\n",
    "    for JP in tqdm(os.listdir(DATA_DIR)):\n",
    "        \n",
    "        N = int(JP.split(\".\")[0])\n",
    "\n",
    "        with open(f\"{DATA_DIR}{JP}\", \"r\") as f:\n",
    "            a = json.load(f)\n",
    "        \n",
    "        image_information = a[\"image_information\"]\n",
    "        is_photographic_probability = image_information[\"is_photographic_probability\"]\n",
    "        ocr = image_information[\"ocr\"]\n",
    "\n",
    "        # 現実写真以外を除去\n",
    "        if not USE_UNREAL_IMAGE:\n",
    "            if is_photographic_probability < 0.8: continue\n",
    "            \n",
    "        # 文字のある画像を除去\n",
    "        if not USE_WORD_IMAGE:\n",
    "            if len(ocr) != 0: continue\n",
    "        \n",
    "        bokes = a[\"bokes\"]\n",
    "\n",
    "        for B in bokes:\n",
    "            # 星が既定の数以下の大喜利を除去\n",
    "            if B[\"star\"] < MIN_STAR:\n",
    "                continue\n",
    "\n",
    "            # 固有名詞を含む大喜利を除去\n",
    "            if not USE_UNIQUE_NOUN_BOKE:\n",
    "                if len(B[\"unique_nouns\"]) != 0: continue\n",
    "\n",
    "            tokenized_boke = tokenizer.encode(B[\"boke\"])\n",
    "            # 単語数が既定の数でない大喜利を除去\n",
    "            if not MIN_SENTENCE_LENGTH <= len(tokenized_boke) < MAX_SENTENCE_LENGTH:\n",
    "                continue\n",
    "\n",
    "            for W in tokenized_boke:\n",
    "                try:\n",
    "                    word_count_dict[W] += 1\n",
    "                except:\n",
    "                    word_count_dict[W] = 1\n",
    "            \n",
    "            tmp.append({\n",
    "                \"image_number\": N,\n",
    "                \"tokenized_boke\": tokenized_boke\n",
    "            })\n",
    "\n",
    "    # 単語の最小出現回数を満たさない大喜利を除去\n",
    "    boke_datas = list()\n",
    "    words = list()\n",
    "\n",
    "    for D in tqdm(tmp):\n",
    "        flag = False\n",
    "        for W in D[\"tokenized_boke\"]:\n",
    "            if word_count_dict[W] < MIN_APPER_WORD:\n",
    "                flag = True\n",
    "                break\n",
    "        if flag: \n",
    "            continue\n",
    "        \n",
    "        boke_datas.append({\n",
    "            \"image_number\": D[\"image_number\"],\n",
    "            \"tokenized_boke\": D[\"tokenized_boke\"]\n",
    "        })\n",
    "        words += D[\"tokenized_boke\"]\n",
    "    words = set(words)\n",
    "    image_numbers = list(set([D[\"image_number\"] for D in boke_datas]))\n",
    "    del tmp\n",
    "\n",
    "    # tokenize\n",
    "    index_to_index = dict()\n",
    "\n",
    "    c = 3\n",
    "    for D in tqdm(boke_datas):\n",
    "        tmp = list()\n",
    "        for W in D[\"tokenized_boke\"]:\n",
    "            try:\n",
    "                index_to_index[W]\n",
    "            except:\n",
    "                index_to_index[W] = c\n",
    "                c += 1\n",
    "            tmp.append(index_to_index[W])\n",
    "        D[\"tokenized_boke\"] = [1] + tmp + [2]\n",
    "\n",
    "    index_to_word = {\n",
    "        V: tokenizer.decode([K]) for K, V in index_to_index.items()\n",
    "    }\n",
    "    index_to_word[0] = \"<PAD>\"\n",
    "    index_to_word[1] = \"<START>\"\n",
    "    index_to_word[2] = \"<END>\"\n",
    "\n",
    "    #\n",
    "    train_boke_datas, test_boke_datas = train_test_split(boke_datas, test_size = 0.01)\n",
    "\n",
    "    with open(f\"{RESULT_DIR}train_boke_datas.json\", \"w\") as f:\n",
    "        json.dump(train_boke_datas, f)\n",
    "    with open(f\"{RESULT_DIR}test_boke_datas.json\", \"w\") as f:\n",
    "        json.dump(test_boke_datas, f)\n",
    "    with open(f\"{RESULT_DIR}index_to_word.json\", \"w\") as f:\n",
    "        json.dump(index_to_word, f)\n",
    "\n",
    "else:\n",
    "    with open(f\"{RESULT_DIR}train_boke_datas.json\", \"r\") as f:\n",
    "        train_boke_datas = json.load(f)\n",
    "    with open(f\"{RESULT_DIR}test_boke_datas.json\", \"r\") as f:\n",
    "        test_boke_datas = json.load(f)\n",
    "    with open(f\"{RESULT_DIR}index_to_word.json\", \"r\") as f:\n",
    "        index_to_word = json.load(f)\n",
    "\n",
    "    image_numbers = [D[\"image_number\"] for D in train_boke_datas] + [D[\"image_number\"] for D in test_boke_datas]\n",
    "    image_numbers = list(set(image_numbers))\n",
    "\n",
    "print(f\"学習に用いる大喜利の数: {len(train_boke_datas)}\\n\", \n",
    "      f\"検証に用いる大喜利の数: {len(test_boke_datas)}\\n\",\n",
    "      f\"使用する画像の数: {len(image_numbers)}\\n\",\n",
    "      f\"単語の種類: {len(index_to_word)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 244286/244286 [00:00<00:00, 304848.57it/s]\n"
     ]
    }
   ],
   "source": [
    "# 画像を特徴量に変換する\n",
    "tmp = list()\n",
    "for IN in tqdm(image_numbers):\n",
    "    if os.path.exists(f\"{IMAGE_FEATURE_DIR}{IN}.npy\"):\n",
    "        continue\n",
    "    tmp.append(f\"{IMAGE_DIR}{IN}.jpg\")\n",
    "\n",
    "if len(tmp) != 0:\n",
    "    image_dataloader = make_image_dataloader(tmp, batch_size = 8, num_workers = NUM_WORKERS)\n",
    "\n",
    "    # vision transformer\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = ViTModel.from_pretrained('google/vit-large-patch16-224-in21k')\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    for Is, IPs in tqdm(image_dataloader):\n",
    "        Is = Is.to(device)\n",
    "        outputs = model(**Is)\n",
    "        features = outputs.last_hidden_state.detach().cpu().numpy()\n",
    "\n",
    "        for f, IP in zip(features, IPs):\n",
    "            N = IP.split(\"/\")[-1].split(\".\")[0]\n",
    "            np.save(f\"{IMAGE_FEATURE_DIR}{N}\", f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, max_len, embedding_dim):\n",
    "        \"\"\"\n",
    "            max_len: \n",
    "            embedding_dim: \n",
    "        \"\"\"\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        pe = torch.zeros(max_len, embedding_dim)\n",
    "        position = torch.arange(0, max_len, dtype = torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, embedding_dim, 2).float() * -(torch.log(torch.tensor(10000.0)) / embedding_dim))\n",
    "\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.pe\n",
    "\n",
    "class MultiHeadSelfAttentionBlock(nn.Module):\n",
    "    def __init__(self, embedding_dim, num_heads):\n",
    "        super(MultiHeadSelfAttentionBlock, self).__init__()\n",
    "\n",
    "        self.self_attention = nn.MultiheadAttention(embed_dim = embedding_dim, \n",
    "                                                    num_heads = num_heads, batch_first = True)\n",
    "        self.layer_norm1 = nn.LayerNorm(embedding_dim)\n",
    "\n",
    "        self.fc = nn.Linear(embedding_dim, embedding_dim)\n",
    "        self.layer_norm2 = nn.LayerNorm(embedding_dim)\n",
    "    \n",
    "    def forward(self, x, attn_mask = None):\n",
    "        # self attention\n",
    "        x_dash, _ = self.self_attention(x, x, x, attn_mask = attn_mask)\n",
    "        x = self.layer_norm1(x_dash) + x\n",
    "\n",
    "        # feed forward\n",
    "        x_dash = F.leaky_relu( self.fc(x) )\n",
    "        return self.layer_norm2(x_dash) + x\n",
    "\n",
    "class MultiHeadCrossAttentionBlock(nn.Module):\n",
    "    def __init__(self, embedding_dim, num_heads):\n",
    "        super(MultiHeadCrossAttentionBlock, self).__init__()\n",
    "\n",
    "        self.self_attention = nn.MultiheadAttention(embed_dim = embedding_dim, \n",
    "                                                    num_heads = num_heads, batch_first = True)\n",
    "        self.layer_norm1 = nn.LayerNorm(embedding_dim)\n",
    "\n",
    "        self.cross_attention = nn.MultiheadAttention(embed_dim = embedding_dim, \n",
    "                                                     num_heads = num_heads, batch_first = True)\n",
    "        self.layer_norm2 = nn.LayerNorm(embedding_dim)\n",
    "\n",
    "        self.fc = nn.Linear(embedding_dim, embedding_dim)\n",
    "        self.layer_norm3 = nn.LayerNorm(embedding_dim)\n",
    "    \n",
    "    def forward(self, src, tgt, attn_mask = None):\n",
    "        # self attention\n",
    "        tgt_dash, _ = self.self_attention(tgt, tgt, tgt, attn_mask = attn_mask)\n",
    "        tgt = self.layer_norm1(tgt_dash) + tgt\n",
    "        \n",
    "        # cross attention\n",
    "        src_dash, _ = self.cross_attention(tgt, src, src)\n",
    "        tgt = self.layer_norm2(src_dash) + tgt\n",
    "\n",
    "        # feed forward\n",
    "        tgt_dash = F.leaky_relu( self.fc(tgt) )\n",
    "        return self.layer_norm3(tgt_dash) + tgt\n",
    "\n",
    "# 大喜利生成モデルのクラス\n",
    "class TransformerBokeGeneratorModel(nn.Module):\n",
    "    def __init__(self, num_image_patch, image_feature_dim, num_word, sentence_length, embedding_dim = 512, num_heads = 4):\n",
    "        \"\"\"\n",
    "            num_image_patch\n",
    "            num_word: 学習に用いる単語の総数\n",
    "            image_feature_dim: 画像の特徴量の次元数\n",
    "            sentence_length: 入力する文章の単語数\n",
    "            embedding_dim: 単語の埋め込み次元数\n",
    "            num_heads: \n",
    "        \"\"\"\n",
    "        super(TransformerBokeGeneratorModel, self).__init__()\n",
    "        self.num_word = num_word\n",
    "        self.image_feature_dim = image_feature_dim\n",
    "        self.sentence_length = sentence_length\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "        # image encoder\n",
    "        self.fc1 = nn.Linear(image_feature_dim, embedding_dim)\n",
    "        self.pe1 = PositionalEncoding(num_image_patch, embedding_dim)\n",
    "        self.self_attention = MultiHeadSelfAttentionBlock(embedding_dim = embedding_dim, num_heads = num_heads)\n",
    "        \n",
    "        # sentence decoder\n",
    "        self.embedding = nn.Embedding(num_word, embedding_dim, padding_idx = 0)\n",
    "        self.pe2 = PositionalEncoding(sentence_length, embedding_dim)\n",
    "\n",
    "        self.cross_attention1 = MultiHeadCrossAttentionBlock(embedding_dim = embedding_dim, num_heads = num_heads)\n",
    "        self.cross_attention2 = MultiHeadCrossAttentionBlock(embedding_dim = embedding_dim, num_heads = num_heads)\n",
    "        # Sself.cross_attention3 = MultiHeadCrossAttentionBlock(embedding_dim = embedding_dim, num_heads = num_heads)\n",
    "        self.fc2 = nn.Linear(embedding_dim, num_word)\n",
    "\n",
    "    # LSTMの初期値は0で，画像の特徴量と文章の特徴量を全結合層の前で結合する\n",
    "    def forward(self, image_features, sentences):\n",
    "        \"\"\"\n",
    "            image_features: 画像の特徴量(batch_size, num_patch, image_feature_dim)\n",
    "            sentences: 入力する文章(batch_size, sentence_length)\n",
    "        \"\"\"\n",
    "        # encode image \n",
    "        src = F.leaky_relu( self.fc1( image_features ) )\n",
    "        src = self.pe1( src )\n",
    "        src = self.self_attention( src )\n",
    "\n",
    "        # decode sentence\n",
    "        tgt = self.embedding( sentences )\n",
    "        tgt = self.pe2( tgt )\n",
    "\n",
    "        attn_mask = torch.triu(torch.ones(self.sentence_length, self.sentence_length), diagonal = 1)\n",
    "        attn_mask = attn_mask.masked_fill(attn_mask == 1, float('-inf')).to(self.device)\n",
    "\n",
    "        tgt = self.cross_attention1( tgt = tgt, src = src, attn_mask = attn_mask )\n",
    "        tgt = self.cross_attention2( tgt = tgt, src = src, attn_mask = attn_mask  )\n",
    "        # tgt = self.cross_attention3( tgt = tgt, src = src, attn_mask = attn_mask  )\n",
    "        return self.fc2( tgt )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 大喜利生成AIの学習用データローダを作る関数\n",
    "def make_dataloader(boke_datas, max_sentence_length, batch_size, num_workers = 4):\n",
    "    \"\"\"\n",
    "        boke_datas: {\"image_number\":画像のお題番号 ,\"tokenized_boke\":トークナイズされた大喜利}からなるリスト\n",
    "        max_sentence_length: 学習データの最大単語数(<START>, <END>トークンを含まない)\n",
    "        num_workers: データローダが使用するCPUのスレッド数\n",
    "    \"\"\"\n",
    "    class SentenceGeneratorDataset(Dataset):\n",
    "        def __init__(self, image_file_numbers, sentences, teacher_signals):\n",
    "            \"\"\"\n",
    "                image_file_numbers: 画像の番号からなるリスト\n",
    "                sentences: 入力文章からなるリスト\n",
    "                teacher_signals: 教師信号からなるリスト\n",
    "            \"\"\"\n",
    "            if len(image_file_numbers) != len(sentences) and len(teacher_signals) != len(sentences):\n",
    "                raise ValueError(\"データリストの長さが一致しません\")\n",
    "\n",
    "            self.image_file_numbers = image_file_numbers\n",
    "            self.sentences = sentences\n",
    "            self.teacher_signals = teacher_signals\n",
    "\n",
    "        def __len__(self):\n",
    "            return len(self.teacher_signals)\n",
    "\n",
    "        def __getitem__(self, idx):\n",
    "            image_feature = np.load(f\"{IMAGE_FEATURE_DIR}{self.image_file_numbers[idx]}.npy\")\n",
    "            sentence = self.sentences[idx]\n",
    "            teacher_signal = self.teacher_signals[idx]\n",
    "\n",
    "            return image_feature, sentence, teacher_signal\n",
    "\n",
    "    def collate_fn_tf(batch):\n",
    "        image_features = torch.tensor(np.array([B[0] for B in batch]))\n",
    "        sentences = torch.tensor(np.array([B[1] for B in batch]))\n",
    "        teacher_signals = torch.tensor(np.array([B[2] for B in batch]))\n",
    "\n",
    "        return image_features, sentences, teacher_signals\n",
    "\n",
    "    image_file_numbers = list()\n",
    "    sentences = list()\n",
    "    teacher_signals = list()\n",
    "\n",
    "    for D in tqdm(boke_datas):\n",
    "        image_file_numbers.append(D[\"image_number\"])\n",
    "        tmp = D[\"tokenized_boke\"] + [0] * (2 + max_sentence_length - len(D[\"tokenized_boke\"]))\n",
    "        sentences.append(tmp[:-1])\n",
    "        teacher_signals.append(tmp[1:])\n",
    "\n",
    "    dataset = SentenceGeneratorDataset(image_file_numbers, sentences, teacher_signals)\n",
    "    dataloader = DataLoader(\n",
    "        dataset, \n",
    "        batch_size = batch_size, \n",
    "        num_workers = num_workers, \n",
    "        collate_fn = collate_fn_tf\n",
    "    )\n",
    "\n",
    "    print(f\"num data: {len(teacher_signals)}\")\n",
    "\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2030920/2030920 [00:03<00:00, 586547.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num data: 2030920\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20515/20515 [00:00<00:00, 1232382.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num data: 20515\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/25: 100%|██████████| 15867/15867 [1:56:41<00:00,  2.27it/s, train_loss=4.94, train_accuracy=0.226] \n",
      "Evaluating: 100%|██████████| 161/161 [00:39<00:00,  4.04it/s, test_loss=4.62, test_accuracy=0.248]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/25, Train Loss: 4.9374, Train Accuracy: 0.2263, Test Loss: 4.6199, Test Accuracy: 0.2484\n",
      "-------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/25: 100%|██████████| 15867/15867 [1:45:43<00:00,  2.50it/s, train_loss=4.48, train_accuracy=0.257]\n",
      "Evaluating: 100%|██████████| 161/161 [00:39<00:00,  4.05it/s, test_loss=4.43, test_accuracy=0.268]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2/25, Train Loss: 4.4814, Train Accuracy: 0.2567, Test Loss: 4.4255, Test Accuracy: 0.2681\n",
      "-------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/25: 100%|██████████| 15867/15867 [1:44:58<00:00,  2.52it/s, train_loss=4.29, train_accuracy=0.271]\n",
      "Evaluating: 100%|██████████| 161/161 [00:39<00:00,  4.05it/s, test_loss=4.32, test_accuracy=0.278]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3/25, Train Loss: 4.2881, Train Accuracy: 0.2712, Test Loss: 4.3204, Test Accuracy: 0.2781\n",
      "-------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/25: 100%|██████████| 15867/15867 [1:50:53<00:00,  2.38it/s, train_loss=4.16, train_accuracy=0.282]\n",
      "Evaluating: 100%|██████████| 161/161 [00:39<00:00,  4.03it/s, test_loss=4.28, test_accuracy=0.282]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4/25, Train Loss: 4.1583, Train Accuracy: 0.2818, Test Loss: 4.2827, Test Accuracy: 0.2821\n",
      "-------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/25: 100%|██████████| 15867/15867 [1:53:00<00:00,  2.34it/s, train_loss=4.08, train_accuracy=0.287]\n",
      "Evaluating: 100%|██████████| 161/161 [00:39<00:00,  4.06it/s, test_loss=4.24, test_accuracy=0.287]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5/25, Train Loss: 4.0831, Train Accuracy: 0.2873, Test Loss: 4.2378, Test Accuracy: 0.2869\n",
      "-------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/25: 100%|██████████| 15867/15867 [1:45:07<00:00,  2.52it/s, train_loss=4.01, train_accuracy=0.294]\n",
      "Evaluating: 100%|██████████| 161/161 [01:13<00:00,  2.20it/s, test_loss=4.21, test_accuracy=0.29] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6/25, Train Loss: 4.0064, Train Accuracy: 0.2944, Test Loss: 4.2085, Test Accuracy: 0.2900\n",
      "-------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/25: 100%|██████████| 15867/15867 [1:45:21<00:00,  2.51it/s, train_loss=3.94, train_accuracy=0.3]   \n",
      "Evaluating: 100%|██████████| 161/161 [00:40<00:00,  4.02it/s, test_loss=4.17, test_accuracy=0.296]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7/25, Train Loss: 3.9446, Train Accuracy: 0.3002, Test Loss: 4.1724, Test Accuracy: 0.2956\n",
      "-------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/25: 100%|██████████| 15867/15867 [1:45:10<00:00,  2.51it/s, train_loss=3.88, train_accuracy=0.306] \n",
      "Evaluating: 100%|██████████| 161/161 [00:40<00:00,  4.01it/s, test_loss=4.16, test_accuracy=0.298]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8/25, Train Loss: 3.8830, Train Accuracy: 0.3058, Test Loss: 4.1561, Test Accuracy: 0.2982\n",
      "-------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/25: 100%|██████████| 15867/15867 [1:44:47<00:00,  2.52it/s, train_loss=3.84, train_accuracy=0.31]\n",
      "Evaluating: 100%|██████████| 161/161 [00:39<00:00,  4.06it/s, test_loss=4.13, test_accuracy=0.302]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9/25, Train Loss: 3.8384, Train Accuracy: 0.3105, Test Loss: 4.1319, Test Accuracy: 0.3020\n",
      "-------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/25: 100%|██████████| 15867/15867 [1:44:47<00:00,  2.52it/s, train_loss=3.79, train_accuracy=0.315]\n",
      "Evaluating: 100%|██████████| 161/161 [00:49<00:00,  3.27it/s, test_loss=4.13, test_accuracy=0.304]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10/25, Train Loss: 3.7942, Train Accuracy: 0.3146, Test Loss: 4.1298, Test Accuracy: 0.3040\n",
      "-------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/25: 100%|██████████| 15867/15867 [1:44:44<00:00,  2.52it/s, train_loss=3.75, train_accuracy=0.319]\n",
      "Evaluating:   0%|          | 0/161 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "train_dataloader = make_dataloader(train_boke_datas, max_sentence_length = MAX_SENTENCE_LENGTH, batch_size = BATCH_SIZE_FOR_GENERATOR, num_workers = NUM_WORKERS)\n",
    "test_dataloader = make_dataloader(test_boke_datas, max_sentence_length = MAX_SENTENCE_LENGTH, batch_size = BATCH_SIZE_FOR_GENERATOR, num_workers = NUM_WORKERS)\n",
    "\n",
    "# モデルの学習\n",
    "train_loss_history = []\n",
    "train_accuracy_history = []\n",
    "test_loss_history = []\n",
    "test_accuracy_history = []\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = TransformerBokeGeneratorModel(num_image_patch = 197, \n",
    "                                      image_feature_dim = 1024, \n",
    "                                      num_word = len(index_to_word), \n",
    "                                      sentence_length = MAX_SENTENCE_LENGTH + 1, \n",
    "                                      embedding_dim = 2048, \n",
    "                                      num_heads = 4)\n",
    "model.to(device)\n",
    "optimizer = optim.AdamW(model.parameters(), lr = LEARNING_RATO_FOR_GENERATOR)\n",
    "\n",
    "# 文章生成の精度を計算する関数\n",
    "def calculate_accuracy(teacher_signals, outputs):\n",
    "    \"\"\"\n",
    "        teacher_signals: 教師信号\n",
    "        outputs: モデルの出力\n",
    "    \"\"\"\n",
    "    _, predicted_words = outputs.max(dim = -1)\n",
    "    # パディングに対して精度を計算しない\n",
    "    mask = (teacher_signals != 0)\n",
    "    correct = ((predicted_words == teacher_signals) & mask).sum().item()\n",
    "    total = mask.sum().item()\n",
    "    accuracy = correct / total if total > 0 else 0.0\n",
    "    return accuracy\n",
    "\n",
    "# 1イテレーション学習する関数\n",
    "def train_step(model, optimizer, batch_data, batch_labels):\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(*batch_data)\n",
    "    # パディングに対して損失を計算しない\n",
    "    loss = F.cross_entropy(outputs.view(-1, outputs.size(-1)), batch_labels.view(-1),\n",
    "                           ignore_index = 0)\n",
    "    accuracy = calculate_accuracy(batch_labels, F.softmax(outputs, dim = -1))\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss.item(), accuracy\n",
    "\n",
    "# 1イテレーション検証する関数\n",
    "def evaluate(model, batch_data, batch_labels):\n",
    "    with torch.no_grad():\n",
    "        outputs = model(*batch_data)\n",
    "        loss = F.cross_entropy(outputs.view(-1, outputs.size(-1)), batch_labels.view(-1),\n",
    "                               ignore_index = 0)\n",
    "        accuracy = calculate_accuracy(batch_labels, F.softmax(outputs, dim = -1))\n",
    "    return loss.item(), accuracy\n",
    "\n",
    "for epoch in range(EPOCH_FOR_GENERATOR):\n",
    "\n",
    "    # train\n",
    "    train_loss_obj = 0.0\n",
    "    train_accuracy_obj = 0.0\n",
    "    model.train()\n",
    "    pb = tqdm(train_dataloader, desc = f\"Epoch {epoch+1}/{EPOCH_FOR_GENERATOR}\")\n",
    "    \n",
    "    for image_features, sentences, teacher_signals in pb:\n",
    "        image_features = image_features.to(device)\n",
    "        sentences = sentences.to(device)\n",
    "        teacher_signals = teacher_signals.to(device)\n",
    "        \n",
    "        loss, accuracy = train_step(model, optimizer, (image_features, sentences), teacher_signals)\n",
    "        train_loss_obj += loss\n",
    "        train_accuracy_obj += accuracy\n",
    "        pb.set_postfix({\"train_loss\": train_loss_obj / (pb.n + 1), \"train_accuracy\": train_accuracy_obj / (pb.n + 1)})\n",
    "\n",
    "    train_loss = train_loss_obj / len(train_dataloader)\n",
    "    train_accuracy = train_accuracy_obj / len(train_dataloader)\n",
    "\n",
    "    # test\n",
    "    test_loss_obj = 0.0\n",
    "    test_accuracy_obj = 0.0\n",
    "    model.eval()\n",
    "    pb = tqdm(test_dataloader, desc = \"Evaluating\")\n",
    "\n",
    "    for image_features, sentences, teacher_signals in pb:\n",
    "        image_features = image_features.to(device)\n",
    "        sentences = sentences.to(device)\n",
    "        teacher_signals = teacher_signals.to(device)\n",
    "\n",
    "        loss, accuracy = evaluate(model, (image_features, sentences), teacher_signals)\n",
    "        test_loss_obj += loss\n",
    "        test_accuracy_obj += accuracy\n",
    "        pb.set_postfix({\"test_loss\": test_loss_obj / (pb.n + 1), \"test_accuracy\": test_accuracy_obj / (pb.n + 1)})\n",
    "\n",
    "    test_loss = test_loss_obj / len(test_dataloader)\n",
    "    test_accuracy = test_accuracy_obj / len(test_dataloader)\n",
    "\n",
    "    print(f\"Epoch: {epoch+1}/{EPOCH_FOR_GENERATOR}, \"\n",
    "          f\"Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, \"\n",
    "          f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}\")\n",
    "    print(\"-\" * 25)\n",
    "\n",
    "    train_loss_history.append(train_loss)\n",
    "    train_accuracy_history.append(train_accuracy)\n",
    "    test_loss_history.append(test_loss)\n",
    "    test_accuracy_history.append(test_accuracy)\n",
    "\n",
    "    # 学習精度を更新した場合、重みを保存\n",
    "    if max(train_accuracy_history) == train_accuracy:\n",
    "        torch.save(model.state_dict(), f\"{RESULT_DIR}best_model_weights.pth\")\n",
    "\n",
    "# 学習結果を保存\n",
    "with open(f\"{RESULT_DIR}history.json\", \"w\") as f:\n",
    "    json.dump({\n",
    "        \"train_loss\": train_loss_history,\n",
    "        \"train_accuracy\": train_accuracy_history,\n",
    "        \"test_loss\": test_loss_history,\n",
    "        \"test_accuracy\": test_accuracy_history\n",
    "    }, f)\n",
    "\n",
    "# 学習結果を描画\n",
    "fig = plt.figure(figsize = (10, 5))\n",
    "ax = fig.add_subplot(1, 2, 1)\n",
    "ax.plot(train_loss_history, label = \"train\")\n",
    "ax.plot(test_loss_history, label = \"test\")\n",
    "ax.set_xlabel(\"epoch\")\n",
    "ax.set_ylabel(\"loss\")\n",
    "ax.legend()\n",
    "ax.grid()\n",
    "\n",
    "ax = fig.add_subplot(1, 2, 2)\n",
    "ax.plot(train_accuracy_history, label = \"train\")\n",
    "ax.plot(test_accuracy_history, label = \"test\")\n",
    "ax.set_xlabel(\"epoch\")\n",
    "ax.set_ylabel(\"accuracy\")\n",
    "ax.legend()\n",
    "ax.grid()\n",
    "\n",
    "fig.savefig(f\"{RESULT_DIR}history.png\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Colab_20241111",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
