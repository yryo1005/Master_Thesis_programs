{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 001\n",
    "\n",
    "現実写真のみ、文字なし，alpha = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import random\n",
    "import shutil\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image, ImageOps\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import subprocess\n",
    "if not os.path.exists(\"Japanese_BPEEncoder_V2\"):\n",
    "    subprocess.run([\"git\", \"clone\", \"https://github.com/tanreinama/Japanese-BPEEncoder_V2.git\", \"Japanese_BPEEncoder_V2\"])\n",
    "from Japanese_BPEEncoder_V2.encode_swe import SWEEncoder_ja"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXPERIENCE_NUMBER = \"001\"\n",
    "ALPHA = 100\n",
    "CHAIN = 3\n",
    "MAX_DEPTH = 3\n",
    "\n",
    "# PCによって変更する\n",
    "NUM_WORKERS = 20\n",
    "# データセットが既に存在する場合に，再度作り直すか\n",
    "RESET_DATA = False\n",
    "# Augmix-Autoencoderを学習しなおすか\n",
    "RETRAIN_AUMIX_AUTOENCODER = False\n",
    "\n",
    "# 現実写真以外を使用するか\n",
    "USE_UNREAL_IMAGE = False\n",
    "# 文字を含む画像を使用するか\n",
    "USE_WORD_IMAGE = False\n",
    "# 固有名詞を含む大喜利を使用するか\n",
    "USE_UNIQUE_NOUN_BOKE = False\n",
    "\n",
    "# 大喜利の最小の星の数\n",
    "MIN_STAR = 0\n",
    "# 単語の最小出現回数\n",
    "MIN_APPER_WORD = 32\n",
    "# 大喜利の最小単語数\n",
    "MIN_SENTENCE_LENGTH = 4\n",
    "# 大喜利の最大単語数\n",
    "MAX_SENTENCE_LENGTH = 31\n",
    "\n",
    "RESULT_DIR = f\"../../results/GUMI_AMAE_{ALPHA}/{EXPERIENCE_NUMBER}/\"\n",
    "if not os.path.exists(f\"../../results/GUMI_AMAE_{ALPHA}/\"):\n",
    "    os.mkdir(f\"../../results/GUMI_AMAE_{ALPHA}/\")\n",
    "if not os.path.exists(RESULT_DIR):\n",
    "    os.mkdir(RESULT_DIR)\n",
    "\n",
    "# \n",
    "BATCH_SIZE_FOR_AUTOENCODER = 32\n",
    "EPOCH_FOR_AUTOENCODER = 25\n",
    "LEARNING_RATO_FOR_AUTOENCODER = 0.0001\n",
    "IMAGE_HEIGHT = 128\n",
    "IMAGE_WIDTH = 128\n",
    "IMAGE_FEATURE_DIM = 16384\n",
    "\n",
    "BATCH_SIZE_FOR_GENERATOR = 32\n",
    "EPOCH_FOR_GENERATOR = 25\n",
    "LEARNING_RATO_FOR_GENERATOR = 0.0001\n",
    "\n",
    "DATA_DIR = \"../../datas/boke_data_assemble/\"\n",
    "IMAGE_DIR = \"../../datas/boke_image/\"\n",
    "\n",
    "IMAGE_FEATURE_DIR = f\"../../datas/encoded/augmix_autoencoder_{ALPHA}_image_feature/\"\n",
    "if not os.path.exists(IMAGE_FEATURE_DIR):\n",
    "    os.mkdir(IMAGE_FEATURE_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 668982/668982 [03:40<00:00, 3027.20it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(228908, 25435)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if not os.path.exists(f\"{RESULT_DIR}test_image_paths.json\") or RESET_DATA:\n",
    "    image_paths = list()\n",
    "\n",
    "    for IP in tqdm(os.listdir(IMAGE_DIR)):\n",
    "        \n",
    "        N = int(IP.split(\".\")[0])\n",
    "        if not os.path.exists(f\"{DATA_DIR}{N}.json\"):\n",
    "            continue\n",
    "\n",
    "        with open(f\"{DATA_DIR}{N}.json\", \"r\") as f:\n",
    "            a = json.load(f)\n",
    "        \n",
    "        image_information = a[\"image_information\"]\n",
    "        is_photographic_probability = image_information[\"is_photographic_probability\"]\n",
    "        ocr = image_information[\"ocr\"]\n",
    "\n",
    "        # 現実写真以外を除去\n",
    "        if not USE_UNREAL_IMAGE:\n",
    "            if is_photographic_probability < 0.8: continue\n",
    "            \n",
    "        # 文字のある画像を除去\n",
    "        if not USE_WORD_IMAGE:\n",
    "            if len(ocr) != 0: continue\n",
    "        \n",
    "        image_paths.append(f\"{IMAGE_DIR}{IP}\")\n",
    "\n",
    "    train_image_paths, test_image_paths = train_test_split(image_paths, test_size = 0.1)\n",
    "\n",
    "    with open(f\"{RESULT_DIR}train_image_paths.json\", \"w\") as f:\n",
    "        json.dump(train_image_paths, f)\n",
    "    with open(f\"{RESULT_DIR}test_image_paths.json\", \"w\") as f:\n",
    "        json.dump(test_image_paths, f)\n",
    "    \n",
    "else:\n",
    "    with open(f\"{RESULT_DIR}train_image_paths.json\", \"r\") as f:\n",
    "        train_image_paths = json.load(f)\n",
    "    with open(f\"{RESULT_DIR}test_image_paths.json\", \"r\") as f:\n",
    "        test_image_paths = json.load(f)\n",
    "\n",
    "len(train_image_paths), len(test_image_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def autocontrast(pil_img):\n",
    "    return ImageOps.autocontrast(pil_img)\n",
    "\n",
    "def equalize(pil_img):\n",
    "    return ImageOps.equalize(pil_img)\n",
    "\n",
    "def posterize(pil_img):\n",
    "    f = random.randint(4, 7)\n",
    "    return ImageOps.posterize(pil_img, 8 - f)\n",
    "\n",
    "def rotate(pil_img):\n",
    "    d = random.uniform(-10.0, 10.0)\n",
    "    return pil_img.rotate(d, resample = Image.BILINEAR)\n",
    "\n",
    "def solarize(pil_img):\n",
    "    f = random.randint(0, 255)\n",
    "    return ImageOps.solarize(pil_img, 255 - f)\n",
    "\n",
    "def shear_x(pil_img):\n",
    "    w, h = pil_img.size\n",
    "    f = random.uniform(-0.1, 0.1)\n",
    "    return pil_img.transform((w, h),\n",
    "                             Image.AFFINE, (1, f, 0, 0, 1, 0),\n",
    "                             resample = Image.BILINEAR)\n",
    "\n",
    "def shear_y(pil_img):\n",
    "    w, h = pil_img.size\n",
    "    f = random.uniform(-0.1, 0.1)\n",
    "    return pil_img.transform((w, h),\n",
    "                             Image.AFFINE, (1, 0, 0, f, 1, 0),\n",
    "                             resample = Image.BILINEAR)\n",
    "\n",
    "def center_shear_x(pil_img):\n",
    "    w, h = pil_img.size\n",
    "    f = random.uniform(-0.1, 0.1)\n",
    "    return pil_img.transform((w, h),\n",
    "                             Image.AFFINE, (1, f, -f * w / 2, 0, 1, 0),\n",
    "                             resample = Image.BILINEAR)\n",
    "\n",
    "def center_shear_y(pil_img):\n",
    "    w, h = pil_img.size\n",
    "    f = random.uniform(-0.1, 0.1)\n",
    "    return pil_img.transform((w, h),\n",
    "                             Image.AFFINE, (1, 0, 0, f, 1, -f * h / 2),\n",
    "                             resample = Image.BILINEAR)\n",
    "\n",
    "def translate_x(pil_img):\n",
    "    w, h = pil_img.size\n",
    "    f = random.uniform(-0.1, 0.1) * w\n",
    "    return pil_img.transform((w, h),\n",
    "                             Image.AFFINE, (1, 0, f, 0, 1, 0),\n",
    "                             resample = Image.BILINEAR)\n",
    "\n",
    "def translate_y(pil_img):\n",
    "    w, h = pil_img.size\n",
    "    f = random.uniform(-0.1, 0.1) * h\n",
    "    return pil_img.transform((w, h),\n",
    "                             Image.AFFINE, (1, 0, 0, 0, 1, f),\n",
    "                             resample = Image.BILINEAR)\n",
    "\n",
    "def downsampling(pil_img):\n",
    "    w, h = pil_img.size\n",
    "    th = random.randint(w // 8, h // 2)\n",
    "    tw = random.randint(w // 8, h // 2)\n",
    "    return pil_img.resize((tw, th)).resize((w, h))\n",
    "\n",
    "def add_noise(pil_img):\n",
    "    w, h = pil_img.size\n",
    "    v = random.randint(0, 10)\n",
    "    n = np.random.normal(0, v, (h, w, 3))\n",
    "    return Image.fromarray((pil_img + n).astype(np.uint8))\n",
    "\n",
    "def choice_random_operation():\n",
    "    idx = random.randint(0, len(operations) - 1)\n",
    "    return operations[idx]\n",
    "\n",
    "def augmix_operation(pil_img, chain = 3, max_depth = 3):\n",
    "    width, height = pil_img.size\n",
    "\n",
    "    mixed_img = np.zeros((height, width, 3))\n",
    "\n",
    "    weights = np.random.dirichlet([1.0] * chain)\n",
    "\n",
    "    for W in weights:\n",
    "        img = pil_img.copy()\n",
    "\n",
    "        for _ in range(random.randint(1, max_depth)):\n",
    "            operation = choice_random_operation()\n",
    "            img = operation(img)\n",
    "\n",
    "        mixed_img += W * img\n",
    "\n",
    "    weight = np.random.uniform(0, 1)\n",
    "    result = weight * np.array(pil_img) + (1 - weight) * np.array(mixed_img)\n",
    "\n",
    "    return result\n",
    "\n",
    "operations = [\n",
    "            # autocontrast, equalize, posterize, solarize,\n",
    "            rotate,\n",
    "            # shear_x, shear_y,\n",
    "            center_shear_x, center_shear_y,\n",
    "            translate_x, translate_y,\n",
    "            downsampling,\n",
    "            add_noise\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 画像の前処理\n",
    "image_preprocess = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# augmixのデータローダを作る関数\n",
    "def make_augmix_image_dataloader(image_paths, batch_size, chain, max_depth, num_workers = 4):\n",
    "\n",
    "    class LoadImageDataset(Dataset):\n",
    "        def __init__(self, image_paths):\n",
    "            \"\"\"\n",
    "                image_paths: 画像のパスからなるリスト\n",
    "            \"\"\"\n",
    "            self.image_paths = image_paths\n",
    "\n",
    "        def __len__(self):\n",
    "            return len(self.image_paths)\n",
    "\n",
    "        def __getitem__(self, idx):\n",
    "            image = Image.open(image_paths[idx]).convert(\"RGB\")\n",
    "\n",
    "            return image, self.image_paths[idx]\n",
    "    \n",
    "    def collate_fn_tf(batch):\n",
    "        images = [B[0].resize((IMAGE_WIDTH, IMAGE_HEIGHT)) for B in batch]\n",
    "        augmix_images1 = torch.stack([ image_preprocess( augmix_operation(I, chain = chain, max_depth = max_depth) / 255.0 ) for I in images ])\n",
    "        augmix_images2 = torch.stack([ image_preprocess( augmix_operation(I, chain = chain, max_depth = max_depth) / 255.0 ) for I in images ])\n",
    "        images = torch.stack([image_preprocess(I) for I in images])\n",
    "        image_numbers = [B[1] for B in batch]\n",
    "\n",
    "        return images, augmix_images1, augmix_images2, image_numbers\n",
    "\n",
    "    print(f\"num data: {len(image_paths)}\")\n",
    "\n",
    "    dataset = LoadImageDataset(image_paths)\n",
    "    dataloader = DataLoader(\n",
    "        dataset, \n",
    "        batch_size = batch_size, \n",
    "        num_workers = num_workers, \n",
    "        collate_fn = collate_fn_tf\n",
    "    )\n",
    "\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageEncoder(nn.Module):\n",
    "    def __init__(self, image_feature_dim):\n",
    "        \"\"\"\n",
    "            image_feature_dim: \n",
    "        \"\"\"\n",
    "        super(ImageEncoder, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size = 3, stride = 2, padding = 1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size = 3, stride = 2, padding = 1)\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size = 3, stride = 2, padding = 1)\n",
    "        self.conv4 = nn.Conv2d(128, 256, kernel_size = 3, stride = 2, padding = 1)\n",
    "\n",
    "        self.fc1 = nn.Linear(16384, 16384)\n",
    "        self.fc2 = nn.Linear(16384, image_feature_dim)\n",
    "    \n",
    "    def forward(self, images):\n",
    "        x = F.leaky_relu( self.conv1(images) )\n",
    "        # 32, 64, 64\n",
    "        x = F.leaky_relu( self.conv2(x) )\n",
    "        # 64, 32, 32\n",
    "        x = F.leaky_relu( self.conv3(x) )\n",
    "        # 128, 16, 16\n",
    "        x = F.leaky_relu( self.conv4(x) )\n",
    "        # 256, 8, 8\n",
    "\n",
    "        x = nn.Flatten()(x)\n",
    "        x = F.leaky_relu(self.fc1(x))\n",
    "        return F.leaky_relu(self.fc2(x))\n",
    "\n",
    "class ImageDecoder(nn.Module):\n",
    "    def __init__(self, image_feature_dim):\n",
    "        \"\"\"\n",
    "            image_feature_dim: \n",
    "        \"\"\"\n",
    "        super(ImageDecoder, self).__init__()\n",
    "\n",
    "        self.fc1 = nn.Linear(image_feature_dim, 16384)\n",
    "\n",
    "        self.deconv1 = nn.ConvTranspose2d(256, 128, kernel_size = 3, stride = 2, padding = 1, output_padding = 1)\n",
    "        self.deconv2 = nn.ConvTranspose2d(128, 64, kernel_size = 3, stride = 2, padding = 1, output_padding = 1)\n",
    "        self.deconv3 = nn.ConvTranspose2d(64, 32, kernel_size = 3, stride = 2, padding = 1, output_padding = 1)\n",
    "        self.deconv4 = nn.ConvTranspose2d(32, 32, kernel_size = 3, stride = 2, padding = 1, output_padding = 1)\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(32, 3, kernel_size = 3, stride = 1, padding = 1)\n",
    "    \n",
    "    def forward(self, image_features):\n",
    "        x = F.leaky_relu(self.fc1(image_features))\n",
    "        x = nn.Unflatten(1, (256, 8, 8))(x)\n",
    "        # 256, 8, 8\n",
    "\n",
    "        x = F.leaky_relu( self.deconv1(x) )\n",
    "        # 128, 16, 16\n",
    "        x = F.leaky_relu( self.deconv2(x) )\n",
    "        # 64, 32, 32\n",
    "        x = F.leaky_relu( self.deconv3(x) )\n",
    "        # 32, 64, 64\n",
    "        x = F.leaky_relu( self.deconv4(x) )\n",
    "        # 32, 128, 128\n",
    "        return nn.Sigmoid()( self.conv1(x) )\n",
    "        # 3, 128, 128\n",
    "\n",
    "class AugmixAutoencoder(nn.Module):\n",
    "    def __init__(self, image_feature_dim):\n",
    "        \"\"\"\n",
    "            image_feature_dim: \n",
    "        \"\"\"\n",
    "        super(AugmixAutoencoder, self).__init__()\n",
    "\n",
    "        self.encoder = ImageEncoder(image_feature_dim)\n",
    "        self.decoder = ImageDecoder(image_feature_dim)\n",
    "    \n",
    "    def forward(self, images):\n",
    "        image_features = self.encoder(images)\n",
    "        return self.decoder( image_features ), image_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_by_augmix_autoencoder(augmix_autoeuncoder, image_paths, device = \"cuda\"):\n",
    "    tmp_images = [Image.open(IP).resize((IMAGE_WIDTH, IMAGE_WIDTH)) for IP in image_paths]\n",
    "    tmp_images = [image_preprocess(I) for I in tmp_images] + [ image_preprocess( augmix_operation(I, chain = CHAIN, max_depth = MAX_DEPTH) / 255.0 ) for I in tmp_images ]\n",
    "    images = torch.stack(tmp_images).to(torch.float32).to(device)\n",
    "\n",
    "    predict_images, _ = augmix_autoeuncoder(images)\n",
    "    predict_images = predict_images.permute(0, 2, 3, 1).cpu().detach().numpy()\n",
    "\n",
    "    fig = plt.figure()\n",
    "    for i in range(len(tmp_images)):\n",
    "        ax = fig.add_subplot(2, len(tmp_images), i + 1)\n",
    "        ax.imshow(tmp_images[i].permute(1, 2, 0))\n",
    "        ax.axis(\"off\")\n",
    "        ax.set_title(\"input\")\n",
    "\n",
    "        ax = fig.add_subplot(2, len(tmp_images), len(tmp_images) + i + 1)\n",
    "        ax.imshow(predict_images[i])\n",
    "        ax.axis(\"off\")\n",
    "        ax.set_title(\"predict\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num data: 228908\n",
      "num data: 25435\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/25:   6%|▌         | 397/7154 [02:02<32:41,  3.44it/s, train_loss=0.179]"
     ]
    }
   ],
   "source": [
    "if not os.path.exists(f\"{RESULT_DIR}augmix_autoencoder_history.png\") or RETRAIN_AUTOENCODER:\n",
    "    train_loss_history = []\n",
    "    test_loss_history = []\n",
    "\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    augmix_autoencoder = AugmixAutoencoder(image_feature_dim = IMAGE_FEATURE_DIM)\n",
    "    augmix_autoencoder.to(device)\n",
    "    optimizer = optim.AdamW(augmix_autoencoder.parameters(), lr = LEARNING_RATO_FOR_AUTOENCODER)\n",
    "\n",
    "    train_image_dataloader = make_augmix_image_dataloader(train_image_paths, batch_size = BATCH_SIZE_FOR_AUTOENCODER, chain = CHAIN, max_depth = MAX_DEPTH, num_workers = NUM_WORKERS)\n",
    "    test_image_dataloader = make_augmix_image_dataloader(test_image_paths, batch_size = BATCH_SIZE_FOR_AUTOENCODER, chain = CHAIN, max_depth = MAX_DEPTH, num_workers = NUM_WORKERS)\n",
    "\n",
    "    # 1イテレーション学習する関数\n",
    "    def train_step_for_augmix_autoencoder(augmix_autoencoder, optimizer, images, augmix_images1, augmix_images2):\n",
    "        optimizer.zero_grad()\n",
    "        original_outputs, original_features = augmix_autoencoder(images)\n",
    "        am1_outputs, am1_features = augmix_autoencoder(augmix_images1)\n",
    "        am2_outputs, am2_features = augmix_autoencoder(augmix_images2)\n",
    "\n",
    "        loss1 = nn.MSELoss()(original_outputs, images)\n",
    "        loss2 = nn.MSELoss()(am1_outputs, images)\n",
    "        loss3 = nn.MSELoss()(am2_outputs, images)\n",
    "        loss4 = nn.MSELoss()(original_features, am1_features)\n",
    "        loss5 = nn.MSELoss()(original_features, am2_features)\n",
    "\n",
    "        loss = loss1 + loss2 + loss3 + ALPHA * (loss4 + loss5)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        return loss.item()\n",
    "\n",
    "    # 1イテレーション検証する関数\n",
    "    def test_step_for_augmix_autoencoder(augmix_autoencoder, images, augmix_images1, augmix_images2):\n",
    "        with torch.no_grad():\n",
    "            original_outputs, original_features = augmix_autoencoder(images)\n",
    "            am1_outputs, am1_features = augmix_autoencoder(augmix_images1)\n",
    "            am2_outputs, am2_features = augmix_autoencoder(augmix_images2)\n",
    "\n",
    "            loss1 = nn.MSELoss()(original_outputs, images)\n",
    "            loss2 = nn.MSELoss()(am1_outputs, images)\n",
    "            loss3 = nn.MSELoss()(am2_outputs, images)\n",
    "            loss4 = nn.MSELoss()(original_features, am1_features)\n",
    "            loss5 = nn.MSELoss()(original_features, am2_features)\n",
    "\n",
    "            loss = loss1 + loss2 + loss3 + ALPHA * (loss4 + loss5)\n",
    "        return loss.item()\n",
    "\n",
    "    for epoch in range(EPOCH_FOR_AUTOENCODER):\n",
    "\n",
    "        # train\n",
    "        train_loss_obj = 0.0\n",
    "        augmix_autoencoder.train()\n",
    "        pb = tqdm(train_image_dataloader, desc = f\"Epoch {epoch+1}/{EPOCH_FOR_AUTOENCODER}\")\n",
    "        \n",
    "        for images, augmix_images1, augmix_images2, _ in pb:\n",
    "            images = images.float().to(\"cuda\")\n",
    "            augmix_images1 = augmix_images1.float().to(\"cuda\")\n",
    "            augmix_images2 = augmix_images2.float().to(\"cuda\")\n",
    "\n",
    "            loss = train_step_for_augmix_autoencoder(augmix_autoencoder, optimizer, images, augmix_images1, augmix_images2)\n",
    "            train_loss_obj += loss\n",
    "            pb.set_postfix({\"train_loss\": train_loss_obj / (pb.n + 1),})\n",
    "        train_loss = train_loss_obj / len(train_image_dataloader)\n",
    "\n",
    "        # test\n",
    "        test_loss_obj = 0.0\n",
    "        augmix_autoencoder.eval()\n",
    "        pb = tqdm(test_image_dataloader, desc = f\"Epoch {epoch+1}/{EPOCH_FOR_AUTOENCODER}\")\n",
    "        \n",
    "        for images, augmix_images1, augmix_images2, _ in pb:\n",
    "            images = images.float().to(\"cuda\")\n",
    "            augmix_images1 = augmix_images1.float().to(\"cuda\")\n",
    "            augmix_images2 = augmix_images2.float().to(\"cuda\")\n",
    "\n",
    "            loss = test_step_for_augmix_autoencoder(augmix_autoencoder, images, augmix_images1, augmix_images2)\n",
    "            test_loss_obj += loss\n",
    "            pb.set_postfix({\"test_loss\": test_loss_obj / (pb.n + 1),})\n",
    "        test_loss = test_loss_obj / len(test_image_dataloader)\n",
    "\n",
    "        print(f\"Epoch: {epoch+1}/{EPOCH_FOR_AUTOENCODER}, \"\n",
    "            f\"Train Loss: {train_loss:.4f}, Test Loss: {test_loss:.4f}\")\n",
    "        print(\"-\" * 25)\n",
    "\n",
    "        train_loss_history.append(train_loss)\n",
    "        test_loss_history.append(test_loss)\n",
    "\n",
    "        # 検証誤差を更新した場合、重みを保存\n",
    "        if min(test_loss_history) == test_loss:\n",
    "            torch.save(augmix_autoencoder.state_dict(), f\"{RESULT_DIR}best_augmix_autoencoder_weights.pth\")\n",
    "\n",
    "        predict_by_augmix_autoencoder(augmix_autoencoder, test_image_paths[:5])\n",
    "\n",
    "    # 学習結果を保存\n",
    "    with open(f\"{RESULT_DIR}augmix_autoencoder_history.json\", \"w\") as f:\n",
    "        json.dump({\n",
    "            \"train_loss\": train_loss_history,\n",
    "            \"test_loss\": test_loss_history,\n",
    "        }, f)\n",
    "\n",
    "    # 学習結果を描画\n",
    "    fig = plt.figure(figsize = (5, 5))\n",
    "    ax = fig.add_subplot(1, 1, 1)\n",
    "    ax.plot(train_loss_history, label = \"train\")\n",
    "    ax.plot(test_loss_history, label = \"test\")\n",
    "    ax.set_xlabel(\"epoch\")\n",
    "    ax.set_ylabel(\"loss\")\n",
    "    ax.legend()\n",
    "    ax.grid()\n",
    "\n",
    "    fig.savefig(f\"{RESULT_DIR}augmix_autoencoder_history.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 668970/668970 [04:48<00:00, 2315.31it/s]\n",
      "100%|██████████| 2135982/2135982 [00:02<00:00, 799140.13it/s] \n",
      "100%|██████████| 2051435/2051435 [00:02<00:00, 816468.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "学習に用いる大喜利の数: 2030920\n",
      " 検証に用いる大喜利の数: 20515\n",
      " 使用する画像の数: 244286\n",
      " 単語の種類: 16705\n"
     ]
    }
   ],
   "source": [
    "# データセットの作成\n",
    "if not os.path.exists(f\"{RESULT_DIR}index_to_word.json\") or RESET_DATA:\n",
    "    # tokenizer\n",
    "    with open('Japanese_BPEEncoder_V2/ja-swe32kfix.txt') as f:\n",
    "        bpe = f.read().split('\\n')\n",
    "\n",
    "    with open('Japanese_BPEEncoder_V2/emoji.json') as f:\n",
    "        emoji = json.loads(f.read())\n",
    "\n",
    "    tokenizer = SWEEncoder_ja(bpe, emoji)\n",
    "\n",
    "    tmp = list()\n",
    "    word_count_dict = dict()\n",
    "\n",
    "    for JP in tqdm(os.listdir(DATA_DIR)):\n",
    "        \n",
    "        N = int(JP.split(\".\")[0])\n",
    "\n",
    "        with open(f\"{DATA_DIR}{JP}\", \"r\") as f:\n",
    "            a = json.load(f)\n",
    "        \n",
    "        image_information = a[\"image_information\"]\n",
    "        is_photographic_probability = image_information[\"is_photographic_probability\"]\n",
    "        ocr = image_information[\"ocr\"]\n",
    "\n",
    "        # 現実写真以外を除去\n",
    "        if not USE_UNREAL_IMAGE:\n",
    "            if is_photographic_probability < 0.8: continue\n",
    "            \n",
    "        # 文字のある画像を除去\n",
    "        if not USE_WORD_IMAGE:\n",
    "            if len(ocr) != 0: continue\n",
    "        \n",
    "        bokes = a[\"bokes\"]\n",
    "\n",
    "        for B in bokes:\n",
    "            # 星が既定の数以下の大喜利を除去\n",
    "            if B[\"star\"] < MIN_STAR:\n",
    "                continue\n",
    "\n",
    "            # 固有名詞を含む大喜利を除去\n",
    "            if not USE_UNIQUE_NOUN_BOKE:\n",
    "                if len(B[\"unique_nouns\"]) != 0: continue\n",
    "\n",
    "            tokenized_boke = tokenizer.encode(B[\"boke\"])\n",
    "            # 単語数が既定の数でない大喜利を除去\n",
    "            if not MIN_SENTENCE_LENGTH <= len(tokenized_boke) < MAX_SENTENCE_LENGTH:\n",
    "                continue\n",
    "\n",
    "            for W in tokenized_boke:\n",
    "                try:\n",
    "                    word_count_dict[W] += 1\n",
    "                except:\n",
    "                    word_count_dict[W] = 1\n",
    "            \n",
    "            tmp.append({\n",
    "                \"image_number\": N,\n",
    "                \"tokenized_boke\": tokenized_boke\n",
    "            })\n",
    "\n",
    "    # 単語の最小出現回数を満たさない大喜利を除去\n",
    "    boke_datas = list()\n",
    "    words = list()\n",
    "\n",
    "    for D in tqdm(tmp):\n",
    "        flag = False\n",
    "        for W in D[\"tokenized_boke\"]:\n",
    "            if word_count_dict[W] < MIN_APPER_WORD:\n",
    "                flag = True\n",
    "                break\n",
    "        if flag: \n",
    "            continue\n",
    "        \n",
    "        boke_datas.append({\n",
    "            \"image_number\": D[\"image_number\"],\n",
    "            \"tokenized_boke\": D[\"tokenized_boke\"]\n",
    "        })\n",
    "        words += D[\"tokenized_boke\"]\n",
    "    words = set(words)\n",
    "    image_numbers = list(set([D[\"image_number\"] for D in boke_datas]))\n",
    "    del tmp\n",
    "\n",
    "    # tokenize\n",
    "    index_to_index = dict()\n",
    "\n",
    "    c = 3\n",
    "    for D in tqdm(boke_datas):\n",
    "        tmp = list()\n",
    "        for W in D[\"tokenized_boke\"]:\n",
    "            try:\n",
    "                index_to_index[W]\n",
    "            except:\n",
    "                index_to_index[W] = c\n",
    "                c += 1\n",
    "            tmp.append(index_to_index[W])\n",
    "        D[\"tokenized_boke\"] = [1] + tmp + [2]\n",
    "\n",
    "    index_to_word = {\n",
    "        V: tokenizer.decode([K]) for K, V in index_to_index.items()\n",
    "    }\n",
    "    index_to_word[0] = \"<PAD>\"\n",
    "    index_to_word[1] = \"<START>\"\n",
    "    index_to_word[2] = \"<END>\"\n",
    "\n",
    "    #\n",
    "    train_boke_datas, test_boke_datas = train_test_split(boke_datas, test_size = 0.01)\n",
    "\n",
    "    with open(f\"{RESULT_DIR}train_boke_datas.json\", \"w\") as f:\n",
    "        json.dump(train_boke_datas, f)\n",
    "    with open(f\"{RESULT_DIR}test_boke_datas.json\", \"w\") as f:\n",
    "        json.dump(test_boke_datas, f)\n",
    "    with open(f\"{RESULT_DIR}index_to_word.json\", \"w\") as f:\n",
    "        json.dump(index_to_word, f)\n",
    "\n",
    "else:\n",
    "    with open(f\"{RESULT_DIR}train_boke_datas.json\", \"r\") as f:\n",
    "        train_boke_datas = json.load(f)\n",
    "    with open(f\"{RESULT_DIR}test_boke_datas.json\", \"r\") as f:\n",
    "        test_boke_datas = json.load(f)\n",
    "    with open(f\"{RESULT_DIR}index_to_word.json\", \"r\") as f:\n",
    "        index_to_word = json.load(f)\n",
    "\n",
    "    image_numbers = [D[\"image_number\"] for D in train_boke_datas] + [D[\"image_number\"] for D in test_boke_datas]\n",
    "    image_numbers = list(set(image_numbers))\n",
    "\n",
    "print(f\"学習に用いる大喜利の数: {len(train_boke_datas)}\\n\", \n",
    "      f\"検証に用いる大喜利の数: {len(test_boke_datas)}\\n\",\n",
    "      f\"使用する画像の数: {len(image_numbers)}\\n\",\n",
    "      f\"単語の種類: {len(index_to_word)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 画像の前処理\n",
    "image_preprocess = transforms.Compose([\n",
    "    transforms.Resize((IMAGE_HEIGHT, IMAGE_WIDTH)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# 画像のデータローダを作る関数\n",
    "def make_image_dataloader(image_paths, batch_size, num_workers = 4):\n",
    "\n",
    "    class LoadImageDataset(Dataset):\n",
    "        def __init__(self, image_paths):\n",
    "            \"\"\"\n",
    "                image_paths: 画像のパスからなるリスト\n",
    "            \"\"\"\n",
    "            self.image_paths = image_paths\n",
    "\n",
    "        def __len__(self):\n",
    "            return len(self.image_paths)\n",
    "\n",
    "        def __getitem__(self, idx):\n",
    "            image = Image.open(image_paths[idx]).convert(\"RGB\")\n",
    "\n",
    "            return image, self.image_paths[idx]\n",
    "    \n",
    "    def collate_fn_tf(batch):\n",
    "        images = torch.stack([image_preprocess(B[0]) for B in batch])\n",
    "        image_numbers = [B[1] for B in batch]\n",
    "\n",
    "        return images, image_numbers\n",
    "\n",
    "    print(f\"num data: {len(image_paths)}\")\n",
    "\n",
    "    dataset = LoadImageDataset(image_paths)\n",
    "    dataloader = DataLoader(\n",
    "        dataset, \n",
    "        batch_size = batch_size, \n",
    "        num_workers = num_workers, \n",
    "        collate_fn = collate_fn_tf\n",
    "    )\n",
    "\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 244286/244286 [00:00<00:00, 753413.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num data: 244286\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-11-db6fd902ca2e>:18: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  augmix_autoencoder.load_state_dict(torch.load(f\"{RESULT_DIR}best_augmix_autoencoder_weights.pth\"))\n",
      "100%|██████████| 1909/1909 [01:49<00:00, 17.39it/s]\n"
     ]
    }
   ],
   "source": [
    "# 画像を特徴量に変換する\n",
    "if RETRAIN_AUMIX_AUTOENCODER:\n",
    "    shutil.rmtree(IMAGE_FEATURE_DIR) \n",
    "    os.mkdir(IMAGE_FEATURE_DIR)\n",
    "\n",
    "tmp = list()\n",
    "for IN in tqdm(image_numbers):\n",
    "    if os.path.exists(f\"{IMAGE_FEATURE_DIR}{IN}.npy\"):\n",
    "        continue\n",
    "    tmp.append(f\"{IMAGE_DIR}{IN}.jpg\")\n",
    "\n",
    "if len(tmp) != 0:\n",
    "    image_dataloader = make_image_dataloader(tmp, batch_size = 128, num_workers = NUM_WORKERS)\n",
    "\n",
    "    # encoder of Autoencoder\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    augmix_autoencoder = AugmixAutoencoder(image_feature_dim = IMAGE_FEATURE_DIM)\n",
    "    augmix_autoencoder.load_state_dict(torch.load(f\"{RESULT_DIR}best_augmix_autoencoder_weights.pth\"))\n",
    "    model = augmix_autoencoder.encoder\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    for Is, IPs in tqdm(image_dataloader):\n",
    "        Is = Is.to(device)\n",
    "        features = model(Is).detach().cpu().numpy()\n",
    "\n",
    "        for f, IP in zip(features, IPs):\n",
    "            N = IP.split(\"/\")[-1].split(\".\")[0]\n",
    "            np.save(f\"{IMAGE_FEATURE_DIR}{N}\", f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 大喜利生成モデルのクラス\n",
    "class BokeGeneratorModel(nn.Module):\n",
    "    def __init__(self, num_word, image_feature_dim, sentence_length, embedding_dim = 512):\n",
    "        \"\"\"\n",
    "            num_word: 学習に用いる単語の総数\n",
    "            image_feature_dim: 画像の特徴量の次元数\n",
    "            sentence_length: 入力する文章の単語数\n",
    "            embedding_dim: 単語の埋め込み次元数\n",
    "        \"\"\"\n",
    "        super(BokeGeneratorModel, self).__init__()\n",
    "        self.num_word = num_word\n",
    "        self.image_feature_dim = image_feature_dim\n",
    "        self.sentence_length = sentence_length\n",
    "        self.embedding_dim = embedding_dim\n",
    "        \n",
    "        self.fc1 = nn.Linear(image_feature_dim, embedding_dim)\n",
    "        self.embedding = nn.Embedding(num_word, embedding_dim, padding_idx = 0)\n",
    "        self.lstm = nn.LSTM(input_size = embedding_dim, hidden_size = embedding_dim, \n",
    "                            batch_first = True)\n",
    "        self.fc2 = nn.Linear(embedding_dim + embedding_dim, 2 * embedding_dim)\n",
    "        self.fc3 = nn.Linear(2 * embedding_dim, 2 * embedding_dim)\n",
    "        self.fc4 = nn.Linear(2 * embedding_dim, num_word)\n",
    "    \n",
    "    # LSTMの初期値は0で，画像の特徴量と文章の特徴量を全結合層の前で結合する\n",
    "    def forward(self, image_features, sentences):\n",
    "        \"\"\"\n",
    "            image_features: 画像の特徴量\n",
    "            sentences: 入力する文章\n",
    "        \"\"\"\n",
    "        x1 = F.leaky_relu(self.fc1(image_features))\n",
    "        x1 = x1.unsqueeze(1).repeat(1, self.sentence_length, 1)\n",
    "\n",
    "        x2 = self.embedding(sentences)\n",
    "        x2, _ = self.lstm(x2)\n",
    "\n",
    "        x = torch.cat((x1, x2), dim = -1)\n",
    "        x = F.leaky_relu(self.fc2(x))\n",
    "        x = F.leaky_relu(self.fc3(x))\n",
    "\n",
    "        return self.fc4(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 大喜利生成AIの学習用データローダを作る関数\n",
    "def make_dataloader(boke_datas, max_sentence_length, batch_size, num_workers = 4):\n",
    "    \"\"\"\n",
    "        boke_datas: {\"image_number\":画像のお題番号 ,\"tokenized_boke\":トークナイズされた大喜利}からなるリスト\n",
    "        max_sentence_length: 学習データの最大単語数(<START>, <END>トークンを含まない)\n",
    "        num_workers: データローダが使用するCPUのスレッド数\n",
    "    \"\"\"\n",
    "    class SentenceGeneratorDataset(Dataset):\n",
    "        def __init__(self, image_file_numbers, sentences, teacher_signals):\n",
    "            \"\"\"\n",
    "                image_file_numbers: 画像の番号からなるリスト\n",
    "                sentences: 入力文章からなるリスト\n",
    "                teacher_signals: 教師信号からなるリスト\n",
    "            \"\"\"\n",
    "            if len(image_file_numbers) != len(sentences) and len(teacher_signals) != len(sentences):\n",
    "                raise ValueError(\"データリストの長さが一致しません\")\n",
    "\n",
    "            self.image_file_numbers = image_file_numbers\n",
    "            self.sentences = sentences\n",
    "            self.teacher_signals = teacher_signals\n",
    "\n",
    "        def __len__(self):\n",
    "            return len(self.teacher_signals)\n",
    "\n",
    "        def __getitem__(self, idx):\n",
    "            image_feature = np.load(f\"{IMAGE_FEATURE_DIR}{self.image_file_numbers[idx]}.npy\")\n",
    "            sentence = self.sentences[idx]\n",
    "            teacher_signal = self.teacher_signals[idx]\n",
    "\n",
    "            return image_feature, sentence, teacher_signal\n",
    "\n",
    "    def collate_fn_tf(batch):\n",
    "        image_features = torch.tensor(np.array([B[0] for B in batch]))\n",
    "        sentences = torch.tensor(np.array([B[1] for B in batch]))\n",
    "        teacher_signals = torch.tensor(np.array([B[2] for B in batch]))\n",
    "\n",
    "        return image_features, sentences, teacher_signals\n",
    "\n",
    "    image_file_numbers = list()\n",
    "    sentences = list()\n",
    "    teacher_signals = list()\n",
    "\n",
    "    for D in tqdm(boke_datas):\n",
    "        image_file_numbers.append(D[\"image_number\"])\n",
    "        tmp = D[\"tokenized_boke\"] + [0] * (2 + max_sentence_length - len(D[\"tokenized_boke\"]))\n",
    "        sentences.append(tmp[:-1])\n",
    "        teacher_signals.append(tmp[1:])\n",
    "\n",
    "    dataset = SentenceGeneratorDataset(image_file_numbers, sentences, teacher_signals)\n",
    "    dataloader = DataLoader(\n",
    "        dataset, \n",
    "        batch_size = batch_size, \n",
    "        num_workers = num_workers, \n",
    "        collate_fn = collate_fn_tf\n",
    "    )\n",
    "\n",
    "    print(f\"num data: {len(teacher_signals)}\")\n",
    "\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2030920/2030920 [00:05<00:00, 348734.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num data: 2030920\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20515/20515 [00:00<00:00, 626054.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num data: 20515\n"
     ]
    }
   ],
   "source": [
    "train_dataloader = make_dataloader(train_boke_datas, max_sentence_length = MAX_SENTENCE_LENGTH, batch_size = BATCH_SIZE_FOR_GENERATOR, num_workers = NUM_WORKERS)\n",
    "test_dataloader = make_dataloader(test_boke_datas, max_sentence_length = MAX_SENTENCE_LENGTH, batch_size = BATCH_SIZE_FOR_GENERATOR, num_workers = NUM_WORKERS)\n",
    "\n",
    "# モデルの学習\n",
    "train_loss_history = []\n",
    "train_accuracy_history = []\n",
    "test_loss_history = []\n",
    "test_accuracy_history = []\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = BokeGeneratorModel(num_word = len(index_to_word), \n",
    "                           image_feature_dim = IMAGE_FEATURE_DIM, \n",
    "                           sentence_length = MAX_SENTENCE_LENGTH + 1, \n",
    "                           embedding_dim = 2048)\n",
    "model.to(device)\n",
    "optimizer = optim.AdamW(model.parameters(), lr = LEARNING_RATO_FOR_GENERATOR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/25: 100%|██████████| 63467/63467 [50:56<00:00, 20.76it/s, train_loss=4.28, train_accuracy=0.305]\n",
      "Evaluating: 100%|██████████| 642/642 [00:09<00:00, 66.23it/s, test_loss=4, test_accuracy=0.333]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/25, Train Loss: 4.2770, Train Accuracy: 0.3052, Test Loss: 3.9826, Test Accuracy: 0.3316\n",
      "-------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/25: 100%|██████████| 63467/63467 [50:59<00:00, 20.75it/s, train_loss=3.82, train_accuracy=0.348]\n",
      "Evaluating: 100%|██████████| 642/642 [00:09<00:00, 66.10it/s, test_loss=3.92, test_accuracy=0.344]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2/25, Train Loss: 3.8172, Train Accuracy: 0.3478, Test Loss: 3.9040, Test Accuracy: 0.3428\n",
      "-------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/25: 100%|██████████| 63467/63467 [50:58<00:00, 20.75it/s, train_loss=3.63, train_accuracy=0.366]\n",
      "Evaluating: 100%|██████████| 642/642 [00:09<00:00, 66.18it/s, test_loss=3.93, test_accuracy=0.344]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3/25, Train Loss: 3.6319, Train Accuracy: 0.3661, Test Loss: 3.9276, Test Accuracy: 0.3436\n",
      "-------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/25: 100%|██████████| 63467/63467 [50:59<00:00, 20.75it/s, train_loss=3.49, train_accuracy=0.38] \n",
      "Evaluating: 100%|██████████| 642/642 [00:09<00:00, 66.03it/s, test_loss=4.01, test_accuracy=0.343]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4/25, Train Loss: 3.4883, Train Accuracy: 0.3805, Test Loss: 3.9954, Test Accuracy: 0.3417\n",
      "-------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/25: 100%|██████████| 63467/63467 [50:57<00:00, 20.76it/s, train_loss=3.37, train_accuracy=0.393]\n",
      "Evaluating: 100%|██████████| 642/642 [00:09<00:00, 66.13it/s, test_loss=4.11, test_accuracy=0.338]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5/25, Train Loss: 3.3656, Train Accuracy: 0.3934, Test Loss: 4.0932, Test Accuracy: 0.3372\n",
      "-------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/25: 100%|██████████| 63467/63467 [50:57<00:00, 20.75it/s, train_loss=3.26, train_accuracy=0.405]\n",
      "Evaluating: 100%|██████████| 642/642 [00:09<00:00, 66.02it/s, test_loss=4.21, test_accuracy=0.334]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6/25, Train Loss: 3.2575, Train Accuracy: 0.4052, Test Loss: 4.1975, Test Accuracy: 0.3325\n",
      "-------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/25: 100%|██████████| 63467/63467 [50:57<00:00, 20.76it/s, train_loss=3.16, train_accuracy=0.416]\n",
      "Evaluating: 100%|██████████| 642/642 [00:09<00:00, 66.29it/s, test_loss=4.33, test_accuracy=0.329]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7/25, Train Loss: 3.1622, Train Accuracy: 0.4164, Test Loss: 4.3182, Test Accuracy: 0.3279\n",
      "-------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/25: 100%|██████████| 63467/63467 [50:57<00:00, 20.76it/s, train_loss=3.08, train_accuracy=0.427]\n",
      "Evaluating: 100%|██████████| 642/642 [00:09<00:00, 66.24it/s, test_loss=4.45, test_accuracy=0.326]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8/25, Train Loss: 3.0771, Train Accuracy: 0.4266, Test Loss: 4.4326, Test Accuracy: 0.3249\n",
      "-------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/25: 100%|██████████| 63467/63467 [50:59<00:00, 20.74it/s, train_loss=3, train_accuracy=0.436]   \n",
      "Evaluating: 100%|██████████| 642/642 [00:09<00:00, 66.04it/s, test_loss=4.56, test_accuracy=0.324]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9/25, Train Loss: 3.0017, Train Accuracy: 0.4359, Test Loss: 4.5477, Test Accuracy: 0.3234\n",
      "-------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/25: 100%|██████████| 63467/63467 [50:58<00:00, 20.75it/s, train_loss=2.93, train_accuracy=0.445]\n",
      "Evaluating: 100%|██████████| 642/642 [00:09<00:00, 66.37it/s, test_loss=4.67, test_accuracy=0.32] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10/25, Train Loss: 2.9334, Train Accuracy: 0.4447, Test Loss: 4.6569, Test Accuracy: 0.3193\n",
      "-------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/25: 100%|██████████| 63467/63467 [50:56<00:00, 20.77it/s, train_loss=2.87, train_accuracy=0.453]\n",
      "Evaluating: 100%|██████████| 642/642 [00:09<00:00, 66.35it/s, test_loss=4.78, test_accuracy=0.319]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 11/25, Train Loss: 2.8711, Train Accuracy: 0.4529, Test Loss: 4.7618, Test Accuracy: 0.3178\n",
      "-------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/25: 100%|██████████| 63467/63467 [50:55<00:00, 20.77it/s, train_loss=2.81, train_accuracy=0.46] \n",
      "Evaluating: 100%|██████████| 642/642 [00:09<00:00, 65.77it/s, test_loss=4.88, test_accuracy=0.316]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 12/25, Train Loss: 2.8148, Train Accuracy: 0.4604, Test Loss: 4.8604, Test Accuracy: 0.3145\n",
      "-------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/25: 100%|██████████| 63467/63467 [50:57<00:00, 20.76it/s, train_loss=2.76, train_accuracy=0.468]\n",
      "Evaluating: 100%|██████████| 642/642 [00:09<00:00, 65.62it/s, test_loss=5, test_accuracy=0.309]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 13/25, Train Loss: 2.7637, Train Accuracy: 0.4675, Test Loss: 4.9658, Test Accuracy: 0.3070\n",
      "-------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/25: 100%|██████████| 63467/63467 [50:59<00:00, 20.75it/s, train_loss=2.72, train_accuracy=0.474]\n",
      "Evaluating: 100%|██████████| 642/642 [00:09<00:00, 66.13it/s, test_loss=5.06, test_accuracy=0.309]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 14/25, Train Loss: 2.7163, Train Accuracy: 0.4742, Test Loss: 5.0482, Test Accuracy: 0.3076\n",
      "-------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/25: 100%|██████████| 63467/63467 [50:56<00:00, 20.76it/s, train_loss=2.67, train_accuracy=0.48] \n",
      "Evaluating: 100%|██████████| 642/642 [00:09<00:00, 66.19it/s, test_loss=5.13, test_accuracy=0.311]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 15/25, Train Loss: 2.6730, Train Accuracy: 0.4802, Test Loss: 5.1181, Test Accuracy: 0.3103\n",
      "-------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16/25: 100%|██████████| 63467/63467 [50:57<00:00, 20.75it/s, train_loss=2.63, train_accuracy=0.486]\n",
      "Evaluating: 100%|██████████| 642/642 [00:09<00:00, 66.01it/s, test_loss=5.22, test_accuracy=0.307]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 16/25, Train Loss: 2.6331, Train Accuracy: 0.4859, Test Loss: 5.2023, Test Accuracy: 0.3064\n",
      "-------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17/25: 100%|██████████| 63467/63467 [50:58<00:00, 20.75it/s, train_loss=2.6, train_accuracy=0.491]\n",
      "Evaluating: 100%|██████████| 642/642 [00:09<00:00, 66.15it/s, test_loss=5.3, test_accuracy=0.308] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 17/25, Train Loss: 2.5965, Train Accuracy: 0.4912, Test Loss: 5.2792, Test Accuracy: 0.3070\n",
      "-------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18/25: 100%|██████████| 63467/63467 [50:58<00:00, 20.75it/s, train_loss=2.56, train_accuracy=0.496]\n",
      "Evaluating: 100%|██████████| 642/642 [00:09<00:00, 66.29it/s, test_loss=5.34, test_accuracy=0.305]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 18/25, Train Loss: 2.5632, Train Accuracy: 0.4963, Test Loss: 5.3304, Test Accuracy: 0.3044\n",
      "-------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19/25: 100%|██████████| 63467/63467 [58:37<00:00, 18.04it/s, train_loss=2.53, train_accuracy=0.501]\n",
      "Evaluating: 100%|██████████| 642/642 [00:10<00:00, 64.06it/s, test_loss=5.42, test_accuracy=0.305]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 19/25, Train Loss: 2.5318, Train Accuracy: 0.5010, Test Loss: 5.4048, Test Accuracy: 0.3043\n",
      "-------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20/25: 100%|██████████| 63467/63467 [58:36<00:00, 18.05it/s, train_loss=2.5, train_accuracy=0.505]   \n",
      "Evaluating: 100%|██████████| 642/642 [00:09<00:00, 65.13it/s, test_loss=5.49, test_accuracy=0.308]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 20/25, Train Loss: 2.5025, Train Accuracy: 0.5054, Test Loss: 5.4774, Test Accuracy: 0.3068\n",
      "-------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 21/25: 100%|██████████| 63467/63467 [58:49<00:00, 17.98it/s, train_loss=2.48, train_accuracy=0.509]  \n",
      "Evaluating: 100%|██████████| 642/642 [00:09<00:00, 65.29it/s, test_loss=5.55, test_accuracy=0.306]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 21/25, Train Loss: 2.4756, Train Accuracy: 0.5095, Test Loss: 5.5283, Test Accuracy: 0.3049\n",
      "-------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 22/25: 100%|██████████| 63467/63467 [57:47<00:00, 18.31it/s, train_loss=2.45, train_accuracy=0.513]\n",
      "Evaluating: 100%|██████████| 642/642 [00:09<00:00, 65.34it/s, test_loss=5.58, test_accuracy=0.3]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 22/25, Train Loss: 2.4508, Train Accuracy: 0.5134, Test Loss: 5.5690, Test Accuracy: 0.2993\n",
      "-------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 23/25: 100%|██████████| 63467/63467 [51:18<00:00, 20.62it/s, train_loss=2.43, train_accuracy=0.517]\n",
      "Evaluating: 100%|██████████| 642/642 [00:09<00:00, 65.34it/s, test_loss=5.64, test_accuracy=0.299]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 23/25, Train Loss: 2.4286, Train Accuracy: 0.5167, Test Loss: 5.6238, Test Accuracy: 0.2986\n",
      "-------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 24/25: 100%|██████████| 63467/63467 [51:22<00:00, 20.59it/s, train_loss=2.41, train_accuracy=0.52] \n",
      "Evaluating: 100%|██████████| 642/642 [00:09<00:00, 65.33it/s, test_loss=5.7, test_accuracy=0.306] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 24/25, Train Loss: 2.4081, Train Accuracy: 0.5199, Test Loss: 5.6951, Test Accuracy: 0.3055\n",
      "-------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 25/25: 100%|██████████| 63467/63467 [51:10<00:00, 20.67it/s, train_loss=2.39, train_accuracy=0.523]\n",
      "Evaluating: 100%|██████████| 642/642 [00:09<00:00, 65.67it/s, test_loss=5.75, test_accuracy=0.305]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 25/25, Train Loss: 2.3892, Train Accuracy: 0.5230, Test Loss: 5.7355, Test Accuracy: 0.3038\n",
      "-------------------------\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 文章生成の精度を計算する関数\n",
    "def calculate_accuracy(teacher_signals, outputs):\n",
    "    \"\"\"\n",
    "        teacher_signals: 教師信号\n",
    "        outputs: モデルの出力\n",
    "    \"\"\"\n",
    "    _, predicted_words = outputs.max(dim = -1)\n",
    "    # パディングに対して精度を計算しない\n",
    "    mask = (teacher_signals != 0)\n",
    "    correct = ((predicted_words == teacher_signals) & mask).sum().item()\n",
    "    total = mask.sum().item()\n",
    "    accuracy = correct / total if total > 0 else 0.0\n",
    "    return accuracy\n",
    "\n",
    "# 1イテレーション学習する関数\n",
    "def train_step(model, optimizer, batch_data, batch_labels):\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(*batch_data)\n",
    "    # パディングに対して損失を計算しない\n",
    "    loss = F.cross_entropy(outputs.view(-1, outputs.size(-1)), batch_labels.view(-1),\n",
    "                           ignore_index = 0)\n",
    "    accuracy = calculate_accuracy(batch_labels, F.softmax(outputs, dim = -1))\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss.item(), accuracy\n",
    "\n",
    "# 1イテレーション検証する関数\n",
    "def evaluate(model, batch_data, batch_labels):\n",
    "    with torch.no_grad():\n",
    "        outputs = model(*batch_data)\n",
    "        loss = F.cross_entropy(outputs.view(-1, outputs.size(-1)), batch_labels.view(-1),\n",
    "                               ignore_index = 0)\n",
    "        accuracy = calculate_accuracy(batch_labels, F.softmax(outputs, dim = -1))\n",
    "    return loss.item(), accuracy\n",
    "\n",
    "for epoch in range(EPOCH_FOR_GENERATOR):\n",
    "\n",
    "    # train\n",
    "    train_loss_obj = 0.0\n",
    "    train_accuracy_obj = 0.0\n",
    "    model.train()\n",
    "    pb = tqdm(train_dataloader, desc = f\"Epoch {epoch+1}/{EPOCH_FOR_GENERATOR}\")\n",
    "    \n",
    "    for image_features, sentences, teacher_signals in pb:\n",
    "        image_features = image_features.to(device)\n",
    "        sentences = sentences.to(device)\n",
    "        teacher_signals = teacher_signals.to(device)\n",
    "        \n",
    "        loss, accuracy = train_step(model, optimizer, (image_features, sentences), teacher_signals)\n",
    "        train_loss_obj += loss\n",
    "        train_accuracy_obj += accuracy\n",
    "        pb.set_postfix({\"train_loss\": train_loss_obj / (pb.n + 1), \"train_accuracy\": train_accuracy_obj / (pb.n + 1)})\n",
    "\n",
    "    train_loss = train_loss_obj / len(train_dataloader)\n",
    "    train_accuracy = train_accuracy_obj / len(train_dataloader)\n",
    "\n",
    "    # test\n",
    "    test_loss_obj = 0.0\n",
    "    test_accuracy_obj = 0.0\n",
    "    model.eval()\n",
    "    pb = tqdm(test_dataloader, desc = \"Evaluating\")\n",
    "\n",
    "    for image_features, sentences, teacher_signals in pb:\n",
    "        image_features = image_features.to(device)\n",
    "        sentences = sentences.to(device)\n",
    "        teacher_signals = teacher_signals.to(device)\n",
    "\n",
    "        loss, accuracy = evaluate(model, (image_features, sentences), teacher_signals)\n",
    "        test_loss_obj += loss\n",
    "        test_accuracy_obj += accuracy\n",
    "        pb.set_postfix({\"test_loss\": test_loss_obj / (pb.n + 1), \"test_accuracy\": test_accuracy_obj / (pb.n + 1)})\n",
    "\n",
    "    test_loss = test_loss_obj / len(test_dataloader)\n",
    "    test_accuracy = test_accuracy_obj / len(test_dataloader)\n",
    "\n",
    "    print(f\"Epoch: {epoch+1}/{EPOCH_FOR_GENERATOR}, \"\n",
    "          f\"Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, \"\n",
    "          f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}\")\n",
    "    print(\"-\" * 25)\n",
    "\n",
    "    train_loss_history.append(train_loss)\n",
    "    train_accuracy_history.append(train_accuracy)\n",
    "    test_loss_history.append(test_loss)\n",
    "    test_accuracy_history.append(test_accuracy)\n",
    "\n",
    "    # 学習精度を更新した場合、重みを保存\n",
    "    if max(train_accuracy_history) == train_accuracy:\n",
    "        torch.save(model.state_dict(), f\"{RESULT_DIR}best_model_weights.pth\")\n",
    "\n",
    "# 学習結果を保存\n",
    "with open(f\"{RESULT_DIR}history.json\", \"w\") as f:\n",
    "    json.dump({\n",
    "        \"train_loss\": train_loss_history,\n",
    "        \"train_accuracy\": train_accuracy_history,\n",
    "        \"test_loss\": test_loss_history,\n",
    "        \"test_accuracy\": test_accuracy_history\n",
    "    }, f)\n",
    "\n",
    "# 学習結果を描画\n",
    "fig = plt.figure(figsize = (10, 5))\n",
    "ax = fig.add_subplot(1, 2, 1)\n",
    "ax.plot(train_loss_history, label = \"train\")\n",
    "ax.plot(test_loss_history, label = \"test\")\n",
    "ax.set_xlabel(\"epoch\")\n",
    "ax.set_ylabel(\"loss\")\n",
    "ax.legend()\n",
    "ax.grid()\n",
    "\n",
    "ax = fig.add_subplot(1, 2, 2)\n",
    "ax.plot(train_accuracy_history, label = \"train\")\n",
    "ax.plot(test_accuracy_history, label = \"test\")\n",
    "ax.set_xlabel(\"epoch\")\n",
    "ax.set_ylabel(\"accuracy\")\n",
    "ax.legend()\n",
    "ax.grid()\n",
    "\n",
    "fig.savefig(f\"{RESULT_DIR}history.png\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Colab_20241111",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
