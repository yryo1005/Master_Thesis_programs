{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import random\n",
    "import shutil\n",
    "import argparse\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import subprocess\n",
    "if not os.path.exists(\"Japanese_BPEEncoder_V2\"):\n",
    "    subprocess.run([\"git\", \"clone\", \"https://github.com/tanreinama/Japanese-BPEEncoder_V2.git\", \"Japanese_BPEEncoder_V2\"])\n",
    "from Japanese_BPEEncoder_V2.encode_swe import SWEEncoder_ja"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ###\n",
    "# # コマンドライン引数の処理\n",
    "# parser = argparse.ArgumentParser(description=\"設定用プログラム\")\n",
    "\n",
    "# parser.add_argument(\"--num_workers\", type = int, default = 16, help = \"データローダが使用するCPUのスレッド数(GPUの総スレッド数の8割が推奨)\")\n",
    "# parser.add_argument(\"--reset_data\", action = \"store_true\", help = \"データセットを再作成するか\")\n",
    "# parser.add_argument(\"--retrain_augmix_autoencoder\", action = \"store_true\", help = \"Augmix Autoencoderを再学習するか\")\n",
    "# parser.add_argument(\"--use_unreal_image\", action = \"store_true\", help = \"現実写真以外を使用する\")\n",
    "# parser.add_argument(\"--use_word_image\", action = \"store_true\", help = \"文字を含む画像を使用する\")\n",
    "# parser.add_argument(\"--use_unique_noun_boke\", action = \"store_true\", help = \"固有名詞を含む大喜利を使用する\")\n",
    "# parser.add_argument(\"--min_star\", type = int, default = 0, help = \"大喜利の最小の星の数\")\n",
    "# parser.add_argument(\"--min_apper_word\", type = int, default = 32, help = \"単語の最小出現回数\")\n",
    "# parser.add_argument(\"--min_sentence_length\", type = int, default = 4, help = \"大喜利の最小単語数\")\n",
    "# parser.add_argument(\"--max_sentence_length\", type = int, default = 31, help = \"大喜利の最大単語数\")\n",
    "# parser.add_argument(\"--image_height\", type = int, default = 128, help = \"画像の高さ\")\n",
    "# parser.add_argument(\"--image_width\", type = int, default = 128, help = \"画像の幅\")\n",
    "# parser.add_argument(\"--amae_feature_dim\", type = int, default = 16384, help = \"AMAEの特徴量次元数\")\n",
    "# parser.add_argument(\"--amae_epoch\", type = int, default = 25, help = \"AMAEの学習反復回数\")\n",
    "# parser.add_argument(\"--amae_batch_size\", type = int, default = 32, help = \"AMAEのバッチサイズ\")\n",
    "# parser.add_argument(\"--amae_learning_rate\", type = float, default = 0.0001, help = \"AMAEの学習率\")\n",
    "# parser.add_argument(\"--amae_alpha\", type = float, default = 1.0, help = \"AMAEの制約項調整パラメータ\")\n",
    "# parser.add_argument(\"--amae_chain\", type = float, default = 1.0, help = \"Augmixで何枚の画像に加工するか\")\n",
    "# parser.add_argument(\"--amae_depth\", type = float, default = 1.0, help = \"Augmixで1枚に最大何種類の加工をするか\")\n",
    "# parser.add_argument(\"--epoch\", type = int, default = 25, help = \"学習反復回数\")\n",
    "# parser.add_argument(\"--batch_size\", type = int, default = 512, help = \"バッチサイズ\")\n",
    "# parser.add_argument(\"--learning_rate\", type = float, default = 0.0001, help = \"学習率\")\n",
    "# parser.add_argument(\"--feature_dim\", type = int, default = 1024, help = \"モデルの特徴量次元数\")\n",
    "\n",
    "# args = parser.parse_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "result directory: ../../results/Augmix_Autoencoder/False_False_False_0_32_4_31_128_128_25_32_0.0001_16384_10_3_3/\n",
      "result directory: ../../results/GUMI_AMAE/False_False_False_0_32_4_31_128_128_25_32_0.0001_16384_10_3_3_25_512_0.0001_1024/\n"
     ]
    }
   ],
   "source": [
    "###\n",
    "NUM_WORKERS = 16 # args.num_workers\n",
    "RESET_DATA = False # args.reset_data\n",
    "\n",
    "USE_UNREAL_IMAGE = False # args.use_unreal_image\n",
    "USE_WORD_IMAGE = False # args.use_word_image\n",
    "USE_UNIQUE_NOUN_BOKE = False # args.use_unique_noun_boke\n",
    "\n",
    "MIN_STAR = 0 # args.min_star\n",
    "MIN_APPER_WORD = 32 # args.min_apper_word\n",
    "MIN_SENTENCE_LENGTH = 4 # args.min_sentence_length\n",
    "MAX_SENTENCE_LENGTH = 31 # args.max_sentence_length\n",
    "\n",
    "RETRAIN_AUGMIX_AUTOENCODER = False # args.retrain_augmix_autoencoder\n",
    "IMAGE_HEIGHT = 128 # args.image_height\n",
    "IMAGE_WIDTH = 128 # args.image_width\n",
    "AMAE_EPOCH = 25 # args.amae_epoch\n",
    "AMAE_BATCH_SIZE = 32 # args.amae_batch_size\n",
    "AMAE_LEARNING_RATE = 0.0001 # args.amae_learning_rate\n",
    "AMAE_FEATURE_DIM = 16384 # args.amae_feature_dim\n",
    "AMAE_ALPHA = 10 # args.amae_alpha\n",
    "AMAE_CHAIN = 3 # args.amae_chain\n",
    "AMAE_DEPTH = 3 # args.amae_depth\n",
    "\n",
    "EPOCH = 25 # args.epoch\n",
    "BATCH_SIZE = 512 # args.batch_size\n",
    "LEARNING_RATE = 0.0001 # args.learning_rate\n",
    "FEATURE_DIM = 1024 # args.feature_dim\n",
    "\n",
    "AMAE_RESULT_DIR = f\"../../results/Augmix_Autoencoder/{USE_UNREAL_IMAGE}_{USE_WORD_IMAGE}_{USE_UNIQUE_NOUN_BOKE}_{MIN_STAR}_{MIN_APPER_WORD}_{MIN_SENTENCE_LENGTH}_{MAX_SENTENCE_LENGTH}_{IMAGE_HEIGHT}_{IMAGE_WIDTH}_{AMAE_EPOCH}_{AMAE_BATCH_SIZE}_{AMAE_LEARNING_RATE}_{AMAE_FEATURE_DIM}_{AMAE_ALPHA}_{AMAE_CHAIN}_{AMAE_DEPTH}/\"\n",
    "\n",
    "if not os.path.exists(\"../../results/Augmix_Autoencoder/\"):\n",
    "    os.mkdir(\"../../results/Augmix_Autoencoder/\")\n",
    "if not os.path.exists(AMAE_RESULT_DIR):\n",
    "    os.mkdir(AMAE_RESULT_DIR)\n",
    "print(f\"result directory: {AMAE_RESULT_DIR}\")\n",
    "with open(f\"{AMAE_RESULT_DIR}config.json\", \"w\") as f:\n",
    "    json.dump({\n",
    "        \"USE_UNREAL_IMAGE\": USE_UNREAL_IMAGE,\n",
    "        \"USE_WORD_IMAGE\": USE_WORD_IMAGE,\n",
    "        \"USE_UNIQUE_NOUN_BOKE\": USE_UNIQUE_NOUN_BOKE,\n",
    "        \"MIN_STAR\": MIN_STAR,\n",
    "        \"MIN_APPER_WORD\": MIN_APPER_WORD,\n",
    "        \"MIN_SENTENCE_LENGTH\": MIN_SENTENCE_LENGTH,\n",
    "        \"MAX_SENTENCE_LENGTH\": MAX_SENTENCE_LENGTH,\n",
    "        \"IMAGE_HEIGHT\": IMAGE_HEIGHT,\n",
    "        \"IMAGE_WIDTH\": IMAGE_WIDTH,\n",
    "        \"EPOCH\": AMAE_EPOCH,\n",
    "        \"BATCH_SIZE\": AMAE_BATCH_SIZE,\n",
    "        \"LEARNING_RATE\": AMAE_LEARNING_RATE,\n",
    "        \"FEATURE_DIM\": AMAE_FEATURE_DIM,\n",
    "        \"ALPHA\": AMAE_ALPHA,\n",
    "        \"CAHIN\": AMAE_CHAIN,\n",
    "        \"DEPTH\": AMAE_DEPTH\n",
    "    }, f)\n",
    "\n",
    "RESULT_DIR = f\"../../results/GUMI_AMAE/{USE_UNREAL_IMAGE}_{USE_WORD_IMAGE}_{USE_UNIQUE_NOUN_BOKE}_{MIN_STAR}_{MIN_APPER_WORD}_{MIN_SENTENCE_LENGTH}_{MAX_SENTENCE_LENGTH}_{IMAGE_HEIGHT}_{IMAGE_WIDTH}_{AMAE_EPOCH}_{AMAE_BATCH_SIZE}_{AMAE_LEARNING_RATE}_{AMAE_FEATURE_DIM}_{AMAE_ALPHA}_{AMAE_CHAIN}_{AMAE_DEPTH}_{EPOCH}_{BATCH_SIZE}_{LEARNING_RATE}_{FEATURE_DIM}/\"\n",
    "\n",
    "if not os.path.exists(\"../../results/GUMI_AMAE/\"):\n",
    "    os.mkdir(\"../../results/GUMI_AMAE/\")\n",
    "if not os.path.exists(RESULT_DIR):\n",
    "    os.mkdir(RESULT_DIR)\n",
    "print(f\"result directory: {RESULT_DIR}\")\n",
    "with open(f\"{RESULT_DIR}config.json\", \"w\") as f:\n",
    "    json.dump({\n",
    "        \"USE_UNREAL_IMAGE\": USE_UNREAL_IMAGE,\n",
    "        \"USE_WORD_IMAGE\": USE_WORD_IMAGE,\n",
    "        \"USE_UNIQUE_NOUN_BOKE\": USE_UNIQUE_NOUN_BOKE,\n",
    "        \"MIN_STAR\": MIN_STAR,\n",
    "        \"MIN_APPER_WORD\": MIN_APPER_WORD,\n",
    "        \"MIN_SENTENCE_LENGTH\": MIN_SENTENCE_LENGTH,\n",
    "        \"MAX_SENTENCE_LENGTH\": MAX_SENTENCE_LENGTH,\n",
    "        \"IMAGE_HEIGHT\": IMAGE_HEIGHT,\n",
    "        \"IMAGE_WIDTH\": IMAGE_WIDTH,\n",
    "        \"AMAE_EPOCH\": AMAE_EPOCH,\n",
    "        \"AMAE_BATCH_SIZE\": AMAE_BATCH_SIZE,\n",
    "        \"AMAE_LEARNING_RATE\": AMAE_LEARNING_RATE,\n",
    "        \"AMAE_FEATURE_DIM\": AMAE_FEATURE_DIM,\n",
    "        \"ALPHA\": AMAE_ALPHA,\n",
    "        \"CAHIN\": AMAE_CHAIN,\n",
    "        \"DEPTH\": AMAE_DEPTH,\n",
    "        \"EPOCH\": EPOCH,\n",
    "        \"BATCH_SIZE\": BATCH_SIZE,\n",
    "        \"LEARNING_RATE\": LEARNING_RATE,\n",
    "        \"FEATURE_DIM\": FEATURE_DIM,\n",
    "    }, f)\n",
    "\n",
    "DATA_DIR = \"../../datas/boke_data_assemble/\"\n",
    "IMAGE_DIR = \"../../datas/boke_image/\"\n",
    "\n",
    "IMAGE_FEATURE_DIR = f\"../../datas/encoded/Augmix_Autoencoder_{USE_UNREAL_IMAGE}_{USE_WORD_IMAGE}_{USE_UNIQUE_NOUN_BOKE}_{MIN_STAR}_{MIN_APPER_WORD}_{MIN_SENTENCE_LENGTH}_{MAX_SENTENCE_LENGTH}_{IMAGE_HEIGHT}_{IMAGE_WIDTH}_{AMAE_EPOCH}_{AMAE_BATCH_SIZE}_{AMAE_LEARNING_RATE}_{AMAE_FEATURE_DIM}_{AMAE_ALPHA}_{AMAE_CHAIN}_{AMAE_DEPTH}/\"\n",
    "if not os.path.exists(IMAGE_FEATURE_DIR):\n",
    "    os.mkdir(IMAGE_FEATURE_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AMAEの学習に用いる画像の数: 228908\n",
      " AmAEの検証に用いる画像の数: 25435\n",
      "\n"
     ]
    }
   ],
   "source": [
    "###\n",
    "if not os.path.exists(f\"{AMAE_RESULT_DIR}test_image_paths.json\") or RESET_DATA:\n",
    "    image_paths = list()\n",
    "\n",
    "    for IP in tqdm(os.listdir(IMAGE_DIR)):\n",
    "        \n",
    "        N = int(IP.split(\".\")[0])\n",
    "        if not os.path.exists(f\"{DATA_DIR}{N}.json\"):\n",
    "            continue\n",
    "\n",
    "        with open(f\"{DATA_DIR}{N}.json\", \"r\") as f:\n",
    "            a = json.load(f)\n",
    "        \n",
    "        image_information = a[\"image_information\"]\n",
    "        is_photographic_probability = image_information[\"is_photographic_probability\"]\n",
    "        ocr = image_information[\"ocr\"]\n",
    "\n",
    "        # 現実写真以外を除去\n",
    "        if not USE_UNREAL_IMAGE:\n",
    "            if is_photographic_probability < 0.8: continue\n",
    "            \n",
    "        # 文字のある画像を除去\n",
    "        if not USE_WORD_IMAGE:\n",
    "            if len(ocr) != 0: continue\n",
    "        \n",
    "        image_paths.append(f\"{IMAGE_DIR}{IP}\")\n",
    "\n",
    "    train_image_paths, test_image_paths = train_test_split(image_paths, test_size = 0.1)\n",
    "\n",
    "    with open(f\"{AMAE_RESULT_DIR}train_image_paths.json\", \"w\") as f:\n",
    "        json.dump(train_image_paths, f)\n",
    "    with open(f\"{AMAE_RESULT_DIR}test_image_paths.json\", \"w\") as f:\n",
    "        json.dump(test_image_paths, f)\n",
    "    \n",
    "else:\n",
    "    with open(f\"{AMAE_RESULT_DIR}train_image_paths.json\", \"r\") as f:\n",
    "        train_image_paths = json.load(f)\n",
    "    with open(f\"{AMAE_RESULT_DIR}test_image_paths.json\", \"r\") as f:\n",
    "        test_image_paths = json.load(f)\n",
    "\n",
    "print(f\"AMAEの学習に用いる画像の数: {len(train_image_paths)}\\n\",\n",
    "      f\"AmAEの検証に用いる画像の数: {len(test_image_paths)}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def autocontrast(pil_img):\n",
    "    return ImageOps.autocontrast(pil_img)\n",
    "\n",
    "def equalize(pil_img):\n",
    "    return ImageOps.equalize(pil_img)\n",
    "\n",
    "def posterize(pil_img):\n",
    "    f = random.randint(4, 7)\n",
    "    return ImageOps.posterize(pil_img, 8 - f)\n",
    "\n",
    "def rotate(pil_img):\n",
    "    d = random.uniform(-10.0, 10.0)\n",
    "    return pil_img.rotate(d, resample = Image.BILINEAR)\n",
    "\n",
    "def solarize(pil_img):\n",
    "    f = random.randint(0, 255)\n",
    "    return ImageOps.solarize(pil_img, 255 - f)\n",
    "\n",
    "def shear_x(pil_img):\n",
    "    w, h = pil_img.size\n",
    "    f = random.uniform(-0.1, 0.1)\n",
    "    return pil_img.transform((w, h),\n",
    "                             Image.AFFINE, (1, f, 0, 0, 1, 0),\n",
    "                             resample = Image.BILINEAR)\n",
    "\n",
    "def shear_y(pil_img):\n",
    "    w, h = pil_img.size\n",
    "    f = random.uniform(-0.1, 0.1)\n",
    "    return pil_img.transform((w, h),\n",
    "                             Image.AFFINE, (1, 0, 0, f, 1, 0),\n",
    "                             resample = Image.BILINEAR)\n",
    "\n",
    "def center_shear_x(pil_img):\n",
    "    w, h = pil_img.size\n",
    "    f = random.uniform(-0.1, 0.1)\n",
    "    return pil_img.transform((w, h),\n",
    "                             Image.AFFINE, (1, f, -f * w / 2, 0, 1, 0),\n",
    "                             resample = Image.BILINEAR)\n",
    "\n",
    "def center_shear_y(pil_img):\n",
    "    w, h = pil_img.size\n",
    "    f = random.uniform(-0.1, 0.1)\n",
    "    return pil_img.transform((w, h),\n",
    "                             Image.AFFINE, (1, 0, 0, f, 1, -f * h / 2),\n",
    "                             resample = Image.BILINEAR)\n",
    "\n",
    "def translate_x(pil_img):\n",
    "    w, h = pil_img.size\n",
    "    f = random.uniform(-0.1, 0.1) * w\n",
    "    return pil_img.transform((w, h),\n",
    "                             Image.AFFINE, (1, 0, f, 0, 1, 0),\n",
    "                             resample = Image.BILINEAR)\n",
    "\n",
    "def translate_y(pil_img):\n",
    "    w, h = pil_img.size\n",
    "    f = random.uniform(-0.1, 0.1) * h\n",
    "    return pil_img.transform((w, h),\n",
    "                             Image.AFFINE, (1, 0, 0, 0, 1, f),\n",
    "                             resample = Image.BILINEAR)\n",
    "\n",
    "def downsampling(pil_img):\n",
    "    w, h = pil_img.size\n",
    "    th = random.randint(w // 8, h // 2)\n",
    "    tw = random.randint(w // 8, h // 2)\n",
    "    return pil_img.resize((tw, th)).resize((w, h))\n",
    "\n",
    "def add_noise(pil_img):\n",
    "    w, h = pil_img.size\n",
    "    v = random.randint(0, 10)\n",
    "    n = np.random.normal(0, v, (h, w, 3))\n",
    "    return Image.fromarray((pil_img + n).astype(np.uint8))\n",
    "\n",
    "def choice_random_operation():\n",
    "    idx = random.randint(0, len(operations) - 1)\n",
    "    return operations[idx]\n",
    "\n",
    "def augmix_operation(pil_img, chain = 3, max_depth = 3):\n",
    "    width, height = pil_img.size\n",
    "\n",
    "    mixed_img = np.zeros((height, width, 3))\n",
    "\n",
    "    weights = np.random.dirichlet([1.0] * chain)\n",
    "\n",
    "    for W in weights:\n",
    "        img = pil_img.copy()\n",
    "\n",
    "        for _ in range(random.randint(1, max_depth)):\n",
    "            operation = choice_random_operation()\n",
    "            img = operation(img)\n",
    "\n",
    "        mixed_img += W * img\n",
    "\n",
    "    weight = np.random.uniform(0, 1)\n",
    "    result = weight * np.array(pil_img) + (1 - weight) * np.array(mixed_img)\n",
    "\n",
    "    return result\n",
    "\n",
    "operations = [\n",
    "            # autocontrast, equalize, posterize, solarize,\n",
    "            rotate,\n",
    "            # shear_x, shear_y,\n",
    "            center_shear_x, center_shear_y,\n",
    "            translate_x, translate_y,\n",
    "            downsampling,\n",
    "            add_noise\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 画像の前処理\n",
    "image_preprocess = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# augmixのデータローダを作る関数\n",
    "def make_augmix_image_dataloader(image_paths, batch_size, chain, max_depth, num_workers = 4):\n",
    "\n",
    "    class LoadImageDataset(Dataset):\n",
    "        def __init__(self, image_paths):\n",
    "            \"\"\"\n",
    "                image_paths: 画像のパスからなるリスト\n",
    "            \"\"\"\n",
    "            self.image_paths = image_paths\n",
    "\n",
    "        def __len__(self):\n",
    "            return len(self.image_paths)\n",
    "\n",
    "        def __getitem__(self, idx):\n",
    "            image = Image.open(image_paths[idx]).convert(\"RGB\")\n",
    "\n",
    "            return image, self.image_paths[idx]\n",
    "    \n",
    "    def collate_fn_tf(batch):\n",
    "        images = [B[0].resize((IMAGE_WIDTH, IMAGE_HEIGHT)) for B in batch]\n",
    "        augmix_images1 = torch.stack([ image_preprocess( augmix_operation(I, chain = chain, max_depth = max_depth) / 255.0 ) for I in images ])\n",
    "        augmix_images2 = torch.stack([ image_preprocess( augmix_operation(I, chain = chain, max_depth = max_depth) / 255.0 ) for I in images ])\n",
    "        images = torch.stack([image_preprocess(I) for I in images])\n",
    "        image_numbers = [B[1] for B in batch]\n",
    "\n",
    "        return images, augmix_images1, augmix_images2, image_numbers\n",
    "\n",
    "    print(f\"num data: {len(image_paths)}\")\n",
    "\n",
    "    dataset = LoadImageDataset(image_paths)\n",
    "    dataloader = DataLoader(\n",
    "        dataset, \n",
    "        batch_size = batch_size, \n",
    "        num_workers = num_workers, \n",
    "        collate_fn = collate_fn_tf\n",
    "    )\n",
    "\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "###\n",
    "class ImageEncoder(nn.Module):\n",
    "    def __init__(self, image_feature_dim):\n",
    "        \"\"\"\n",
    "            image_feature_dim: \n",
    "        \"\"\"\n",
    "        super(ImageEncoder, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size = 3, stride = 2, padding = 1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size = 3, stride = 2, padding = 1)\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size = 3, stride = 2, padding = 1)\n",
    "        self.conv4 = nn.Conv2d(128, 256, kernel_size = 3, stride = 2, padding = 1)\n",
    "\n",
    "        self.fc1 = nn.Linear(16384, 16384)\n",
    "        self.fc2 = nn.Linear(16384, image_feature_dim)\n",
    "    \n",
    "    def forward(self, images):\n",
    "        x = F.leaky_relu( self.conv1(images) )\n",
    "        # 32, 64, 64\n",
    "        x = F.leaky_relu( self.conv2(x) )\n",
    "        # 64, 32, 32\n",
    "        x = F.leaky_relu( self.conv3(x) )\n",
    "        # 128, 16, 16\n",
    "        x = F.leaky_relu( self.conv4(x) )\n",
    "        # 256, 8, 8\n",
    "\n",
    "        x = nn.Flatten()(x)\n",
    "        x = F.leaky_relu(self.fc1(x))\n",
    "        return F.leaky_relu(self.fc2(x))\n",
    "\n",
    "class ImageDecoder(nn.Module):\n",
    "    def __init__(self, image_feature_dim):\n",
    "        \"\"\"\n",
    "            image_feature_dim: \n",
    "        \"\"\"\n",
    "        super(ImageDecoder, self).__init__()\n",
    "\n",
    "        self.fc1 = nn.Linear(image_feature_dim, 16384)\n",
    "\n",
    "        self.deconv1 = nn.ConvTranspose2d(256, 128, kernel_size = 3, stride = 2, padding = 1, output_padding = 1)\n",
    "        self.deconv2 = nn.ConvTranspose2d(128, 64, kernel_size = 3, stride = 2, padding = 1, output_padding = 1)\n",
    "        self.deconv3 = nn.ConvTranspose2d(64, 32, kernel_size = 3, stride = 2, padding = 1, output_padding = 1)\n",
    "        self.deconv4 = nn.ConvTranspose2d(32, 32, kernel_size = 3, stride = 2, padding = 1, output_padding = 1)\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(32, 3, kernel_size = 3, stride = 1, padding = 1)\n",
    "    \n",
    "    def forward(self, image_features):\n",
    "        x = F.leaky_relu(self.fc1(image_features))\n",
    "        x = nn.Unflatten(1, (256, 8, 8))(x)\n",
    "        # 256, 8, 8\n",
    "\n",
    "        x = F.leaky_relu( self.deconv1(x) )\n",
    "        # 128, 16, 16\n",
    "        x = F.leaky_relu( self.deconv2(x) )\n",
    "        # 64, 32, 32\n",
    "        x = F.leaky_relu( self.deconv3(x) )\n",
    "        # 32, 64, 64\n",
    "        x = F.leaky_relu( self.deconv4(x) )\n",
    "        # 32, 128, 128\n",
    "        return nn.Sigmoid()( self.conv1(x) )\n",
    "        # 3, 128, 128\n",
    "\n",
    "class AugmixAutoencoder(nn.Module):\n",
    "    def __init__(self, image_feature_dim):\n",
    "        \"\"\"\n",
    "            image_feature_dim: \n",
    "        \"\"\"\n",
    "        super(AugmixAutoencoder, self).__init__()\n",
    "\n",
    "        self.encoder = ImageEncoder(image_feature_dim)\n",
    "        self.decoder = ImageDecoder(image_feature_dim)\n",
    "    \n",
    "    def forward(self, images):\n",
    "        x = self.encoder(images)\n",
    "        return self.decoder( x ), x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_by_augmix_autoencoder(augmix_autoeuncoder, image_paths, chain, max_depth, device = \"cuda\"):\n",
    "    tmp_images = [Image.open(IP).resize((IMAGE_WIDTH, IMAGE_WIDTH)) for IP in image_paths]\n",
    "    tmp_images = [image_preprocess(I) for I in tmp_images] + [ image_preprocess( augmix_operation(I, chain = chain, max_depth = max_depth) / 255.0 ) for I in tmp_images ]\n",
    "    images = torch.stack(tmp_images).to(torch.float32).to(device)\n",
    "\n",
    "    predict_images, _ = augmix_autoeuncoder(images)\n",
    "    predict_images = predict_images.permute(0, 2, 3, 1).cpu().detach().numpy()\n",
    "\n",
    "    fig = plt.figure(figsize = (15, 5))\n",
    "    for i in range(len(tmp_images)):\n",
    "        ax = fig.add_subplot(2, len(tmp_images), i + 1)\n",
    "        ax.imshow(tmp_images[i].permute(1, 2, 0))\n",
    "        ax.axis(\"off\")\n",
    "        ax.set_title(\"input\")\n",
    "\n",
    "        ax = fig.add_subplot(2, len(tmp_images), len(tmp_images) + i + 1)\n",
    "        ax.imshow(predict_images[i])\n",
    "        ax.axis(\"off\")\n",
    "        ax.set_title(\"predict\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(f\"{AMAE_RESULT_DIR}history.png\") or RETRAIN_AUGMIX_AUTOENCODER:\n",
    "    augmix_autoencoder = AugmixAutoencoder(image_feature_dim = AMAE_FEATURE_DIM)\n",
    "\n",
    "    # 学習履歴がある場合，途中から再開する\n",
    "    if os.path.exists(f\"{AMAE_RESULT_DIR}history.json\"):\n",
    "        with open(f\"{AMAE_RESULT_DIR}history.json\", \"r\") as f:\n",
    "            a = json.load(f)\n",
    "            train_loss_history = a[\"train_loss\"]\n",
    "            test_loss_history = a[\"test_loss\"]\n",
    "        augmix_autoencoder.load_state_dict(torch.load(f\"{AMAE_RESULT_DIR}model_{len(train_loss_history):03}.pth\"))\n",
    "        START_EPOCH = len(train_loss_history)\n",
    "    else:\n",
    "        train_loss_history = []\n",
    "        test_loss_history = []\n",
    "        START_EPOCH = 0\n",
    "\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    augmix_autoencoder.to(device)\n",
    "    optimizer = optim.AdamW(augmix_autoencoder.parameters(), lr = AMAE_LEARNING_RATE)\n",
    "\n",
    "    train_image_dataloader = make_augmix_image_dataloader(train_image_paths, batch_size = AMAE_BATCH_SIZE, chain = AMAE_CHAIN, max_depth = AMAE_DEPTH, num_workers = NUM_WORKERS)\n",
    "    test_image_dataloader = make_augmix_image_dataloader(test_image_paths, batch_size = AMAE_BATCH_SIZE, chain = AMAE_CHAIN, max_depth = AMAE_DEPTH, num_workers = NUM_WORKERS)\n",
    "\n",
    "    # 1イテレーション学習する関数\n",
    "    def train_step_for_augmix_autoencoder(augmix_autoencoder, optimizer, images, augmix_images1, augmix_images2):\n",
    "        optimizer.zero_grad()\n",
    "        original_outputs, original_features = augmix_autoencoder(images)\n",
    "        am1_outputs, am1_features = augmix_autoencoder(augmix_images1)\n",
    "        am2_outputs, am2_features = augmix_autoencoder(augmix_images2)\n",
    "\n",
    "        loss1 = nn.MSELoss()(original_outputs, images)\n",
    "        loss2 = nn.MSELoss()(am1_outputs, images)\n",
    "        loss3 = nn.MSELoss()(am2_outputs, images)\n",
    "        loss4 = nn.MSELoss()(original_features, am1_features)\n",
    "        loss5 = nn.MSELoss()(original_features, am2_features)\n",
    "\n",
    "        loss = loss1 + loss2 + loss3 + AMAE_ALPHA * (loss4 + loss5)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        return loss.item()\n",
    "\n",
    "    # 1イテレーション検証する関数\n",
    "    def test_step_for_augmix_autoencoder(augmix_autoencoder, images, augmix_images1, augmix_images2):\n",
    "        with torch.no_grad():\n",
    "            original_outputs, original_features = augmix_autoencoder(images)\n",
    "            am1_outputs, am1_features = augmix_autoencoder(augmix_images1)\n",
    "            am2_outputs, am2_features = augmix_autoencoder(augmix_images2)\n",
    "\n",
    "            loss1 = nn.MSELoss()(original_outputs, images)\n",
    "            loss2 = nn.MSELoss()(am1_outputs, images)\n",
    "            loss3 = nn.MSELoss()(am2_outputs, images)\n",
    "            loss4 = nn.MSELoss()(original_features, am1_features)\n",
    "            loss5 = nn.MSELoss()(original_features, am2_features)\n",
    "\n",
    "            loss = loss1 + loss2 + loss3 + AMAE_ALPHA * (loss4 + loss5)\n",
    "        return loss.item()\n",
    "\n",
    "    for epoch in range(START_EPOCH, AMAE_EPOCH):\n",
    "\n",
    "        # train\n",
    "        train_loss_obj = 0.0\n",
    "        augmix_autoencoder.train()\n",
    "        pb = tqdm(train_image_dataloader, desc = f\"Epoch {epoch+1}/{AMAE_EPOCH}\")\n",
    "        \n",
    "        for images, augmix_images1, augmix_images2, _ in pb:\n",
    "            images = images.float().to(\"cuda\")\n",
    "            augmix_images1 = augmix_images1.float().to(\"cuda\")\n",
    "            augmix_images2 = augmix_images2.float().to(\"cuda\")\n",
    "\n",
    "            loss = train_step_for_augmix_autoencoder(augmix_autoencoder, optimizer, images, augmix_images1, augmix_images2)\n",
    "            train_loss_obj += loss\n",
    "            pb.set_postfix({\"train_loss\": train_loss_obj / (pb.n + 1),})\n",
    "\n",
    "        train_loss = train_loss_obj / len(train_image_dataloader)\n",
    "\n",
    "        # test\n",
    "        test_loss_obj = 0.0\n",
    "        augmix_autoencoder.eval()\n",
    "        pb = tqdm(test_image_dataloader, desc = f\"Epoch {epoch+1}/{AMAE_EPOCH}\")\n",
    "        \n",
    "        for images, augmix_images1, augmix_images2, _ in pb:\n",
    "            images = images.float().to(\"cuda\")\n",
    "            augmix_images1 = augmix_images1.float().to(\"cuda\")\n",
    "            augmix_images2 = augmix_images2.float().to(\"cuda\")\n",
    "\n",
    "            loss = test_step_for_augmix_autoencoder(augmix_autoencoder, images, augmix_images1, augmix_images2)\n",
    "            test_loss_obj += loss\n",
    "            pb.set_postfix({\"test_loss\": test_loss_obj / (pb.n + 1),})\n",
    "\n",
    "        test_loss = test_loss_obj / len(test_image_dataloader)\n",
    "\n",
    "        print(f\"Epoch: {epoch+1}/{AMAE_EPOCH}, \"\n",
    "            f\"Train Loss: {train_loss:.4f}, Test Loss: {test_loss:.4f}\")\n",
    "        print(\"-\" * 25)\n",
    "\n",
    "        train_loss_history.append(train_loss)\n",
    "        test_loss_history.append(test_loss)\n",
    "\n",
    "        torch.save(augmix_autoencoder.state_dict(), f\"{AMAE_RESULT_DIR}model_{len(train_loss_history):03}.pth\")\n",
    "        if os.path.exists(f\"{AMAE_RESULT_DIR}model_{len(train_loss_history) - 1:03}.pth\"):\n",
    "            os.remove(f\"{AMAE_RESULT_DIR}model_{len(train_loss_history) - 1:03}.pth\")\n",
    "\n",
    "        # 検証誤差を更新した場合、重みを保存\n",
    "        if min(test_loss_history) == test_loss:\n",
    "            torch.save(augmix_autoencoder.state_dict(), f\"{AMAE_RESULT_DIR}best_model.pth\")\n",
    "\n",
    "        # 学習結果を保存\n",
    "        with open(f\"{AMAE_RESULT_DIR}history.json\", \"w\") as f:\n",
    "            json.dump({\n",
    "                \"train_loss\": train_loss_history,\n",
    "                \"test_loss\": test_loss_history,\n",
    "            }, f)\n",
    "\n",
    "        predict_by_augmix_autoencoder(augmix_autoencoder, test_image_paths[:5], chain = AMAE_CHAIN, max_depth = AMAE_DEPTH)\n",
    "\n",
    "    # 学習結果を描画\n",
    "    fig = plt.figure(figsize = (5, 5))\n",
    "    ax = fig.add_subplot(1, 1, 1)\n",
    "    ax.plot(train_loss_history, label = \"train\")\n",
    "    ax.plot(test_loss_history, label = \"test\")\n",
    "    ax.set_xlabel(\"epoch\")\n",
    "    ax.set_ylabel(\"loss\")\n",
    "    ax.legend()\n",
    "    ax.grid()\n",
    "\n",
    "    fig.savefig(f\"{AMAE_RESULT_DIR}history.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "学習に用いる大喜利の数: 2030920\n",
      " 検証に用いる大喜利の数: 20515\n",
      " 使用する画像の数: 244286\n",
      " 単語の種類: 16705\n"
     ]
    }
   ],
   "source": [
    "# データセットの作成\n",
    "if not os.path.exists(f\"{RESULT_DIR}index_to_word.json\") or RESET_DATA:\n",
    "    # tokenizer\n",
    "    with open('Japanese_BPEEncoder_V2/ja-swe32kfix.txt') as f:\n",
    "        bpe = f.read().split('\\n')\n",
    "\n",
    "    with open('Japanese_BPEEncoder_V2/emoji.json') as f:\n",
    "        emoji = json.loads(f.read())\n",
    "\n",
    "    tokenizer = SWEEncoder_ja(bpe, emoji)\n",
    "\n",
    "    tmp = list()\n",
    "    word_count_dict = dict()\n",
    "\n",
    "    for JP in tqdm(os.listdir(DATA_DIR)):\n",
    "        \n",
    "        N = int(JP.split(\".\")[0])\n",
    "\n",
    "        with open(f\"{DATA_DIR}{JP}\", \"r\") as f:\n",
    "            a = json.load(f)\n",
    "        \n",
    "        image_information = a[\"image_information\"]\n",
    "        is_photographic_probability = image_information[\"is_photographic_probability\"]\n",
    "        ocr = image_information[\"ocr\"]\n",
    "\n",
    "        # 現実写真以外を除去\n",
    "        if not USE_UNREAL_IMAGE:\n",
    "            if is_photographic_probability < 0.8: continue\n",
    "            \n",
    "        # 文字のある画像を除去\n",
    "        if not USE_WORD_IMAGE:\n",
    "            if len(ocr) != 0: continue\n",
    "        \n",
    "        bokes = a[\"bokes\"]\n",
    "\n",
    "        for B in bokes:\n",
    "            # 星が既定の数以下の大喜利を除去\n",
    "            if B[\"star\"] < MIN_STAR:\n",
    "                continue\n",
    "\n",
    "            # 固有名詞を含む大喜利を除去\n",
    "            if not USE_UNIQUE_NOUN_BOKE:\n",
    "                if len(B[\"unique_nouns\"]) != 0: continue\n",
    "\n",
    "            tokenized_boke = tokenizer.encode(B[\"boke\"])\n",
    "            # 単語数が既定の数でない大喜利を除去\n",
    "            if not MIN_SENTENCE_LENGTH <= len(tokenized_boke) < MAX_SENTENCE_LENGTH:\n",
    "                continue\n",
    "\n",
    "            for W in tokenized_boke:\n",
    "                try:\n",
    "                    word_count_dict[W] += 1\n",
    "                except:\n",
    "                    word_count_dict[W] = 1\n",
    "            \n",
    "            tmp.append({\n",
    "                \"image_number\": N,\n",
    "                \"tokenized_boke\": tokenized_boke\n",
    "            })\n",
    "\n",
    "    # 単語の最小出現回数を満たさない大喜利を除去\n",
    "    boke_datas = list()\n",
    "    words = list()\n",
    "\n",
    "    for D in tqdm(tmp):\n",
    "        flag = False\n",
    "        for W in D[\"tokenized_boke\"]:\n",
    "            if word_count_dict[W] < MIN_APPER_WORD:\n",
    "                flag = True\n",
    "                break\n",
    "        if flag: \n",
    "            continue\n",
    "        \n",
    "        boke_datas.append({\n",
    "            \"image_number\": D[\"image_number\"],\n",
    "            \"tokenized_boke\": D[\"tokenized_boke\"]\n",
    "        })\n",
    "        words += D[\"tokenized_boke\"]\n",
    "    words = set(words)\n",
    "    image_numbers = list(set([D[\"image_number\"] for D in boke_datas]))\n",
    "    del tmp\n",
    "\n",
    "    # tokenize\n",
    "    index_to_index = dict()\n",
    "\n",
    "    c = 3\n",
    "    for D in tqdm(boke_datas):\n",
    "        tmp = list()\n",
    "        for W in D[\"tokenized_boke\"]:\n",
    "            try:\n",
    "                index_to_index[W]\n",
    "            except:\n",
    "                index_to_index[W] = c\n",
    "                c += 1\n",
    "            tmp.append(index_to_index[W])\n",
    "        D[\"tokenized_boke\"] = [1] + tmp + [2]\n",
    "\n",
    "    index_to_word = {\n",
    "        V: tokenizer.decode([K]) for K, V in index_to_index.items()\n",
    "    }\n",
    "    index_to_word[0] = \"<PAD>\"\n",
    "    index_to_word[1] = \"<START>\"\n",
    "    index_to_word[2] = \"<END>\"\n",
    "\n",
    "    #\n",
    "    train_boke_datas, test_boke_datas = train_test_split(boke_datas, test_size = 0.01)\n",
    "\n",
    "    with open(f\"{RESULT_DIR}train_boke_datas.json\", \"w\") as f:\n",
    "        json.dump(train_boke_datas, f)\n",
    "    with open(f\"{RESULT_DIR}test_boke_datas.json\", \"w\") as f:\n",
    "        json.dump(test_boke_datas, f)\n",
    "    with open(f\"{RESULT_DIR}index_to_word.json\", \"w\") as f:\n",
    "        json.dump(index_to_word, f)\n",
    "\n",
    "else:\n",
    "    with open(f\"{RESULT_DIR}train_boke_datas.json\", \"r\") as f:\n",
    "        train_boke_datas = json.load(f)\n",
    "    with open(f\"{RESULT_DIR}test_boke_datas.json\", \"r\") as f:\n",
    "        test_boke_datas = json.load(f)\n",
    "    with open(f\"{RESULT_DIR}index_to_word.json\", \"r\") as f:\n",
    "        index_to_word = json.load(f)\n",
    "\n",
    "    image_numbers = [D[\"image_number\"] for D in train_boke_datas] + [D[\"image_number\"] for D in test_boke_datas]\n",
    "    image_numbers = list(set(image_numbers))\n",
    "\n",
    "print(f\"学習に用いる大喜利の数: {len(train_boke_datas)}\\n\", \n",
    "      f\"検証に用いる大喜利の数: {len(test_boke_datas)}\\n\",\n",
    "      f\"使用する画像の数: {len(image_numbers)}\\n\",\n",
    "      f\"単語の種類: {len(index_to_word)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 画像の前処理\n",
    "image_preprocess = transforms.Compose([\n",
    "    transforms.Resize((IMAGE_HEIGHT, IMAGE_WIDTH)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# 画像のデータローダを作る関数\n",
    "def make_image_dataloader(image_paths, batch_size, num_workers = 4):\n",
    "\n",
    "    class LoadImageDataset(Dataset):\n",
    "        def __init__(self, image_paths):\n",
    "            \"\"\"\n",
    "                image_paths: 画像のパスからなるリスト\n",
    "            \"\"\"\n",
    "            self.image_paths = image_paths\n",
    "\n",
    "        def __len__(self):\n",
    "            return len(self.image_paths)\n",
    "\n",
    "        def __getitem__(self, idx):\n",
    "            image = Image.open(image_paths[idx]).convert(\"RGB\")\n",
    "\n",
    "            return image, self.image_paths[idx]\n",
    "    \n",
    "    def collate_fn_tf(batch):\n",
    "        images = torch.stack([image_preprocess(B[0]) for B in batch])\n",
    "        image_numbers = [B[1] for B in batch]\n",
    "\n",
    "        return images, image_numbers\n",
    "\n",
    "    print(f\"num data: {len(image_paths)}\")\n",
    "\n",
    "    dataset = LoadImageDataset(image_paths)\n",
    "    dataloader = DataLoader(\n",
    "        dataset, \n",
    "        batch_size = batch_size, \n",
    "        num_workers = num_workers, \n",
    "        collate_fn = collate_fn_tf\n",
    "    )\n",
    "\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 244286/244286 [00:09<00:00, 25223.03it/s]\n"
     ]
    }
   ],
   "source": [
    "# 画像を特徴量に変換する\n",
    "if RETRAIN_AUGMIX_AUTOENCODER:\n",
    "    shutil.rmtree(IMAGE_FEATURE_DIR) \n",
    "    os.mkdir(IMAGE_FEATURE_DIR)\n",
    "\n",
    "tmp = list()\n",
    "for IN in tqdm(image_numbers):\n",
    "    if os.path.exists(f\"{IMAGE_FEATURE_DIR}{IN}.npy\"):\n",
    "        continue\n",
    "    tmp.append(f\"{IMAGE_DIR}{IN}.jpg\")\n",
    "\n",
    "if len(tmp) != 0:\n",
    "    image_dataloader = make_image_dataloader(tmp, batch_size = 128, num_workers = NUM_WORKERS)\n",
    "\n",
    "    # encoder of Autoencoder\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    augmix_autoencoder = AugmixAutoencoder(image_feature_dim = AMAE_FEATURE_DIM)\n",
    "    augmix_autoencoder.load_state_dict(torch.load(f\"{AMAE_RESULT_DIR}best_model.pth\"))\n",
    "    model = augmix_autoencoder.encoder\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    for Is, IPs in tqdm(image_dataloader):\n",
    "        Is = Is.to(device)\n",
    "        features = model(Is).detach().cpu().numpy()\n",
    "\n",
    "        for f, IP in zip(features, IPs):\n",
    "            N = IP.split(\"/\")[-1].split(\".\")[0]\n",
    "            np.save(f\"{IMAGE_FEATURE_DIR}{N}\", f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 大喜利生成AIの学習用データローダを作る関数\n",
    "def make_dataloader(boke_datas, max_sentence_length, num_workers = 4):\n",
    "    \"\"\"\n",
    "        boke_datas: {\"image_number\":画像のお題番号 ,\"tokenized_boke\":トークナイズされた大喜利}からなるリスト\n",
    "        max_sentence_length: 学習データの最大単語数(<START>, <END>トークンを含まない)\n",
    "        num_workers: データローダが使用するCPUのスレッド数\n",
    "    \"\"\"\n",
    "    class SentenceGeneratorDataset(Dataset):\n",
    "        def __init__(self, image_file_numbers, sentences, teacher_signals):\n",
    "            \"\"\"\n",
    "                image_file_numbers: 画像の番号からなるリスト\n",
    "                sentences: 入力文章からなるリスト\n",
    "                teacher_signals: 教師信号からなるリスト\n",
    "            \"\"\"\n",
    "            if len(image_file_numbers) != len(sentences) and len(teacher_signals) != len(sentences):\n",
    "                raise ValueError(\"データリストの長さが一致しません\")\n",
    "\n",
    "            self.image_file_numbers = image_file_numbers\n",
    "            self.sentences = sentences\n",
    "            self.teacher_signals = teacher_signals\n",
    "\n",
    "        def __len__(self):\n",
    "            return len(self.teacher_signals)\n",
    "\n",
    "        def __getitem__(self, idx):\n",
    "            image_feature = np.load(f\"{IMAGE_FEATURE_DIR}{self.image_file_numbers[idx]}.npy\")\n",
    "            sentence = self.sentences[idx]\n",
    "            teacher_signal = self.teacher_signals[idx]\n",
    "\n",
    "            return image_feature, sentence, teacher_signal\n",
    "\n",
    "    def collate_fn_tf(batch):\n",
    "        image_features = torch.tensor(np.array([B[0] for B in batch]))\n",
    "        sentences = torch.tensor(np.array([B[1] for B in batch]))\n",
    "        teacher_signals = torch.tensor(np.array([B[2] for B in batch]))\n",
    "\n",
    "        return image_features, sentences, teacher_signals\n",
    "\n",
    "    image_file_numbers = list()\n",
    "    sentences = list()\n",
    "    teacher_signals = list()\n",
    "\n",
    "    for D in tqdm(boke_datas):\n",
    "        image_file_numbers.append(D[\"image_number\"])\n",
    "        tmp = D[\"tokenized_boke\"] + [0] * (2 + max_sentence_length - len(D[\"tokenized_boke\"]))\n",
    "        sentences.append(tmp[:-1])\n",
    "        teacher_signals.append(tmp[1:])\n",
    "\n",
    "    dataset = SentenceGeneratorDataset(image_file_numbers, sentences, teacher_signals)\n",
    "    dataloader = DataLoader(\n",
    "        dataset, \n",
    "        batch_size = BATCH_SIZE, \n",
    "        num_workers = num_workers, \n",
    "        collate_fn = collate_fn_tf\n",
    "    )\n",
    "\n",
    "    print(f\"num data: {len(teacher_signals)}\")\n",
    "\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 大喜利生成モデルのクラス\n",
    "class BokeGeneratorModel(nn.Module):\n",
    "    def __init__(self, num_word, image_feature_dim, sentence_length, feature_dim = 1024):\n",
    "        \"\"\"\n",
    "            num_word: 学習に用いる単語の総数\n",
    "            image_feature_dim: 画像の特徴量の次元数\n",
    "            sentence_length: 入力する文章の単語数\n",
    "            feature_dim: 特徴量次元数\n",
    "        \"\"\"\n",
    "        super(BokeGeneratorModel, self).__init__()\n",
    "        self.num_word = num_word\n",
    "        self.image_feature_dim = image_feature_dim\n",
    "        self.sentence_length = sentence_length\n",
    "        self.feature_dim = feature_dim\n",
    "        \n",
    "        self.fc1 = nn.Linear(image_feature_dim, feature_dim)\n",
    "        self.embedding = nn.Embedding(num_word, feature_dim, padding_idx = 0)\n",
    "        self.lstm = nn.LSTM(input_size = feature_dim, hidden_size = feature_dim, \n",
    "                            batch_first = True)\n",
    "        self.fc2 = nn.Linear(feature_dim + feature_dim, 2 * feature_dim)\n",
    "        self.fc3 = nn.Linear(2 * feature_dim, 2 * feature_dim)\n",
    "        self.fc4 = nn.Linear(2 * feature_dim, num_word)\n",
    "    \n",
    "    # LSTMの初期値は0で，画像の特徴量と文章の特徴量を全結合層の前で結合する\n",
    "    def forward(self, image_features, sentences):\n",
    "        \"\"\"\n",
    "            image_features: 画像の特徴量\n",
    "            sentences: 入力する文章\n",
    "        \"\"\"\n",
    "        x1 = F.leaky_relu(self.fc1(image_features))\n",
    "        x1 = x1.unsqueeze(1).repeat(1, self.sentence_length, 1)\n",
    "\n",
    "        x2 = self.embedding(sentences)\n",
    "        x2, _ = self.lstm(x2)\n",
    "\n",
    "        x = torch.cat((x1, x2), dim = -1)\n",
    "        x = F.leaky_relu(self.fc2(x))\n",
    "        x = F.leaky_relu(self.fc3(x))\n",
    "\n",
    "        return self.fc4(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2030920/2030920 [00:05<00:00, 338890.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num data: 2030920\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20515/20515 [00:00<00:00, 762583.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num data: 20515\n"
     ]
    }
   ],
   "source": [
    "# 文章生成の精度を計算する関数\n",
    "def calculate_accuracy(teacher_signals, outputs):\n",
    "    \"\"\"\n",
    "        teacher_signals: 教師信号\n",
    "        outputs: モデルの出力\n",
    "    \"\"\"\n",
    "    _, predicted_words = outputs.max(dim = -1)\n",
    "    # パディングに対して精度を計算しない\n",
    "    mask = (teacher_signals != 0)\n",
    "    correct = ((predicted_words == teacher_signals) & mask).sum().item()\n",
    "    total = mask.sum().item()\n",
    "    accuracy = correct / total if total > 0 else 0.0\n",
    "    return accuracy\n",
    "\n",
    "# 1イテレーション学習する関数\n",
    "def train_step(model, optimizer, batch_data, batch_labels):\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(*batch_data)\n",
    "    # パディングに対して損失を計算しない\n",
    "    loss = F.cross_entropy(outputs.view(-1, outputs.size(-1)), batch_labels.view(-1),\n",
    "                           ignore_index = 0)\n",
    "    accuracy = calculate_accuracy(batch_labels, F.softmax(outputs, dim = -1))\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss.item(), accuracy\n",
    "\n",
    "# 1イテレーション検証する関数\n",
    "def evaluate(model, batch_data, batch_labels):\n",
    "    with torch.no_grad():\n",
    "        outputs = model(*batch_data)\n",
    "        loss = F.cross_entropy(outputs.view(-1, outputs.size(-1)), batch_labels.view(-1),\n",
    "                               ignore_index = 0)\n",
    "        accuracy = calculate_accuracy(batch_labels, F.softmax(outputs, dim = -1))\n",
    "    return loss.item(), accuracy\n",
    "\n",
    "train_dataloader = make_dataloader(train_boke_datas, max_sentence_length = MAX_SENTENCE_LENGTH, num_workers = NUM_WORKERS)\n",
    "test_dataloader = make_dataloader(test_boke_datas, max_sentence_length = MAX_SENTENCE_LENGTH, num_workers = NUM_WORKERS)\n",
    "\n",
    "model = BokeGeneratorModel(num_word = len(index_to_word), \n",
    "                           image_feature_dim = AMAE_FEATURE_DIM, \n",
    "                           sentence_length = MAX_SENTENCE_LENGTH + 1, \n",
    "                           feature_dim = FEATURE_DIM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/25:   1%|          | 32/3967 [01:10<19:37,  3.34it/s, train_loss=8.46, train_accuracy=0.0678]  "
     ]
    }
   ],
   "source": [
    "# 学習履歴がある場合，途中から再開する\n",
    "if os.path.exists(f\"{RESULT_DIR}history.json\"):\n",
    "    with open(f\"{RESULT_DIR}history.json\", \"r\") as f:\n",
    "        a = json.load(f)\n",
    "        train_loss_history = a[\"train_loss\"]\n",
    "        train_accuracy_history = a[\"train_accuracy\"]\n",
    "        test_loss_history = a[\"test_loss\"]\n",
    "        test_accuracy_history = a[\"test_accuracy\"]\n",
    "    model.load_state_dict(torch.load(f\"{RESULT_DIR}model_{len(train_loss_history):03}.pth\"))\n",
    "    SATRT_EPOCH = len(train_loss_history)\n",
    "else:\n",
    "    train_loss_history = []\n",
    "    train_accuracy_history = []\n",
    "    test_loss_history = []\n",
    "    test_accuracy_history = []\n",
    "    SATRT_EPOCH = 0\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model.to(device)\n",
    "optimizer = optim.AdamW(model.parameters(), lr = LEARNING_RATE)\n",
    "\n",
    "for epoch in range(SATRT_EPOCH, EPOCH):\n",
    "\n",
    "    # train\n",
    "    train_loss_obj = 0.0\n",
    "    train_accuracy_obj = 0.0\n",
    "    model.train()\n",
    "    pb = tqdm(train_dataloader, desc = f\"Epoch {epoch+1}/{EPOCH}\")\n",
    "    \n",
    "    for image_features, sentences, teacher_signals in pb:\n",
    "        image_features = image_features.to(device)\n",
    "        sentences = sentences.to(device)\n",
    "        teacher_signals = teacher_signals.to(device)\n",
    "        \n",
    "        loss, accuracy = train_step(model, optimizer, (image_features, sentences), teacher_signals)\n",
    "        train_loss_obj += loss\n",
    "        train_accuracy_obj += accuracy\n",
    "        pb.set_postfix({\"train_loss\": train_loss_obj / (pb.n + 1), \"train_accuracy\": train_accuracy_obj / (pb.n + 1)})\n",
    "\n",
    "    train_loss = train_loss_obj / len(train_dataloader)\n",
    "    train_accuracy = train_accuracy_obj / len(train_dataloader)\n",
    "\n",
    "    # test\n",
    "    test_loss_obj = 0.0\n",
    "    test_accuracy_obj = 0.0\n",
    "    model.eval()\n",
    "    pb = tqdm(test_dataloader, desc = \"Evaluating\")\n",
    "\n",
    "    for image_features, sentences, teacher_signals in pb:\n",
    "        image_features = image_features.to(device)\n",
    "        sentences = sentences.to(device)\n",
    "        teacher_signals = teacher_signals.to(device)\n",
    "\n",
    "        loss, accuracy = evaluate(model, (image_features, sentences), teacher_signals)\n",
    "        test_loss_obj += loss\n",
    "        test_accuracy_obj += accuracy\n",
    "        pb.set_postfix({\"test_loss\": test_loss_obj / (pb.n + 1), \"test_accuracy\": test_accuracy_obj / (pb.n + 1)})\n",
    "\n",
    "    test_loss = test_loss_obj / len(test_dataloader)\n",
    "    test_accuracy = test_accuracy_obj / len(test_dataloader)\n",
    "\n",
    "    print(f\"Epoch: {epoch+1}/{EPOCH}, \"\n",
    "          f\"Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, \"\n",
    "          f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}\")\n",
    "    print(\"-\" * 25)\n",
    "\n",
    "    train_loss_history.append(train_loss)\n",
    "    train_accuracy_history.append(train_accuracy)\n",
    "    test_loss_history.append(test_loss)\n",
    "    test_accuracy_history.append(test_accuracy)\n",
    "\n",
    "    torch.save(model.state_dict(), f\"{RESULT_DIR}model_{len(train_loss_history):03}.pth\")\n",
    "    if os.path.exists(f\"{RESULT_DIR}model_{len(train_loss_history) - 1:03}.pth\"):\n",
    "        os.remove(f\"{RESULT_DIR}model_{len(train_loss_history) - 1:03}.pth\")\n",
    "\n",
    "    # 学習精度を更新した場合、重みを保存\n",
    "    if max(train_accuracy_history) == train_accuracy:\n",
    "        torch.save(model.state_dict(), f\"{RESULT_DIR}best_model.pth\")\n",
    "    \n",
    "    # 学習結果を保存\n",
    "    with open(f\"{RESULT_DIR}history.json\", \"w\") as f:\n",
    "        json.dump({\n",
    "            \"train_loss\": train_loss_history,\n",
    "            \"train_accuracy\": train_accuracy_history,\n",
    "            \"test_loss\": test_loss_history,\n",
    "            \"test_accuracy\": test_accuracy_history\n",
    "        }, f)\n",
    "\n",
    "# 学習結果を描画\n",
    "fig = plt.figure(figsize = (10, 5))\n",
    "ax = fig.add_subplot(1, 2, 1)\n",
    "ax.plot(train_loss_history, label = \"train\")\n",
    "ax.plot(test_loss_history, label = \"test\")\n",
    "ax.set_xlabel(\"epoch\")\n",
    "ax.set_ylabel(\"loss\")\n",
    "ax.legend()\n",
    "ax.grid()\n",
    "\n",
    "ax = fig.add_subplot(1, 2, 2)\n",
    "ax.plot(train_accuracy_history, label = \"train\")\n",
    "ax.plot(test_accuracy_history, label = \"test\")\n",
    "ax.set_xlabel(\"epoch\")\n",
    "ax.set_ylabel(\"accuracy\")\n",
    "ax.legend()\n",
    "ax.grid()\n",
    "\n",
    "fig.savefig(f\"{RESULT_DIR}history.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 大喜利生成AI\n",
    "class GUMI_AMAE:\n",
    "    def __init__(self, weight_path, amae_weight_path, index_to_word, sentence_length, feature_dim = 1024, amae_feature_dim = 16384):\n",
    "        \"\"\"\n",
    "            weight_path: 大喜利適合判定モデルの学習済みの重みのパス\n",
    "            amae_weight_path: \n",
    "            index_to_word: 単語のID: 単語の辞書(0:<PAD>, 1:<START>, 2:<END>)\n",
    "            sentence_length: 入力する文章の単語数\n",
    "            feature_dim: 特徴量次元数\n",
    "            amae_feature_dim: \n",
    "        \"\"\"\n",
    "        self.index_to_word = index_to_word\n",
    "        self.sentence_length = sentence_length\n",
    "\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        self.boke_generate_model = BokeGeneratorModel(\n",
    "                                        num_word = len(index_to_word), \n",
    "                                        image_feature_dim = ae_feature_dim, \n",
    "                                        sentence_length = sentence_length, \n",
    "                                        feature_dim = feature_dim)\n",
    "        self.boke_generate_model.load_state_dict(torch.load(weight_path))\n",
    "        self.boke_generate_model.to(self.device)\n",
    "        self.boke_generate_model.eval()\n",
    "\n",
    "        self.augmix_autoencoder = AugmixAutoencoder(image_feature_dim = amae_feature_dim)\n",
    "        self.augmix_autoencoder.load_state_dict(torch.load(amae_weight_path))\n",
    "        self.encoder = self.augmix_autoencoder.encoder\n",
    "        self.encoder = self.encoder.to(device)\n",
    "        self.encoder.eval()\n",
    "\n",
    "        # 画像の前処理\n",
    "        self.image_preprocesser = transforms.Compose([\n",
    "            transforms.Resize((IMAGE_HEIGHT, IMAGE_WIDTH)),\n",
    "            transforms.ToTensor(),\n",
    "        ])\n",
    "    \n",
    "    def __call__(self, image_path, argmax = False, top_k = 5):\n",
    "        \"\"\"\n",
    "            image_path: 大喜利を生成したい画像のパス\n",
    "            argmax: Trueなら最大確率の単語を選ぶ, FalseならTop-Kサンプリングを行う\n",
    "            top_k: Top-Kサンプリング時に考慮する単語の数\n",
    "        \"\"\"\n",
    "        image = Image.open(image_path)\n",
    "        preprocessed_image = self.image_preprocesser(image).to(self.device)\n",
    "        image_feature = self.encoder( preprocessed_image.unsqueeze(0) ) # (1, 2048)\n",
    "        \n",
    "        generated_text = [1] # <START>トークン\n",
    "        for i in range(1, self.sentence_length):\n",
    "            tmp = generated_text + [0] * (self.sentence_length - i) # Padding\n",
    "            tmp = torch.Tensor(np.array(tmp)).unsqueeze(0).to(self.device).to(dtype=torch.int32) # (1, sentence_length)\n",
    "            pred = self.boke_generate_model(image_feature, tmp) # (1, sentence_length, num_word)\n",
    "            target_pred = pred[0][i - 1]\n",
    "\n",
    "            if argmax:\n",
    "                # 最大確率の単語を選ぶ\n",
    "                chosen_id = torch.argmax(target_pred).item()\n",
    "            else:\n",
    "                # Top-Kサンプリング\n",
    "                top_k_probs, top_k_indices = torch.topk(target_pred, top_k)\n",
    "                top_k_probs = torch.nn.functional.softmax(top_k_probs, dim = -1)\n",
    "                chosen_id = np.random.choice(top_k_indices.detach().cpu().numpy(), \n",
    "                                             p = top_k_probs.detach().cpu().numpy())\n",
    "            \n",
    "            generated_text.append(chosen_id)\n",
    "            if chosen_id == 2:\n",
    "                break\n",
    "        \n",
    "        generated_sentence = \"\"\n",
    "        for I in generated_text[1:-1]:\n",
    "            generated_sentence += self.index_to_word[I]\n",
    "        return generated_sentence"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
