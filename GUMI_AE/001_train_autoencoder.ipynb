{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 001\n",
    "\n",
    "現実写真のみ、文字なし"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import shutil\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import subprocess\n",
    "if not os.path.exists(\"Japanese_BPEEncoder_V2\"):\n",
    "    subprocess.run([\"git\", \"clone\", \"https://github.com/tanreinama/Japanese-BPEEncoder_V2.git\", \"Japanese_BPEEncoder_V2\"])\n",
    "from Japanese_BPEEncoder_V2.encode_swe import SWEEncoder_ja"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXPERIENCE_NUMBER = \"001\"\n",
    "\n",
    "# PCによって変更する\n",
    "NUM_WORKERS = 20\n",
    "# データセットが既に存在する場合に，再度作り直すか\n",
    "RESET_DATA = False\n",
    "# Autoencoderを学習しなおすか\n",
    "RETRAIN_AUTOENCODER = False\n",
    "\n",
    "# 現実写真以外を使用するか\n",
    "USE_UNREAL_IMAGE = False\n",
    "# 文字を含む画像を使用するか\n",
    "USE_WORD_IMAGE = False\n",
    "# 固有名詞を含む大喜利を使用するか\n",
    "USE_UNIQUE_NOUN_BOKE = False\n",
    "\n",
    "# 大喜利の最小の星の数\n",
    "MIN_STAR = 0\n",
    "# 単語の最小出現回数\n",
    "MIN_APPER_WORD = 32\n",
    "# 大喜利の最小単語数\n",
    "MIN_SENTENCE_LENGTH = 4\n",
    "# 大喜利の最大単語数\n",
    "MAX_SENTENCE_LENGTH = 31\n",
    "\n",
    "RESULT_DIR = f\"../../results/GUMI_AE/{EXPERIENCE_NUMBER}/\"\n",
    "if not os.path.exists(\"../../results/GUMI_AE/\"):\n",
    "    os.mkdir(\"../../results/GUMI_AE/\")\n",
    "if not os.path.exists(RESULT_DIR):\n",
    "    os.mkdir(RESULT_DIR)\n",
    "\n",
    "# \n",
    "BATCH_SIZE_FOR_AUTOENCODER = 32\n",
    "EPOCH_FOR_AUTOENCODER = 25\n",
    "LEARNING_RATO_FOR_AUTOENCODER = 0.0001\n",
    "IMAGE_HEIGHT = 128\n",
    "IMAGE_WIDTH = 128\n",
    "IMAGE_FEATURE_DIM = 16384\n",
    "\n",
    "BATCH_SIZE_FOR_GENERATOR = 32\n",
    "EPOCH_FOR_GENERATOR = 25\n",
    "LEARNING_RATO_FOR_GENERATOR = 0.0001\n",
    "\n",
    "DATA_DIR = \"../../datas/boke_data_assemble/\"\n",
    "IMAGE_DIR = \"../../datas/boke_image/\"\n",
    "\n",
    "IMAGE_FEATURE_DIR = \"../../datas/encoded/autoencoder_image_feature/\"\n",
    "if not os.path.exists(IMAGE_FEATURE_DIR):\n",
    "    os.mkdir(IMAGE_FEATURE_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(228908, 25435)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if not os.path.exists(f\"{RESULT_DIR}test_image_paths.json\") or RESET_DATA:\n",
    "    image_paths = list()\n",
    "\n",
    "    for IP in tqdm(os.listdir(IMAGE_DIR)):\n",
    "        \n",
    "        N = int(IP.split(\".\")[0])\n",
    "        if not os.path.exists(f\"{DATA_DIR}{N}.json\"):\n",
    "            continue\n",
    "\n",
    "        with open(f\"{DATA_DIR}{N}.json\", \"r\") as f:\n",
    "            a = json.load(f)\n",
    "        \n",
    "        image_information = a[\"image_information\"]\n",
    "        is_photographic_probability = image_information[\"is_photographic_probability\"]\n",
    "        ocr = image_information[\"ocr\"]\n",
    "\n",
    "        # 現実写真以外を除去\n",
    "        if not USE_UNREAL_IMAGE:\n",
    "            if is_photographic_probability < 0.8: continue\n",
    "            \n",
    "        # 文字のある画像を除去\n",
    "        if not USE_WORD_IMAGE:\n",
    "            if len(ocr) != 0: continue\n",
    "        \n",
    "        image_paths.append(f\"{IMAGE_DIR}{IP}\")\n",
    "\n",
    "    train_image_paths, test_image_paths = train_test_split(image_paths, test_size = 0.1)\n",
    "\n",
    "    with open(f\"{RESULT_DIR}train_image_paths.json\", \"w\") as f:\n",
    "        json.dump(train_image_paths, f)\n",
    "    with open(f\"{RESULT_DIR}test_image_paths.json\", \"w\") as f:\n",
    "        json.dump(test_image_paths, f)\n",
    "    \n",
    "else:\n",
    "    with open(f\"{RESULT_DIR}train_image_paths.json\", \"r\") as f:\n",
    "        train_image_paths = json.load(f)\n",
    "    with open(f\"{RESULT_DIR}test_image_paths.json\", \"r\") as f:\n",
    "        test_image_paths = json.load(f)\n",
    "\n",
    "len(train_image_paths), len(test_image_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 画像の前処理\n",
    "image_preprocess = transforms.Compose([\n",
    "    transforms.Resize((IMAGE_HEIGHT, IMAGE_WIDTH)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# 画像のデータローダを作る関数\n",
    "def make_image_dataloader(image_paths, batch_size, num_workers = 4):\n",
    "\n",
    "    class LoadImageDataset(Dataset):\n",
    "        def __init__(self, image_paths):\n",
    "            \"\"\"\n",
    "                image_paths: 画像のパスからなるリスト\n",
    "            \"\"\"\n",
    "            self.image_paths = image_paths\n",
    "\n",
    "        def __len__(self):\n",
    "            return len(self.image_paths)\n",
    "\n",
    "        def __getitem__(self, idx):\n",
    "            image = Image.open(image_paths[idx]).convert(\"RGB\")\n",
    "\n",
    "            return image, self.image_paths[idx]\n",
    "    \n",
    "    def collate_fn_tf(batch):\n",
    "        images = torch.stack([image_preprocess(B[0]) for B in batch])\n",
    "        image_numbers = [B[1] for B in batch]\n",
    "\n",
    "        return images, image_numbers\n",
    "\n",
    "    print(f\"num data: {len(image_paths)}\")\n",
    "\n",
    "    dataset = LoadImageDataset(image_paths)\n",
    "    dataloader = DataLoader(\n",
    "        dataset, \n",
    "        batch_size = batch_size, \n",
    "        num_workers = num_workers, \n",
    "        collate_fn = collate_fn_tf\n",
    "    )\n",
    "\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageEncoder(nn.Module):\n",
    "    def __init__(self, image_feature_dim):\n",
    "        \"\"\"\n",
    "            image_feature_dim: \n",
    "        \"\"\"\n",
    "        super(ImageEncoder, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size = 3, stride = 2, padding = 1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size = 3, stride = 2, padding = 1)\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size = 3, stride = 2, padding = 1)\n",
    "        self.conv4 = nn.Conv2d(128, 256, kernel_size = 3, stride = 2, padding = 1)\n",
    "\n",
    "        self.fc1 = nn.Linear(16384, 16384)\n",
    "        self.fc2 = nn.Linear(16384, image_feature_dim)\n",
    "    \n",
    "    def forward(self, images):\n",
    "        x = F.leaky_relu( self.conv1(images) )\n",
    "        # 32, 64, 64\n",
    "        x = F.leaky_relu( self.conv2(x) )\n",
    "        # 64, 32, 32\n",
    "        x = F.leaky_relu( self.conv3(x) )\n",
    "        # 128, 16, 16\n",
    "        x = F.leaky_relu( self.conv4(x) )\n",
    "        # 256, 8, 8\n",
    "\n",
    "        x = nn.Flatten()(x)\n",
    "        x = F.leaky_relu(self.fc1(x))\n",
    "        return F.leaky_relu(self.fc2(x))\n",
    "\n",
    "class ImageDecoder(nn.Module):\n",
    "    def __init__(self, image_feature_dim):\n",
    "        \"\"\"\n",
    "            image_feature_dim: \n",
    "        \"\"\"\n",
    "        super(ImageDecoder, self).__init__()\n",
    "\n",
    "        self.fc1 = nn.Linear(image_feature_dim, 16384)\n",
    "\n",
    "        self.deconv1 = nn.ConvTranspose2d(256, 128, kernel_size = 3, stride = 2, padding = 1, output_padding = 1)\n",
    "        self.deconv2 = nn.ConvTranspose2d(128, 64, kernel_size = 3, stride = 2, padding = 1, output_padding = 1)\n",
    "        self.deconv3 = nn.ConvTranspose2d(64, 32, kernel_size = 3, stride = 2, padding = 1, output_padding = 1)\n",
    "        self.deconv4 = nn.ConvTranspose2d(32, 32, kernel_size = 3, stride = 2, padding = 1, output_padding = 1)\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(32, 3, kernel_size = 3, stride = 1, padding = 1)\n",
    "    \n",
    "    def forward(self, image_features):\n",
    "        x = F.leaky_relu(self.fc1(image_features))\n",
    "        x = nn.Unflatten(1, (256, 8, 8))(x)\n",
    "        # 256, 8, 8\n",
    "\n",
    "        x = F.leaky_relu( self.deconv1(x) )\n",
    "        # 128, 16, 16\n",
    "        x = F.leaky_relu( self.deconv2(x) )\n",
    "        # 64, 32, 32\n",
    "        x = F.leaky_relu( self.deconv3(x) )\n",
    "        # 32, 64, 64\n",
    "        x = F.leaky_relu( self.deconv4(x) )\n",
    "        # 32, 128, 128\n",
    "        return nn.Sigmoid()( self.conv1(x) )\n",
    "        # 3, 128, 128\n",
    "\n",
    "ImageDecoder(IMAGE_FEATURE_DIM)(torch.zeros([1, IMAGE_FEATURE_DIM]))\n",
    "\n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, image_feature_dim):\n",
    "        \"\"\"\n",
    "            image_feature_dim: \n",
    "        \"\"\"\n",
    "        super(Autoencoder, self).__init__()\n",
    "\n",
    "        self.encoder = ImageEncoder(image_feature_dim)\n",
    "        self.decoder = ImageDecoder(image_feature_dim)\n",
    "    \n",
    "    def forward(self, images):\n",
    "        return self.decoder( self.encoder(images) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_by_autoencoder(autoeuncoder, image_paths, device = \"cuda\"):\n",
    "    tmp_images = [image_preprocess(Image.open(IP)) for IP in image_paths]\n",
    "    images = torch.stack(tmp_images).to(device)\n",
    "\n",
    "    predict_images = autoencoder(images).permute(0, 2, 3, 1).cpu().detach().numpy()\n",
    "\n",
    "    fig = plt.figure()\n",
    "    for i in range(len(image_paths)):\n",
    "        ax = fig.add_subplot(2, len(image_paths), i + 1)\n",
    "        ax.imshow(tmp_images[i].permute(1, 2, 0))\n",
    "        ax.axis(\"off\")\n",
    "        ax.set_title(\"input\")\n",
    "\n",
    "        ax = fig.add_subplot(2, len(image_paths), len(image_paths) + i + 1)\n",
    "        ax.imshow(predict_images[i])\n",
    "        ax.axis(\"off\")\n",
    "        ax.set_title(\"predict\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(f\"{RESULT_DIR}autoencoder_history.png\") or RETRAIN_AUTOENCODER:\n",
    "    train_loss_history = []\n",
    "    test_loss_history = []\n",
    "\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    autoencoder = Autoencoder(image_feature_dim = IMAGE_FEATURE_DIM)\n",
    "    autoencoder.to(device)\n",
    "    optimizer = optim.AdamW(autoencoder.parameters(), lr = LEARNING_RATO_FOR_AUTOENCODER)\n",
    "\n",
    "    train_image_dataloader = make_image_dataloader(train_image_paths, batch_size = BATCH_SIZE_FOR_AUTOENCODER, num_workers = NUM_WORKERS)\n",
    "    test_image_dataloader = make_image_dataloader(test_image_paths, batch_size = BATCH_SIZE_FOR_AUTOENCODER, num_workers = NUM_WORKERS)\n",
    "\n",
    "    # 1イテレーション学習する関数\n",
    "    def train_step_for_autoencoder(autoencoder, optimizer, images):\n",
    "        optimizer.zero_grad()\n",
    "        outputs = autoencoder(images)\n",
    "        loss = nn.MSELoss()(outputs, images)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        return loss.item()\n",
    "\n",
    "    # 1イテレーション検証する関数\n",
    "    def test_step_for_autoencoder(model, images):\n",
    "        with torch.no_grad():\n",
    "            outputs = autoencoder(images)\n",
    "            loss = nn.MSELoss()(outputs, images)\n",
    "        return loss.item()\n",
    "\n",
    "    for epoch in range(EPOCH_FOR_AUTOENCODER):\n",
    "\n",
    "        # train\n",
    "        train_loss_obj = 0.0\n",
    "        autoencoder.train()\n",
    "        pb = tqdm(train_image_dataloader, desc = f\"Epoch {epoch+1}/{EPOCH_FOR_AUTOENCODER}\")\n",
    "        \n",
    "        for images, _ in pb:\n",
    "            images = images.float().to(\"cuda\")\n",
    "\n",
    "            loss = train_step_for_autoencoder(autoencoder, optimizer, images)\n",
    "            train_loss_obj += loss\n",
    "            pb.set_postfix({\"train_loss\": train_loss_obj / (pb.n + 1),})\n",
    "        train_loss = train_loss_obj / len(train_image_dataloader)\n",
    "\n",
    "        # test\n",
    "        test_loss_obj = 0.0\n",
    "        autoencoder.eval()\n",
    "        pb = tqdm(test_image_dataloader, desc = f\"Epoch {epoch+1}/{EPOCH_FOR_AUTOENCODER}\")\n",
    "        \n",
    "        for images, _ in pb:\n",
    "            images = images.to(device)\n",
    "\n",
    "            loss = test_step_for_autoencoder(autoencoder, images)\n",
    "            test_loss_obj += loss\n",
    "            pb.set_postfix({\"test_loss\": test_loss_obj / (pb.n + 1),})\n",
    "        test_loss = test_loss_obj / len(test_image_dataloader)\n",
    "\n",
    "        print(f\"Epoch: {epoch+1}/{EPOCH_FOR_AUTOENCODER}, \"\n",
    "            f\"Train Loss: {train_loss:.4f}, Test Loss: {test_loss:.4f}\")\n",
    "        print(\"-\" * 25)\n",
    "\n",
    "        train_loss_history.append(train_loss)\n",
    "        test_loss_history.append(test_loss)\n",
    "\n",
    "        # 検証誤差を更新した場合、重みを保存\n",
    "        if min(test_loss_history) == test_loss:\n",
    "            torch.save(autoencoder.state_dict(), f\"{RESULT_DIR}best_autoencoder_weights.pth\")\n",
    "\n",
    "        predict_by_autoencoder(autoencoder, test_image_paths[:5])\n",
    "\n",
    "    # 学習結果を保存\n",
    "    with open(f\"{RESULT_DIR}autoencoder_history.json\", \"w\") as f:\n",
    "        json.dump({\n",
    "            \"train_loss\": train_loss_history,\n",
    "            \"test_loss\": test_loss_history,\n",
    "        }, f)\n",
    "\n",
    "    # 学習結果を描画\n",
    "    fig = plt.figure(figsize = (5, 5))\n",
    "    ax = fig.add_subplot(1, 1, 1)\n",
    "    ax.plot(train_loss_history, label = \"train\")\n",
    "    ax.plot(test_loss_history, label = \"test\")\n",
    "    ax.set_xlabel(\"epoch\")\n",
    "    ax.set_ylabel(\"loss\")\n",
    "    ax.legend()\n",
    "    ax.grid()\n",
    "\n",
    "    fig.savefig(f\"{RESULT_DIR}autoencoder_history.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "学習に用いる大喜利の数: 2030920\n",
      " 検証に用いる大喜利の数: 20515\n",
      " 使用する画像の数: 244286\n",
      " 単語の種類: 16705\n"
     ]
    }
   ],
   "source": [
    "# データセットの作成\n",
    "if not os.path.exists(f\"{RESULT_DIR}index_to_word.json\") or RESET_DATA:\n",
    "    # tokenizer\n",
    "    with open('Japanese_BPEEncoder_V2/ja-swe32kfix.txt') as f:\n",
    "        bpe = f.read().split('\\n')\n",
    "\n",
    "    with open('Japanese_BPEEncoder_V2/emoji.json') as f:\n",
    "        emoji = json.loads(f.read())\n",
    "\n",
    "    tokenizer = SWEEncoder_ja(bpe, emoji)\n",
    "\n",
    "    tmp = list()\n",
    "    word_count_dict = dict()\n",
    "\n",
    "    for JP in tqdm(os.listdir(DATA_DIR)):\n",
    "        \n",
    "        N = int(JP.split(\".\")[0])\n",
    "\n",
    "        with open(f\"{DATA_DIR}{JP}\", \"r\") as f:\n",
    "            a = json.load(f)\n",
    "        \n",
    "        image_information = a[\"image_information\"]\n",
    "        is_photographic_probability = image_information[\"is_photographic_probability\"]\n",
    "        ocr = image_information[\"ocr\"]\n",
    "\n",
    "        # 現実写真以外を除去\n",
    "        if not USE_UNREAL_IMAGE:\n",
    "            if is_photographic_probability < 0.8: continue\n",
    "            \n",
    "        # 文字のある画像を除去\n",
    "        if not USE_WORD_IMAGE:\n",
    "            if len(ocr) != 0: continue\n",
    "        \n",
    "        bokes = a[\"bokes\"]\n",
    "\n",
    "        for B in bokes:\n",
    "            # 星が既定の数以下の大喜利を除去\n",
    "            if B[\"star\"] < MIN_STAR:\n",
    "                continue\n",
    "\n",
    "            # 固有名詞を含む大喜利を除去\n",
    "            if not USE_UNIQUE_NOUN_BOKE:\n",
    "                if len(B[\"unique_nouns\"]) != 0: continue\n",
    "\n",
    "            tokenized_boke = tokenizer.encode(B[\"boke\"])\n",
    "            # 単語数が既定の数でない大喜利を除去\n",
    "            if not MIN_SENTENCE_LENGTH <= len(tokenized_boke) < MAX_SENTENCE_LENGTH:\n",
    "                continue\n",
    "\n",
    "            for W in tokenized_boke:\n",
    "                try:\n",
    "                    word_count_dict[W] += 1\n",
    "                except:\n",
    "                    word_count_dict[W] = 1\n",
    "            \n",
    "            tmp.append({\n",
    "                \"image_number\": N,\n",
    "                \"tokenized_boke\": tokenized_boke\n",
    "            })\n",
    "\n",
    "    # 単語の最小出現回数を満たさない大喜利を除去\n",
    "    boke_datas = list()\n",
    "    words = list()\n",
    "\n",
    "    for D in tqdm(tmp):\n",
    "        flag = False\n",
    "        for W in D[\"tokenized_boke\"]:\n",
    "            if word_count_dict[W] < MIN_APPER_WORD:\n",
    "                flag = True\n",
    "                break\n",
    "        if flag: \n",
    "            continue\n",
    "        \n",
    "        boke_datas.append({\n",
    "            \"image_number\": D[\"image_number\"],\n",
    "            \"tokenized_boke\": D[\"tokenized_boke\"]\n",
    "        })\n",
    "        words += D[\"tokenized_boke\"]\n",
    "    words = set(words)\n",
    "    image_numbers = list(set([D[\"image_number\"] for D in boke_datas]))\n",
    "    del tmp\n",
    "\n",
    "    # tokenize\n",
    "    index_to_index = dict()\n",
    "\n",
    "    c = 3\n",
    "    for D in tqdm(boke_datas):\n",
    "        tmp = list()\n",
    "        for W in D[\"tokenized_boke\"]:\n",
    "            try:\n",
    "                index_to_index[W]\n",
    "            except:\n",
    "                index_to_index[W] = c\n",
    "                c += 1\n",
    "            tmp.append(index_to_index[W])\n",
    "        D[\"tokenized_boke\"] = [1] + tmp + [2]\n",
    "\n",
    "    index_to_word = {\n",
    "        V: tokenizer.decode([K]) for K, V in index_to_index.items()\n",
    "    }\n",
    "    index_to_word[0] = \"<PAD>\"\n",
    "    index_to_word[1] = \"<START>\"\n",
    "    index_to_word[2] = \"<END>\"\n",
    "\n",
    "    #\n",
    "    train_boke_datas, test_boke_datas = train_test_split(boke_datas, test_size = 0.01)\n",
    "\n",
    "    with open(f\"{RESULT_DIR}train_boke_datas.json\", \"w\") as f:\n",
    "        json.dump(train_boke_datas, f)\n",
    "    with open(f\"{RESULT_DIR}test_boke_datas.json\", \"w\") as f:\n",
    "        json.dump(test_boke_datas, f)\n",
    "    with open(f\"{RESULT_DIR}index_to_word.json\", \"w\") as f:\n",
    "        json.dump(index_to_word, f)\n",
    "\n",
    "else:\n",
    "    with open(f\"{RESULT_DIR}train_boke_datas.json\", \"r\") as f:\n",
    "        train_boke_datas = json.load(f)\n",
    "    with open(f\"{RESULT_DIR}test_boke_datas.json\", \"r\") as f:\n",
    "        test_boke_datas = json.load(f)\n",
    "    with open(f\"{RESULT_DIR}index_to_word.json\", \"r\") as f:\n",
    "        index_to_word = json.load(f)\n",
    "\n",
    "    image_numbers = [D[\"image_number\"] for D in train_boke_datas] + [D[\"image_number\"] for D in test_boke_datas]\n",
    "    image_numbers = list(set(image_numbers))\n",
    "\n",
    "print(f\"学習に用いる大喜利の数: {len(train_boke_datas)}\\n\", \n",
    "      f\"検証に用いる大喜利の数: {len(test_boke_datas)}\\n\",\n",
    "      f\"使用する画像の数: {len(image_numbers)}\\n\",\n",
    "      f\"単語の種類: {len(index_to_word)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 244286/244286 [00:00<00:00, 819390.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num data: 244285\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-10-ec220e3680c9>:18: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  autoencoder.load_state_dict(torch.load(f\"{RESULT_DIR}best_autoencoder_weights.pth\"))\n",
      "100%|██████████| 1909/1909 [02:14<00:00, 14.15it/s]\n"
     ]
    }
   ],
   "source": [
    "# 画像を特徴量に変換する\n",
    "if RETRAIN_AUTOENCODER:\n",
    "    shutil.rmtree(IMAGE_FEATURE_DIR) \n",
    "    os.mkdir(IMAGE_FEATURE_DIR)\n",
    "\n",
    "tmp = list()\n",
    "for IN in tqdm(image_numbers):\n",
    "    if os.path.exists(f\"{IMAGE_FEATURE_DIR}{IN}.npy\"):\n",
    "        continue\n",
    "    tmp.append(f\"{IMAGE_DIR}{IN}.jpg\")\n",
    "\n",
    "if len(tmp) != 0:\n",
    "    image_dataloader = make_image_dataloader(tmp, batch_size = 128, num_workers = NUM_WORKERS)\n",
    "\n",
    "    # encoder of Autoencoder\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    autoencoder = Autoencoder(image_feature_dim = IMAGE_FEATURE_DIM)\n",
    "    autoencoder.load_state_dict(torch.load(f\"{RESULT_DIR}best_autoencoder_weights.pth\"))\n",
    "    model = autoencoder.encoder\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    for Is, IPs in tqdm(image_dataloader):\n",
    "        Is = Is.to(device)\n",
    "        features = model(Is).detach().cpu().numpy()\n",
    "\n",
    "        for f, IP in zip(features, IPs):\n",
    "            N = IP.split(\"/\")[-1].split(\".\")[0]\n",
    "            np.save(f\"{IMAGE_FEATURE_DIR}{N}\", f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 大喜利生成モデルのクラス\n",
    "class BokeGeneratorModel(nn.Module):\n",
    "    def __init__(self, num_word, image_feature_dim, sentence_length, embedding_dim = 512):\n",
    "        \"\"\"\n",
    "            num_word: 学習に用いる単語の総数\n",
    "            image_feature_dim: 画像の特徴量の次元数\n",
    "            sentence_length: 入力する文章の単語数\n",
    "            embedding_dim: 単語の埋め込み次元数\n",
    "        \"\"\"\n",
    "        super(BokeGeneratorModel, self).__init__()\n",
    "        self.num_word = num_word\n",
    "        self.image_feature_dim = image_feature_dim\n",
    "        self.sentence_length = sentence_length\n",
    "        self.embedding_dim = embedding_dim\n",
    "        \n",
    "        self.fc1 = nn.Linear(image_feature_dim, embedding_dim)\n",
    "        self.embedding = nn.Embedding(num_word, embedding_dim, padding_idx = 0)\n",
    "        self.lstm = nn.LSTM(input_size = embedding_dim, hidden_size = embedding_dim, \n",
    "                            batch_first = True)\n",
    "        self.fc2 = nn.Linear(embedding_dim + embedding_dim, 2 * embedding_dim)\n",
    "        self.fc3 = nn.Linear(2 * embedding_dim, 2 * embedding_dim)\n",
    "        self.fc4 = nn.Linear(2 * embedding_dim, num_word)\n",
    "    \n",
    "    # LSTMの初期値は0で，画像の特徴量と文章の特徴量を全結合層の前で結合する\n",
    "    def forward(self, image_features, sentences):\n",
    "        \"\"\"\n",
    "            image_features: 画像の特徴量\n",
    "            sentences: 入力する文章\n",
    "        \"\"\"\n",
    "        x1 = F.leaky_relu(self.fc1(image_features))\n",
    "        x1 = x1.unsqueeze(1).repeat(1, self.sentence_length, 1)\n",
    "\n",
    "        x2 = self.embedding(sentences)\n",
    "        x2, _ = self.lstm(x2)\n",
    "\n",
    "        x = torch.cat((x1, x2), dim = -1)\n",
    "        x = F.leaky_relu(self.fc2(x))\n",
    "        x = F.leaky_relu(self.fc3(x))\n",
    "\n",
    "        return self.fc4(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 大喜利生成AIの学習用データローダを作る関数\n",
    "def make_dataloader(boke_datas, max_sentence_length, batch_size, num_workers = 4):\n",
    "    \"\"\"\n",
    "        boke_datas: {\"image_number\":画像のお題番号 ,\"tokenized_boke\":トークナイズされた大喜利}からなるリスト\n",
    "        max_sentence_length: 学習データの最大単語数(<START>, <END>トークンを含まない)\n",
    "        num_workers: データローダが使用するCPUのスレッド数\n",
    "    \"\"\"\n",
    "    class SentenceGeneratorDataset(Dataset):\n",
    "        def __init__(self, image_file_numbers, sentences, teacher_signals):\n",
    "            \"\"\"\n",
    "                image_file_numbers: 画像の番号からなるリスト\n",
    "                sentences: 入力文章からなるリスト\n",
    "                teacher_signals: 教師信号からなるリスト\n",
    "            \"\"\"\n",
    "            if len(image_file_numbers) != len(sentences) and len(teacher_signals) != len(sentences):\n",
    "                raise ValueError(\"データリストの長さが一致しません\")\n",
    "\n",
    "            self.image_file_numbers = image_file_numbers\n",
    "            self.sentences = sentences\n",
    "            self.teacher_signals = teacher_signals\n",
    "\n",
    "        def __len__(self):\n",
    "            return len(self.teacher_signals)\n",
    "\n",
    "        def __getitem__(self, idx):\n",
    "            image_feature = np.load(f\"{IMAGE_FEATURE_DIR}{self.image_file_numbers[idx]}.npy\")\n",
    "            sentence = self.sentences[idx]\n",
    "            teacher_signal = self.teacher_signals[idx]\n",
    "\n",
    "            return image_feature, sentence, teacher_signal\n",
    "\n",
    "    def collate_fn_tf(batch):\n",
    "        image_features = torch.tensor(np.array([B[0] for B in batch]))\n",
    "        sentences = torch.tensor(np.array([B[1] for B in batch]))\n",
    "        teacher_signals = torch.tensor(np.array([B[2] for B in batch]))\n",
    "\n",
    "        return image_features, sentences, teacher_signals\n",
    "\n",
    "    image_file_numbers = list()\n",
    "    sentences = list()\n",
    "    teacher_signals = list()\n",
    "\n",
    "    for D in tqdm(boke_datas):\n",
    "        image_file_numbers.append(D[\"image_number\"])\n",
    "        tmp = D[\"tokenized_boke\"] + [0] * (2 + max_sentence_length - len(D[\"tokenized_boke\"]))\n",
    "        sentences.append(tmp[:-1])\n",
    "        teacher_signals.append(tmp[1:])\n",
    "\n",
    "    dataset = SentenceGeneratorDataset(image_file_numbers, sentences, teacher_signals)\n",
    "    dataloader = DataLoader(\n",
    "        dataset, \n",
    "        batch_size = batch_size, \n",
    "        num_workers = num_workers, \n",
    "        collate_fn = collate_fn_tf\n",
    "    )\n",
    "\n",
    "    print(f\"num data: {len(teacher_signals)}\")\n",
    "\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2030920/2030920 [00:04<00:00, 485735.64it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num data: 2030920\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20515/20515 [00:00<00:00, 1101503.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num data: 20515\n"
     ]
    }
   ],
   "source": [
    "train_dataloader = make_dataloader(train_boke_datas, max_sentence_length = MAX_SENTENCE_LENGTH, batch_size = BATCH_SIZE_FOR_GENERATOR, num_workers = NUM_WORKERS)\n",
    "test_dataloader = make_dataloader(test_boke_datas, max_sentence_length = MAX_SENTENCE_LENGTH, batch_size = BATCH_SIZE_FOR_GENERATOR, num_workers = NUM_WORKERS)\n",
    "\n",
    "# モデルの学習\n",
    "train_loss_history = []\n",
    "train_accuracy_history = []\n",
    "test_loss_history = []\n",
    "test_accuracy_history = []\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = BokeGeneratorModel(num_word = len(index_to_word), \n",
    "                           image_feature_dim = IMAGE_FEATURE_DIM, \n",
    "                           sentence_length = MAX_SENTENCE_LENGTH + 1, \n",
    "                           embedding_dim = 2048)\n",
    "model.to(device)\n",
    "optimizer = optim.AdamW(model.parameters(), lr = LEARNING_RATO_FOR_GENERATOR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/25: 100%|██████████| 63467/63467 [1:06:14<00:00, 15.97it/s, train_loss=4.31, train_accuracy=0.302]\n",
      "Evaluating: 100%|██████████| 642/642 [00:10<00:00, 63.79it/s, test_loss=4.04, test_accuracy=0.331]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/25, Train Loss: 4.3109, Train Accuracy: 0.3024, Test Loss: 4.0182, Test Accuracy: 0.3294\n",
      "-------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/25: 100%|██████████| 63467/63467 [51:36<00:00, 20.50it/s, train_loss=3.91, train_accuracy=0.343]\n",
      "Evaluating: 100%|██████████| 642/642 [00:10<00:00, 63.66it/s, test_loss=4.02, test_accuracy=0.341]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2/25, Train Loss: 3.9134, Train Accuracy: 0.3426, Test Loss: 3.9901, Test Accuracy: 0.3387\n",
      "-------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/25: 100%|██████████| 63467/63467 [51:21<00:00, 20.59it/s, train_loss=3.77, train_accuracy=0.355]\n",
      "Evaluating: 100%|██████████| 642/642 [00:10<00:00, 63.98it/s, test_loss=4.03, test_accuracy=0.343]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3/25, Train Loss: 3.7687, Train Accuracy: 0.3554, Test Loss: 3.9988, Test Accuracy: 0.3400\n",
      "-------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/25: 100%|██████████| 63467/63467 [51:26<00:00, 20.57it/s, train_loss=3.66, train_accuracy=0.365]\n",
      "Evaluating: 100%|██████████| 642/642 [00:09<00:00, 64.21it/s, test_loss=4.06, test_accuracy=0.339]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4/25, Train Loss: 3.6638, Train Accuracy: 0.3653, Test Loss: 4.0424, Test Accuracy: 0.3377\n",
      "-------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/25: 100%|██████████| 63467/63467 [51:17<00:00, 20.62it/s, train_loss=3.57, train_accuracy=0.374]\n",
      "Evaluating: 100%|██████████| 642/642 [00:10<00:00, 63.61it/s, test_loss=4.09, test_accuracy=0.335]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5/25, Train Loss: 3.5691, Train Accuracy: 0.3745, Test Loss: 4.0826, Test Accuracy: 0.3345\n",
      "-------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/25: 100%|██████████| 63467/63467 [51:27<00:00, 20.56it/s, train_loss=3.49, train_accuracy=0.383]\n",
      "Evaluating: 100%|██████████| 642/642 [00:10<00:00, 63.84it/s, test_loss=4.15, test_accuracy=0.334]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6/25, Train Loss: 3.4903, Train Accuracy: 0.3826, Test Loss: 4.1418, Test Accuracy: 0.3329\n",
      "-------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/25: 100%|██████████| 63467/63467 [51:31<00:00, 20.53it/s, train_loss=3.42, train_accuracy=0.39] \n",
      "Evaluating: 100%|██████████| 642/642 [00:10<00:00, 63.91it/s, test_loss=4.24, test_accuracy=0.332]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7/25, Train Loss: 3.4242, Train Accuracy: 0.3899, Test Loss: 4.2204, Test Accuracy: 0.3301\n",
      "-------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/25: 100%|██████████| 63467/63467 [51:32<00:00, 20.52it/s, train_loss=3.37, train_accuracy=0.396]\n",
      "Evaluating: 100%|██████████| 642/642 [00:10<00:00, 63.98it/s, test_loss=4.32, test_accuracy=0.331]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8/25, Train Loss: 3.3655, Train Accuracy: 0.3964, Test Loss: 4.2949, Test Accuracy: 0.3297\n",
      "-------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/25: 100%|██████████| 63467/63467 [51:33<00:00, 20.52it/s, train_loss=3.31, train_accuracy=0.402]\n",
      "Evaluating: 100%|██████████| 642/642 [00:10<00:00, 64.04it/s, test_loss=4.37, test_accuracy=0.327]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9/25, Train Loss: 3.3148, Train Accuracy: 0.4023, Test Loss: 4.3549, Test Accuracy: 0.3263\n",
      "-------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/25: 100%|██████████| 63467/63467 [51:32<00:00, 20.52it/s, train_loss=3.27, train_accuracy=0.408]\n",
      "Evaluating: 100%|██████████| 642/642 [00:10<00:00, 63.33it/s, test_loss=4.47, test_accuracy=0.327]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10/25, Train Loss: 3.2717, Train Accuracy: 0.4076, Test Loss: 4.4318, Test Accuracy: 0.3244\n",
      "-------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/25: 100%|██████████| 63467/63467 [51:26<00:00, 20.56it/s, train_loss=3.23, train_accuracy=0.413]\n",
      "Evaluating: 100%|██████████| 642/642 [00:10<00:00, 63.63it/s, test_loss=4.51, test_accuracy=0.322]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 11/25, Train Loss: 3.2316, Train Accuracy: 0.4126, Test Loss: 4.5028, Test Accuracy: 0.3214\n",
      "-------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/25: 100%|██████████| 63467/63467 [51:30<00:00, 20.54it/s, train_loss=3.2, train_accuracy=0.417] \n",
      "Evaluating: 100%|██████████| 642/642 [00:10<00:00, 63.52it/s, test_loss=4.59, test_accuracy=0.323]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 12/25, Train Loss: 3.1959, Train Accuracy: 0.4172, Test Loss: 4.5637, Test Accuracy: 0.3215\n",
      "-------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/25: 100%|██████████| 63467/63467 [1:05:55<00:00, 16.05it/s, train_loss=3.17, train_accuracy=0.421]\n",
      "Evaluating: 100%|██████████| 642/642 [00:10<00:00, 63.72it/s, test_loss=4.63, test_accuracy=0.321]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 13/25, Train Loss: 3.1661, Train Accuracy: 0.4212, Test Loss: 4.6092, Test Accuracy: 0.3196\n",
      "-------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/25: 100%|██████████| 63467/63467 [51:31<00:00, 20.53it/s, train_loss=3.14, train_accuracy=0.425]\n",
      "Evaluating: 100%|██████████| 642/642 [00:10<00:00, 63.80it/s, test_loss=4.71, test_accuracy=0.321]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 14/25, Train Loss: 3.1405, Train Accuracy: 0.4247, Test Loss: 4.6930, Test Accuracy: 0.3201\n",
      "-------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/25: 100%|██████████| 63467/63467 [59:49<00:00, 17.68it/s, train_loss=3.11, train_accuracy=0.428]  \n",
      "Evaluating: 100%|██████████| 642/642 [00:10<00:00, 63.42it/s, test_loss=4.75, test_accuracy=0.32] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 15/25, Train Loss: 3.1134, Train Accuracy: 0.4282, Test Loss: 4.7249, Test Accuracy: 0.3190\n",
      "-------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16/25: 100%|██████████| 63467/63467 [51:25<00:00, 20.57it/s, train_loss=3.09, train_accuracy=0.431]\n",
      "Evaluating: 100%|██████████| 642/642 [00:10<00:00, 63.44it/s, test_loss=4.81, test_accuracy=0.317]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 16/25, Train Loss: 3.0926, Train Accuracy: 0.4311, Test Loss: 4.7834, Test Accuracy: 0.3156\n",
      "-------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17/25: 100%|██████████| 63467/63467 [51:21<00:00, 20.60it/s, train_loss=3.08, train_accuracy=0.434]\n",
      "Evaluating: 100%|██████████| 642/642 [00:10<00:00, 63.06it/s, test_loss=4.85, test_accuracy=0.317]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 17/25, Train Loss: 3.0779, Train Accuracy: 0.4336, Test Loss: 4.8160, Test Accuracy: 0.3142\n",
      "-------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18/25: 100%|██████████| 63467/63467 [51:19<00:00, 20.61it/s, train_loss=3.06, train_accuracy=0.436]  \n",
      "Evaluating: 100%|██████████| 642/642 [00:10<00:00, 63.43it/s, test_loss=4.96, test_accuracy=0.32] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 18/25, Train Loss: 3.0620, Train Accuracy: 0.4360, Test Loss: 4.9040, Test Accuracy: 0.3169\n",
      "-------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19/25: 100%|██████████| 63467/63467 [51:31<00:00, 20.53it/s, train_loss=3.05, train_accuracy=0.438] \n",
      "Evaluating: 100%|██████████| 642/642 [00:10<00:00, 63.16it/s, test_loss=5.12, test_accuracy=0.323]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 19/25, Train Loss: 3.0467, Train Accuracy: 0.4381, Test Loss: 5.0753, Test Accuracy: 0.3204\n",
      "-------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20/25: 100%|██████████| 63467/63467 [51:32<00:00, 20.52it/s, train_loss=3.03, train_accuracy=0.44] \n",
      "Evaluating: 100%|██████████| 642/642 [00:10<00:00, 63.24it/s, test_loss=5.09, test_accuracy=0.321]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 20/25, Train Loss: 3.0329, Train Accuracy: 0.4400, Test Loss: 5.0535, Test Accuracy: 0.3193\n",
      "-------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 21/25: 100%|██████████| 63467/63467 [51:30<00:00, 20.54it/s, train_loss=3.03, train_accuracy=0.442]\n",
      "Evaluating: 100%|██████████| 642/642 [00:10<00:00, 63.15it/s, test_loss=4.98, test_accuracy=0.311]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 21/25, Train Loss: 3.0313, Train Accuracy: 0.4416, Test Loss: 4.9360, Test Accuracy: 0.3081\n",
      "-------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 22/25: 100%|██████████| 63467/63467 [51:31<00:00, 20.53it/s, train_loss=3.02, train_accuracy=0.443]\n",
      "Evaluating: 100%|██████████| 642/642 [00:10<00:00, 62.97it/s, test_loss=4.99, test_accuracy=0.307]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 22/25, Train Loss: 3.0153, Train Accuracy: 0.4431, Test Loss: 4.9566, Test Accuracy: 0.3051\n",
      "-------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 23/25: 100%|██████████| 63467/63467 [51:26<00:00, 20.56it/s, train_loss=3.01, train_accuracy=0.444]\n",
      "Evaluating: 100%|██████████| 642/642 [00:10<00:00, 63.45it/s, test_loss=5.03, test_accuracy=0.312]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 23/25, Train Loss: 3.0058, Train Accuracy: 0.4445, Test Loss: 5.0097, Test Accuracy: 0.3110\n",
      "-------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 24/25: 100%|██████████| 63467/63467 [51:21<00:00, 20.60it/s, train_loss=3, train_accuracy=0.446]   \n",
      "Evaluating: 100%|██████████| 642/642 [00:10<00:00, 63.63it/s, test_loss=5.15, test_accuracy=0.318]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 24/25, Train Loss: 3.0009, Train Accuracy: 0.4456, Test Loss: 5.1278, Test Accuracy: 0.3164\n",
      "-------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 25/25: 100%|██████████| 63467/63467 [51:24<00:00, 20.58it/s, train_loss=2.99, train_accuracy=0.447]\n",
      "Evaluating: 100%|██████████| 642/642 [00:10<00:00, 63.43it/s, test_loss=5.28, test_accuracy=0.318]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 25/25, Train Loss: 2.9941, Train Accuracy: 0.4466, Test Loss: 5.2726, Test Accuracy: 0.3174\n",
      "-------------------------\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 文章生成の精度を計算する関数\n",
    "def calculate_accuracy(teacher_signals, outputs):\n",
    "    \"\"\"\n",
    "        teacher_signals: 教師信号\n",
    "        outputs: モデルの出力\n",
    "    \"\"\"\n",
    "    _, predicted_words = outputs.max(dim = -1)\n",
    "    # パディングに対して精度を計算しない\n",
    "    mask = (teacher_signals != 0)\n",
    "    correct = ((predicted_words == teacher_signals) & mask).sum().item()\n",
    "    total = mask.sum().item()\n",
    "    accuracy = correct / total if total > 0 else 0.0\n",
    "    return accuracy\n",
    "\n",
    "# 1イテレーション学習する関数\n",
    "def train_step(model, optimizer, batch_data, batch_labels):\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(*batch_data)\n",
    "    # パディングに対して損失を計算しない\n",
    "    loss = F.cross_entropy(outputs.view(-1, outputs.size(-1)), batch_labels.view(-1),\n",
    "                           ignore_index = 0)\n",
    "    accuracy = calculate_accuracy(batch_labels, F.softmax(outputs, dim = -1))\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss.item(), accuracy\n",
    "\n",
    "# 1イテレーション検証する関数\n",
    "def evaluate(model, batch_data, batch_labels):\n",
    "    with torch.no_grad():\n",
    "        outputs = model(*batch_data)\n",
    "        loss = F.cross_entropy(outputs.view(-1, outputs.size(-1)), batch_labels.view(-1),\n",
    "                               ignore_index = 0)\n",
    "        accuracy = calculate_accuracy(batch_labels, F.softmax(outputs, dim = -1))\n",
    "    return loss.item(), accuracy\n",
    "\n",
    "for epoch in range(EPOCH_FOR_GENERATOR):\n",
    "\n",
    "    # train\n",
    "    train_loss_obj = 0.0\n",
    "    train_accuracy_obj = 0.0\n",
    "    model.train()\n",
    "    pb = tqdm(train_dataloader, desc = f\"Epoch {epoch+1}/{EPOCH_FOR_GENERATOR}\")\n",
    "    \n",
    "    for image_features, sentences, teacher_signals in pb:\n",
    "        image_features = image_features.to(device)\n",
    "        sentences = sentences.to(device)\n",
    "        teacher_signals = teacher_signals.to(device)\n",
    "        \n",
    "        loss, accuracy = train_step(model, optimizer, (image_features, sentences), teacher_signals)\n",
    "        train_loss_obj += loss\n",
    "        train_accuracy_obj += accuracy\n",
    "        pb.set_postfix({\"train_loss\": train_loss_obj / (pb.n + 1), \"train_accuracy\": train_accuracy_obj / (pb.n + 1)})\n",
    "\n",
    "    train_loss = train_loss_obj / len(train_dataloader)\n",
    "    train_accuracy = train_accuracy_obj / len(train_dataloader)\n",
    "\n",
    "    # test\n",
    "    test_loss_obj = 0.0\n",
    "    test_accuracy_obj = 0.0\n",
    "    model.eval()\n",
    "    pb = tqdm(test_dataloader, desc = \"Evaluating\")\n",
    "\n",
    "    for image_features, sentences, teacher_signals in pb:\n",
    "        image_features = image_features.to(device)\n",
    "        sentences = sentences.to(device)\n",
    "        teacher_signals = teacher_signals.to(device)\n",
    "\n",
    "        loss, accuracy = evaluate(model, (image_features, sentences), teacher_signals)\n",
    "        test_loss_obj += loss\n",
    "        test_accuracy_obj += accuracy\n",
    "        pb.set_postfix({\"test_loss\": test_loss_obj / (pb.n + 1), \"test_accuracy\": test_accuracy_obj / (pb.n + 1)})\n",
    "\n",
    "    test_loss = test_loss_obj / len(test_dataloader)\n",
    "    test_accuracy = test_accuracy_obj / len(test_dataloader)\n",
    "\n",
    "    print(f\"Epoch: {epoch+1}/{EPOCH_FOR_GENERATOR}, \"\n",
    "          f\"Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, \"\n",
    "          f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}\")\n",
    "    print(\"-\" * 25)\n",
    "\n",
    "    train_loss_history.append(train_loss)\n",
    "    train_accuracy_history.append(train_accuracy)\n",
    "    test_loss_history.append(test_loss)\n",
    "    test_accuracy_history.append(test_accuracy)\n",
    "\n",
    "    # 学習精度を更新した場合、重みを保存\n",
    "    if max(train_accuracy_history) == train_accuracy:\n",
    "        torch.save(model.state_dict(), f\"{RESULT_DIR}best_model_weights.pth\")\n",
    "\n",
    "# 学習結果を保存\n",
    "with open(f\"{RESULT_DIR}history.json\", \"w\") as f:\n",
    "    json.dump({\n",
    "        \"train_loss\": train_loss_history,\n",
    "        \"train_accuracy\": train_accuracy_history,\n",
    "        \"test_loss\": test_loss_history,\n",
    "        \"test_accuracy\": test_accuracy_history\n",
    "    }, f)\n",
    "\n",
    "# 学習結果を描画\n",
    "fig = plt.figure(figsize = (10, 5))\n",
    "ax = fig.add_subplot(1, 2, 1)\n",
    "ax.plot(train_loss_history, label = \"train\")\n",
    "ax.plot(test_loss_history, label = \"test\")\n",
    "ax.set_xlabel(\"epoch\")\n",
    "ax.set_ylabel(\"loss\")\n",
    "ax.legend()\n",
    "ax.grid()\n",
    "\n",
    "ax = fig.add_subplot(1, 2, 2)\n",
    "ax.plot(train_accuracy_history, label = \"train\")\n",
    "ax.plot(test_accuracy_history, label = \"test\")\n",
    "ax.set_xlabel(\"epoch\")\n",
    "ax.set_ylabel(\"accuracy\")\n",
    "ax.legend()\n",
    "ax.grid()\n",
    "\n",
    "fig.savefig(f\"{RESULT_DIR}history.png\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Colab_20241111",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
