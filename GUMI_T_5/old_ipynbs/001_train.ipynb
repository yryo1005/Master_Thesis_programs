{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import shutil\n",
    "import argparse\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from transformers import ViTImageProcessor, ViTModel\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import subprocess\n",
    "if not os.path.exists(\"Japanese_BPEEncoder_V2\"):\n",
    "    subprocess.run([\"git\", \"clone\", \"https://github.com/tanreinama/Japanese-BPEEncoder_V2.git\", \"Japanese_BPEEncoder_V2\"])\n",
    "from Japanese_BPEEncoder_V2.encode_swe import SWEEncoder_ja"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # コマンドライン引数の処理\n",
    "# parser = argparse.ArgumentParser(description=\"設定用プログラム\")\n",
    "\n",
    "# parser.add_argument(\"--num_workers\", type = int, default = 16, help = \"データローダが使用するCPUのスレッド数(GPUの総スレッド数の8割が推奨)\")\n",
    "# parser.add_argument(\"--reset_data\", action = \"store_true\", help = \"データセットを再作成するか\")\n",
    "# parser.add_argument(\"--use_unreal_image\", action = \"store_true\", help = \"現実写真以外を使用する\")\n",
    "# parser.add_argument(\"--use_word_image\", action = \"store_true\", help = \"文字を含む画像を使用する\")\n",
    "# parser.add_argument(\"--use_unique_noun_boke\", action = \"store_true\", help = \"固有名詞を含む大喜利を使用する\")\n",
    "# parser.add_argument(\"--min_star\", type = int, default = 0, help = \"大喜利の最小の星の数\")\n",
    "# parser.add_argument(\"--min_apper_word\", type = int, default = 32, help = \"単語の最小出現回数\")\n",
    "# parser.add_argument(\"--min_sentence_length\", type = int, default = 4, help = \"大喜利の最小単語数\")\n",
    "# parser.add_argument(\"--max_sentence_length\", type = int, default = 31, help = \"大喜利の最大単語数\")\n",
    "# parser.add_argument(\"--epoch\", type = int, default = 25, help = \"学習反復回数\")\n",
    "# parser.add_argument(\"--batch_size\", type = int, default = 512, help = \"バッチサイズ\")\n",
    "# parser.add_argument(\"--learning_rate\", type = float, default = 0.001, help = \"学習率\")\n",
    "# parser.add_argument(\"--feature_dim\", type = int, default = 1024, help = \"モデルの特徴量次元数\")\n",
    "# parser.add_argument(\"--num_heads\", type = int, default = 4, help = \"Multi Head Self Attentionのヘッド数\")\n",
    "\n",
    "# args = parser.parse_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "result directory: ../../results/GUMI_T/False_False_False_0_32_4_31_25_512_0.001_1024_4/\n"
     ]
    }
   ],
   "source": [
    "NUM_WORKERS = 16 # args.num_workers\n",
    "RESET_DATA = False # args.reset_data\n",
    "\n",
    "USE_UNREAL_IMAGE = False # args.use_unreal_image\n",
    "USE_WORD_IMAGE = False # args.use_word_image\n",
    "USE_UNIQUE_NOUN_BOKE = False # args.use_unique_noun_boke\n",
    "\n",
    "MIN_STAR = 0 # args.min_star\n",
    "MIN_APPER_WORD = 32 # args.min_apper_word\n",
    "MIN_SENTENCE_LENGTH = 4 # args.min_sentence_length\n",
    "MAX_SENTENCE_LENGTH = 31 # args.max_sentence_length\n",
    "\n",
    "EPOCH = 25 # args.epoch\n",
    "BATCH_SIZE = 512 # args.batch_size\n",
    "LEARNING_RATE = 0.001 # args.learning_rate\n",
    "FEATURE_DIM = 1024 # args.feature_dim\n",
    "NUM_HEADS = 4  # args.num_heads\n",
    "\n",
    "RESULT_DIR = f\"../../results/GUMI_T/{USE_UNREAL_IMAGE}_{USE_WORD_IMAGE}_{USE_UNIQUE_NOUN_BOKE}_{MIN_STAR}_{MIN_APPER_WORD}_{MIN_SENTENCE_LENGTH}_{MAX_SENTENCE_LENGTH}_{EPOCH}_{BATCH_SIZE}_{LEARNING_RATE}_{FEATURE_DIM}_{NUM_HEADS}/\"\n",
    "\n",
    "if not os.path.exists(\"../../results/GUMI_T/\"):\n",
    "    os.mkdir(\"../../results/GUMI_T/\")\n",
    "if not os.path.exists(RESULT_DIR):\n",
    "    os.mkdir(RESULT_DIR)\n",
    "print(f\"result directory: {RESULT_DIR}\")\n",
    "with open(f\"{RESULT_DIR}config.json\", \"w\") as f:\n",
    "    json.dump({\n",
    "        \"USE_UNREAL_IMAGE\": USE_UNREAL_IMAGE,\n",
    "        \"USE_WORD_IMAGE\": USE_WORD_IMAGE,\n",
    "        \"USE_UNIQUE_NOUN_BOKE\": USE_UNIQUE_NOUN_BOKE,\n",
    "        \"MIN_STAR\": MIN_STAR,\n",
    "        \"MIN_APPER_WORD\": MIN_APPER_WORD,\n",
    "        \"MIN_SENTENCE_LENGTH\": MIN_SENTENCE_LENGTH,\n",
    "        \"MAX_SENTENCE_LENGTH\": MAX_SENTENCE_LENGTH,\n",
    "        \"EPOCH\": EPOCH,\n",
    "        \"BATCH_SIZE\": BATCH_SIZE,\n",
    "        \"LEARNING_RATE\": LEARNING_RATE,\n",
    "        \"FEATURE_DIM\": FEATURE_DIM,\n",
    "        \"NUM_HEADS\": NUM_HEADS\n",
    "    }, f)\n",
    "\n",
    "DATA_DIR = \"../../datas/boke_data_assemble/\"\n",
    "IMAGE_DIR = \"../../datas/boke_image/\"\n",
    "\n",
    "IMAGE_FEATURE_DIR = \"../../datas/encoded/vit_image_feature/\"\n",
    "if not os.path.exists(IMAGE_FEATURE_DIR):\n",
    "    os.mkdir(IMAGE_FEATURE_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 668970/668970 [06:34<00:00, 1693.81it/s]\n",
      "100%|██████████| 2135982/2135982 [00:02<00:00, 714941.40it/s] \n",
      "100%|██████████| 2051435/2051435 [00:02<00:00, 967781.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "学習に用いる大喜利の数: 2030920\n",
      " 検証に用いる大喜利の数: 20515\n",
      " 使用する画像の数: 244286\n",
      " 単語の種類: 16705\n"
     ]
    }
   ],
   "source": [
    "# データセットの作成\n",
    "if not os.path.exists(f\"{RESULT_DIR}index_to_word.json\") or RESET_DATA:\n",
    "    # tokenizer\n",
    "    with open('Japanese_BPEEncoder_V2/ja-swe32kfix.txt') as f:\n",
    "        bpe = f.read().split('\\n')\n",
    "\n",
    "    with open('Japanese_BPEEncoder_V2/emoji.json') as f:\n",
    "        emoji = json.loads(f.read())\n",
    "\n",
    "    tokenizer = SWEEncoder_ja(bpe, emoji)\n",
    "\n",
    "    tmp = list()\n",
    "    word_count_dict = dict()\n",
    "\n",
    "    for JP in tqdm(os.listdir(DATA_DIR)):\n",
    "        \n",
    "        N = int(JP.split(\".\")[0])\n",
    "\n",
    "        with open(f\"{DATA_DIR}{JP}\", \"r\") as f:\n",
    "            a = json.load(f)\n",
    "        \n",
    "        image_information = a[\"image_information\"]\n",
    "        is_photographic_probability = image_information[\"is_photographic_probability\"]\n",
    "        ocr = image_information[\"ocr\"]\n",
    "\n",
    "        # 現実写真以外を除去\n",
    "        if not USE_UNREAL_IMAGE:\n",
    "            if is_photographic_probability < 0.8: continue\n",
    "            \n",
    "        # 文字のある画像を除去\n",
    "        if not USE_WORD_IMAGE:\n",
    "            if len(ocr) != 0: continue\n",
    "        \n",
    "        bokes = a[\"bokes\"]\n",
    "\n",
    "        for B in bokes:\n",
    "            # 星が既定の数以下の大喜利を除去\n",
    "            if B[\"star\"] < MIN_STAR:\n",
    "                continue\n",
    "\n",
    "            # 固有名詞を含む大喜利を除去\n",
    "            if not USE_UNIQUE_NOUN_BOKE:\n",
    "                if len(B[\"unique_nouns\"]) != 0: continue\n",
    "\n",
    "            tokenized_boke = tokenizer.encode(B[\"boke\"])\n",
    "            # 単語数が既定の数でない大喜利を除去\n",
    "            if not MIN_SENTENCE_LENGTH <= len(tokenized_boke) < MAX_SENTENCE_LENGTH:\n",
    "                continue\n",
    "\n",
    "            for W in tokenized_boke:\n",
    "                try:\n",
    "                    word_count_dict[W] += 1\n",
    "                except:\n",
    "                    word_count_dict[W] = 1\n",
    "            \n",
    "            tmp.append({\n",
    "                \"image_number\": N,\n",
    "                \"tokenized_boke\": tokenized_boke\n",
    "            })\n",
    "\n",
    "    # 単語の最小出現回数を満たさない大喜利を除去\n",
    "    boke_datas = list()\n",
    "    words = list()\n",
    "\n",
    "    for D in tqdm(tmp):\n",
    "        flag = False\n",
    "        for W in D[\"tokenized_boke\"]:\n",
    "            if word_count_dict[W] < MIN_APPER_WORD:\n",
    "                flag = True\n",
    "                break\n",
    "        if flag: \n",
    "            continue\n",
    "        \n",
    "        boke_datas.append({\n",
    "            \"image_number\": D[\"image_number\"],\n",
    "            \"tokenized_boke\": D[\"tokenized_boke\"]\n",
    "        })\n",
    "        words += D[\"tokenized_boke\"]\n",
    "    words = set(words)\n",
    "    image_numbers = list(set([D[\"image_number\"] for D in boke_datas]))\n",
    "    del tmp\n",
    "\n",
    "    # tokenize\n",
    "    index_to_index = dict()\n",
    "\n",
    "    c = 3\n",
    "    for D in tqdm(boke_datas):\n",
    "        tmp = list()\n",
    "        for W in D[\"tokenized_boke\"]:\n",
    "            try:\n",
    "                index_to_index[W]\n",
    "            except:\n",
    "                index_to_index[W] = c\n",
    "                c += 1\n",
    "            tmp.append(index_to_index[W])\n",
    "        D[\"tokenized_boke\"] = [1] + tmp + [2]\n",
    "\n",
    "    index_to_word = {\n",
    "        V: tokenizer.decode([K]) for K, V in index_to_index.items()\n",
    "    }\n",
    "    index_to_word[0] = \"<PAD>\"\n",
    "    index_to_word[1] = \"<START>\"\n",
    "    index_to_word[2] = \"<END>\"\n",
    "\n",
    "    #\n",
    "    train_boke_datas, test_boke_datas = train_test_split(boke_datas, test_size = 0.01)\n",
    "\n",
    "    with open(f\"{RESULT_DIR}train_boke_datas.json\", \"w\") as f:\n",
    "        json.dump(train_boke_datas, f)\n",
    "    with open(f\"{RESULT_DIR}test_boke_datas.json\", \"w\") as f:\n",
    "        json.dump(test_boke_datas, f)\n",
    "    with open(f\"{RESULT_DIR}index_to_word.json\", \"w\") as f:\n",
    "        json.dump(index_to_word, f)\n",
    "\n",
    "else:\n",
    "    with open(f\"{RESULT_DIR}train_boke_datas.json\", \"r\") as f:\n",
    "        train_boke_datas = json.load(f)\n",
    "    with open(f\"{RESULT_DIR}test_boke_datas.json\", \"r\") as f:\n",
    "        test_boke_datas = json.load(f)\n",
    "    with open(f\"{RESULT_DIR}index_to_word.json\", \"r\") as f:\n",
    "        index_to_word = json.load(f)\n",
    "\n",
    "    image_numbers = [D[\"image_number\"] for D in train_boke_datas] + [D[\"image_number\"] for D in test_boke_datas]\n",
    "    image_numbers = list(set(image_numbers))\n",
    "\n",
    "print(f\"学習に用いる大喜利の数: {len(train_boke_datas)}\\n\", \n",
    "      f\"検証に用いる大喜利の数: {len(test_boke_datas)}\\n\",\n",
    "      f\"使用する画像の数: {len(image_numbers)}\\n\",\n",
    "      f\"単語の種類: {len(index_to_word)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 画像のデータローダを作る関数\n",
    "def make_image_dataloader(image_paths, batch_size, num_workers = 4):\n",
    "    # 画像の前処理\n",
    "    image_preprocess =  ViTImageProcessor.from_pretrained('google/vit-base-patch16-224-in21k')\n",
    "\n",
    "    class LoadImageDataset(Dataset):\n",
    "        def __init__(self, image_paths):\n",
    "            \"\"\"\n",
    "                image_paths: 画像のパスからなるリスト\n",
    "            \"\"\"\n",
    "            self.image_paths = image_paths\n",
    "\n",
    "        def __len__(self):\n",
    "            return len(self.image_paths)\n",
    "\n",
    "        def __getitem__(self, idx):\n",
    "            image = Image.open(image_paths[idx]).convert(\"RGB\")\n",
    "\n",
    "            return image, self.image_paths[idx]\n",
    "    \n",
    "    def collate_fn_tf(batch):\n",
    "        images = image_preprocess(images = [B[0] for B in batch], \n",
    "                                  return_tensors = \"pt\")\n",
    "        image_numbers = [B[1] for B in batch]\n",
    "\n",
    "        return images, image_numbers\n",
    "\n",
    "    print(f\"num data: {len(image_paths)}\")\n",
    "\n",
    "    dataset = LoadImageDataset(image_paths)\n",
    "    dataloader = DataLoader(\n",
    "        dataset, \n",
    "        batch_size = batch_size, \n",
    "        num_workers = num_workers, \n",
    "        collate_fn = collate_fn_tf\n",
    "    )\n",
    "\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 244286/244286 [00:00<00:00, 824507.54it/s]\n"
     ]
    }
   ],
   "source": [
    "# 画像を特徴量に変換する\n",
    "tmp = list()\n",
    "for IN in tqdm(image_numbers):\n",
    "    if os.path.exists(f\"{IMAGE_FEATURE_DIR}{IN}.npy\"):\n",
    "        continue\n",
    "    tmp.append(f\"{IMAGE_DIR}{IN}.jpg\")\n",
    "\n",
    "if len(tmp) != 0:\n",
    "    image_dataloader = make_image_dataloader(tmp, batch_size = 8, num_workers = NUM_WORKERS)\n",
    "\n",
    "    # vision transformer\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = ViTModel.from_pretrained('google/vit-large-patch16-224-in21k')\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    for Is, IPs in tqdm(image_dataloader):\n",
    "        Is = Is.to(device)\n",
    "        outputs = model(**Is)\n",
    "        features = outputs.last_hidden_state.detach().cpu().numpy()\n",
    "\n",
    "        for f, IP in zip(features, IPs):\n",
    "            N = IP.split(\"/\")[-1].split(\".\")[0]\n",
    "            np.save(f\"{IMAGE_FEATURE_DIR}{N}\", f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, max_len, feature_dim):\n",
    "        \"\"\"\n",
    "            max_len: \n",
    "            feature_dim: \n",
    "        \"\"\"\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        pe = torch.zeros(max_len, feature_dim)\n",
    "        position = torch.arange(0, max_len, dtype = torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, feature_dim, 2).float() * -(torch.log(torch.tensor(10000.0)) / feature_dim))\n",
    "\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.pe\n",
    "\n",
    "class MultiHeadSelfAttentionBlock(nn.Module):\n",
    "    def __init__(self, feature_dim, num_heads):\n",
    "        super(MultiHeadSelfAttentionBlock, self).__init__()\n",
    "\n",
    "        self.self_attention = nn.MultiheadAttention(embed_dim = feature_dim, \n",
    "                                                    num_heads = num_heads, batch_first = True)\n",
    "        self.layer_norm1 = nn.LayerNorm(feature_dim)\n",
    "\n",
    "        self.fc = nn.Linear(feature_dim, feature_dim)\n",
    "        self.layer_norm2 = nn.LayerNorm(feature_dim)\n",
    "    \n",
    "    def forward(self, x, attn_mask = None):\n",
    "        # self attention\n",
    "        x_dash, _ = self.self_attention(x, x, x, attn_mask = attn_mask)\n",
    "        x = self.layer_norm1(x_dash) + x\n",
    "\n",
    "        # feed forward\n",
    "        x_dash = F.leaky_relu( self.fc(x) )\n",
    "        return self.layer_norm2(x_dash) + x\n",
    "\n",
    "class MultiHeadCrossAttentionBlock(nn.Module):\n",
    "    def __init__(self, feature_dim, num_heads):\n",
    "        super(MultiHeadCrossAttentionBlock, self).__init__()\n",
    "\n",
    "        self.self_attention = nn.MultiheadAttention(embed_dim = feature_dim, \n",
    "                                                    num_heads = num_heads, batch_first = True)\n",
    "        self.layer_norm1 = nn.LayerNorm(feature_dim)\n",
    "\n",
    "        self.cross_attention = nn.MultiheadAttention(embed_dim = feature_dim, \n",
    "                                                     num_heads = num_heads, batch_first = True)\n",
    "        self.layer_norm2 = nn.LayerNorm(feature_dim)\n",
    "\n",
    "        self.fc = nn.Linear(feature_dim, feature_dim)\n",
    "        self.layer_norm3 = nn.LayerNorm(feature_dim)\n",
    "    \n",
    "    def forward(self, src, tgt, attn_mask = None):\n",
    "        # self attention\n",
    "        tgt_dash, _ = self.self_attention(tgt, tgt, tgt, attn_mask = attn_mask)\n",
    "        tgt = self.layer_norm1(tgt_dash) + tgt\n",
    "        \n",
    "        # cross attention\n",
    "        src_dash, _ = self.cross_attention(tgt, src, src)\n",
    "        tgt = self.layer_norm2(src_dash) + tgt\n",
    "\n",
    "        # feed forward\n",
    "        tgt_dash = F.leaky_relu( self.fc(tgt) )\n",
    "        return self.layer_norm3(tgt_dash) + tgt\n",
    "\n",
    "# 大喜利生成モデルのクラス\n",
    "class TransformerBokeGeneratorModel(nn.Module):\n",
    "    def __init__(self, num_image_patch, image_feature_dim, num_word, sentence_length, feature_dim = 1024, num_heads = 4):\n",
    "        \"\"\"\n",
    "            num_image_patch\n",
    "            num_word: 学習に用いる単語の総数\n",
    "            image_feature_dim: 画像の特徴量の次元数\n",
    "            sentence_length: 入力する文章の単語数\n",
    "            feature_dim: 単語の埋め込み次元数\n",
    "            num_heads: \n",
    "        \"\"\"\n",
    "        super(TransformerBokeGeneratorModel, self).__init__()\n",
    "        self.num_word = num_word\n",
    "        self.image_feature_dim = image_feature_dim\n",
    "        self.sentence_length = sentence_length\n",
    "        self.feature_dim = feature_dim\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "        # image encoder\n",
    "        self.fc1 = nn.Linear(image_feature_dim, feature_dim)\n",
    "        self.pe1 = PositionalEncoding(num_image_patch, feature_dim)\n",
    "        self.self_attention = MultiHeadSelfAttentionBlock(feature_dim = feature_dim, num_heads = num_heads)\n",
    "        \n",
    "        # sentence decoder\n",
    "        self.embedding = nn.Embedding(num_word, feature_dim, padding_idx = 0)\n",
    "        self.pe2 = PositionalEncoding(sentence_length, feature_dim)\n",
    "\n",
    "        self.cross_attention1 = MultiHeadCrossAttentionBlock(feature_dim = feature_dim, num_heads = num_heads)\n",
    "        self.cross_attention2 = MultiHeadCrossAttentionBlock(feature_dim = feature_dim, num_heads = num_heads)\n",
    "        self.fc2 = nn.Linear(feature_dim, num_word)\n",
    "\n",
    "    def forward(self, image_features, sentences):\n",
    "        \"\"\"\n",
    "            image_features: 画像の特徴量(batch_size, num_patch, image_feature_dim)\n",
    "            sentences: 入力する文章(batch_size, sentence_length)\n",
    "        \"\"\"\n",
    "        # encode image \n",
    "        src = F.leaky_relu( self.fc1( image_features ) )\n",
    "        src = self.pe1( src )\n",
    "        src = self.self_attention( src )\n",
    "\n",
    "        # decode sentence\n",
    "        tgt = self.embedding( sentences )\n",
    "        tgt = self.pe2( tgt )\n",
    "\n",
    "        attn_mask = torch.triu(torch.ones(self.sentence_length, self.sentence_length), diagonal = 1)\n",
    "        attn_mask = attn_mask.masked_fill(attn_mask == 1, float('-inf')).to(self.device)\n",
    "\n",
    "        tgt = self.cross_attention1( tgt = tgt, src = src, attn_mask = attn_mask )\n",
    "        tgt = self.cross_attention2( tgt = tgt, src = src, attn_mask = attn_mask  )\n",
    "        return self.fc2( tgt )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 大喜利生成AIの学習用データローダを作る関数\n",
    "def make_dataloader(boke_datas, max_sentence_length, batch_size, num_workers = 4):\n",
    "    \"\"\"\n",
    "        boke_datas: {\"image_number\":画像のお題番号 ,\"tokenized_boke\":トークナイズされた大喜利}からなるリスト\n",
    "        max_sentence_length: 学習データの最大単語数(<START>, <END>トークンを含まない)\n",
    "        num_workers: データローダが使用するCPUのスレッド数\n",
    "    \"\"\"\n",
    "    class SentenceGeneratorDataset(Dataset):\n",
    "        def __init__(self, image_file_numbers, sentences, teacher_signals):\n",
    "            \"\"\"\n",
    "                image_file_numbers: 画像の番号からなるリスト\n",
    "                sentences: 入力文章からなるリスト\n",
    "                teacher_signals: 教師信号からなるリスト\n",
    "            \"\"\"\n",
    "            if len(image_file_numbers) != len(sentences) and len(teacher_signals) != len(sentences):\n",
    "                raise ValueError(\"データリストの長さが一致しません\")\n",
    "\n",
    "            self.image_file_numbers = image_file_numbers\n",
    "            self.sentences = sentences\n",
    "            self.teacher_signals = teacher_signals\n",
    "\n",
    "        def __len__(self):\n",
    "            return len(self.teacher_signals)\n",
    "\n",
    "        def __getitem__(self, idx):\n",
    "            image_feature = np.load(f\"{IMAGE_FEATURE_DIR}{self.image_file_numbers[idx]}.npy\")\n",
    "            sentence = self.sentences[idx]\n",
    "            teacher_signal = self.teacher_signals[idx]\n",
    "\n",
    "            return image_feature, sentence, teacher_signal\n",
    "\n",
    "    def collate_fn_tf(batch):\n",
    "        image_features = torch.tensor(np.array([B[0] for B in batch]))\n",
    "        sentences = torch.tensor(np.array([B[1] for B in batch]))\n",
    "        teacher_signals = torch.tensor(np.array([B[2] for B in batch]))\n",
    "\n",
    "        return image_features, sentences, teacher_signals\n",
    "\n",
    "    image_file_numbers = list()\n",
    "    sentences = list()\n",
    "    teacher_signals = list()\n",
    "\n",
    "    for D in tqdm(boke_datas):\n",
    "        image_file_numbers.append(D[\"image_number\"])\n",
    "        tmp = D[\"tokenized_boke\"] + [0] * (2 + max_sentence_length - len(D[\"tokenized_boke\"]))\n",
    "        sentences.append(tmp[:-1])\n",
    "        teacher_signals.append(tmp[1:])\n",
    "\n",
    "    dataset = SentenceGeneratorDataset(image_file_numbers, sentences, teacher_signals)\n",
    "    dataloader = DataLoader(\n",
    "        dataset, \n",
    "        batch_size = batch_size, \n",
    "        num_workers = num_workers, \n",
    "        collate_fn = collate_fn_tf\n",
    "    )\n",
    "\n",
    "    print(f\"num data: {len(teacher_signals)}\")\n",
    "\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 文章生成の精度を計算する関数\n",
    "def calculate_accuracy(teacher_signals, outputs):\n",
    "    \"\"\"\n",
    "        teacher_signals: 教師信号\n",
    "        outputs: モデルの出力\n",
    "    \"\"\"\n",
    "    _, predicted_words = outputs.max(dim = -1)\n",
    "    # パディングに対して精度を計算しない\n",
    "    mask = (teacher_signals != 0)\n",
    "    correct = ((predicted_words == teacher_signals) & mask).sum().item()\n",
    "    total = mask.sum().item()\n",
    "    accuracy = correct / total if total > 0 else 0.0\n",
    "    return accuracy\n",
    "\n",
    "# 1イテレーション学習する関数\n",
    "def train_step(model, optimizer, batch_data, batch_labels):\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(*batch_data)\n",
    "    # パディングに対して損失を計算しない\n",
    "    loss = F.cross_entropy(outputs.view(-1, outputs.size(-1)), batch_labels.view(-1),\n",
    "                           ignore_index = 0)\n",
    "    accuracy = calculate_accuracy(batch_labels, F.softmax(outputs, dim = -1))\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss.item(), accuracy\n",
    "\n",
    "# 1イテレーション検証する関数\n",
    "def evaluate(model, batch_data, batch_labels):\n",
    "    with torch.no_grad():\n",
    "        outputs = model(*batch_data)\n",
    "        loss = F.cross_entropy(outputs.view(-1, outputs.size(-1)), batch_labels.view(-1),\n",
    "                               ignore_index = 0)\n",
    "        accuracy = calculate_accuracy(batch_labels, F.softmax(outputs, dim = -1))\n",
    "    return loss.item(), accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2030920/2030920 [00:04<00:00, 507286.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num data: 2030920\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20515/20515 [00:00<00:00, 889292.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num data: 20515\n"
     ]
    }
   ],
   "source": [
    "train_dataloader = make_dataloader(train_boke_datas, max_sentence_length = MAX_SENTENCE_LENGTH, batch_size = BATCH_SIZE, num_workers = NUM_WORKERS)\n",
    "test_dataloader = make_dataloader(test_boke_datas, max_sentence_length = MAX_SENTENCE_LENGTH, batch_size = BATCH_SIZE, num_workers = NUM_WORKERS)\n",
    "\n",
    "model = TransformerBokeGeneratorModel(num_image_patch = 197, \n",
    "                                      image_feature_dim = 1024, \n",
    "                                      num_word = len(index_to_word), \n",
    "                                      sentence_length = MAX_SENTENCE_LENGTH + 1, \n",
    "                                      feature_dim = FEATURE_DIM, \n",
    "                                      num_heads = NUM_HEADS)\n",
    "\n",
    "# 学習履歴がある場合，途中から再開する\n",
    "if os.path.exists(f\"{RESULT_DIR}history.json\"):\n",
    "    with open(f\"{RESULT_DIR}history.json\", \"r\") as f:\n",
    "        a = json.load(f)\n",
    "        train_loss_history = a[\"train_loss\"]\n",
    "        train_accuracy_history = a[\"train_accuracy\"]\n",
    "        test_loss_history = a[\"test_loss\"]\n",
    "        test_accuracy_history = a[\"test_accuracy\"]\n",
    "    model.load_state_dict(torch.load(f\"{RESULT_DIR}model_{len(train_loss_history):03}.pth\"))\n",
    "    START_EPOCH = len(train_loss_history)\n",
    "else:\n",
    "    train_loss_history = []\n",
    "    train_accuracy_history = []\n",
    "    test_loss_history = []\n",
    "    test_accuracy_history = []\n",
    "    START_EPOCH = 0\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model.to(device)\n",
    "optimizer = optim.AdamW(model.parameters(), lr = LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/25:   8%|▊         | 304/3967 [06:33<1:19:05,  1.30s/it, train_loss=6.1, train_accuracy=0.169]   \n"
     ]
    }
   ],
   "source": [
    "for epoch in range(START_EPOCH, EPOCH):\n",
    "\n",
    "    # train\n",
    "    train_loss_obj = 0.0\n",
    "    train_accuracy_obj = 0.0\n",
    "    model.train()\n",
    "    pb = tqdm(train_dataloader, desc = f\"Epoch {epoch+1}/{EPOCH}\")\n",
    "    \n",
    "    for image_features, sentences, teacher_signals in pb:\n",
    "        image_features = image_features.to(device)\n",
    "        sentences = sentences.to(device)\n",
    "        teacher_signals = teacher_signals.to(device)\n",
    "        \n",
    "        loss, accuracy = train_step(model, optimizer, (image_features, sentences), teacher_signals)\n",
    "        train_loss_obj += loss\n",
    "        train_accuracy_obj += accuracy\n",
    "        pb.set_postfix({\"train_loss\": train_loss_obj / (pb.n + 1), \"train_accuracy\": train_accuracy_obj / (pb.n + 1)})\n",
    "\n",
    "    train_loss = train_loss_obj / len(train_dataloader)\n",
    "    train_accuracy = train_accuracy_obj / len(train_dataloader)\n",
    "\n",
    "    # test\n",
    "    test_loss_obj = 0.0\n",
    "    test_accuracy_obj = 0.0\n",
    "    model.eval()\n",
    "    pb = tqdm(test_dataloader, desc = \"Evaluating\")\n",
    "\n",
    "    for image_features, sentences, teacher_signals in pb:\n",
    "        image_features = image_features.to(device)\n",
    "        sentences = sentences.to(device)\n",
    "        teacher_signals = teacher_signals.to(device)\n",
    "\n",
    "        loss, accuracy = evaluate(model, (image_features, sentences), teacher_signals)\n",
    "        test_loss_obj += loss\n",
    "        test_accuracy_obj += accuracy\n",
    "        pb.set_postfix({\"test_loss\": test_loss_obj / (pb.n + 1), \"test_accuracy\": test_accuracy_obj / (pb.n + 1)})\n",
    "\n",
    "    test_loss = test_loss_obj / len(test_dataloader)\n",
    "    test_accuracy = test_accuracy_obj / len(test_dataloader)\n",
    "\n",
    "    print(f\"Epoch: {epoch+1}/{EPOCH}, \"\n",
    "          f\"Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, \"\n",
    "          f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}\")\n",
    "    print(\"-\" * 25)\n",
    "\n",
    "    train_loss_history.append(train_loss)\n",
    "    train_accuracy_history.append(train_accuracy)\n",
    "    test_loss_history.append(test_loss)\n",
    "    test_accuracy_history.append(test_accuracy)\n",
    "\n",
    "    torch.save(model.state_dict(), f\"{RESULT_DIR}model_{len(train_loss_history):03}.pth\")\n",
    "    if os.path.exists(f\"{RESULT_DIR}model_{len(train_loss_history) - 1:03}.pth\"):\n",
    "        os.remove(f\"{RESULT_DIR}model_{len(train_loss_history) - 1:03}.pth\")\n",
    "        \n",
    "    # 学習精度を更新した場合、重みを保存\n",
    "    if max(train_accuracy_history) == train_accuracy:\n",
    "        torch.save(model.state_dict(), f\"{RESULT_DIR}best_model.pth\")\n",
    "\n",
    "    # 学習結果を保存\n",
    "    with open(f\"{RESULT_DIR}history.json\", \"w\") as f:\n",
    "        json.dump({\n",
    "            \"train_loss\": train_loss_history,\n",
    "            \"train_accuracy\": train_accuracy_history,\n",
    "            \"test_loss\": test_loss_history,\n",
    "            \"test_accuracy\": test_accuracy_history\n",
    "        }, f)\n",
    "\n",
    "# 学習結果を描画\n",
    "fig = plt.figure(figsize = (10, 5))\n",
    "ax = fig.add_subplot(1, 2, 1)\n",
    "ax.plot(train_loss_history, label = \"train\")\n",
    "ax.plot(test_loss_history, label = \"test\")\n",
    "ax.set_xlabel(\"epoch\")\n",
    "ax.set_ylabel(\"loss\")\n",
    "ax.legend()\n",
    "ax.grid()\n",
    "\n",
    "ax = fig.add_subplot(1, 2, 2)\n",
    "ax.plot(train_accuracy_history, label = \"train\")\n",
    "ax.plot(test_accuracy_history, label = \"test\")\n",
    "ax.set_xlabel(\"epoch\")\n",
    "ax.set_ylabel(\"accuracy\")\n",
    "ax.legend()\n",
    "ax.grid()\n",
    "\n",
    "fig.savefig(f\"{RESULT_DIR}history.png\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Colab_20241111",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
